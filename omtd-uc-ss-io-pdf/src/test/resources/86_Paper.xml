<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>26f009eab9f3e5e46b592c61116c52e3cab4003556b2a55cf4a3182b1f574ba3</job>
    <base_name>43</base_name>
    <doi confidence="possible">http://dx.doi.org/10.1007/1-4020-4889-0_5</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="2"><s id="1" sid="1">The Denoised Web Treebank: Evaluating Dependency Parsing under Noisy Input Conditions</s></article-title>
      </title-group>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="3" page="1" column="1">Joachim Daiber, Rob van der Goot</h1>
        <region class="DoCO:TextChunk" id="10" confidence="possible" page="1" column="1"><s id="4" sid="2">ILLC University of Amsterdam, CLCG University of Groningen <email id="5">J.Daiber@uva.nl</email>, <email id="6">R.van.der.Goot@rug.nl</email> Abstract We introduce the Denoised Web Treebank: a treebank including a normalization layer and a corresponding evaluation metric for dependency parsing of noisy text, such as Tweets.</s><s id="7" sid="3"> This benchmark enables the evaluation of parser robustness as well as text normalization methods, including normalization as machine translation and unsupervised lexical normalization, directly on syntactic trees.</s><s id="8" sid="4"> Experiments show that text normalization together with a combination of domain-specific and generic part-of-speech taggers can lead to a significant improvement in parsing accuracy on this test set.</s><s id="9" sid="5"> Keywords: Parsing, Part-of-Speech Tagging, Social Media Processing, Web Treebank</s></region>
      </section>
      <section class="deo:Introduction">
        <h1 class="DoCO:SectionTitle" id="11" page="1" column="1">1. Introduction</h1>
      </section>
      <region class="DoCO:TextChunk" id="22" page="1" column="1"><s id="12" sid="6">The quality of automatic syntactic analysis of clean, in- domain text has improved steadily in recent decades.</s><s id="13" sid="7"> Out- of-domain text and grammatically noisy text, on the other hand, remain an obstacle and often lead to significant de- creases in parsing accuracy.</s><s id="14" sid="8"> Recently, a lot of effort has been put into adapting natural language processing tools, such as named entity recognition (<xref ref-type="bibr" rid="R18" id="15" class="deo:Reference">Liu et al., 2012</xref>) and POS tagging (<xref ref-type="bibr" rid="R11" id="16" class="deo:Reference">Gimpel et al., 2011</xref>), to noisy content.</s><s id="17" sid="9"> In this paper, we focus on dependency parsing of noisy text.</s><s id="18" sid="10"> Specif- ically, we are interested in how much parse quality can be gained by text normalization.</s><s id="19" sid="11"> For this, we introduce a new dependency treebank with a normalization layer.</s><s id="20" sid="12"> This new dataset can be used to quantify the influence of text normalization on the parsing of user-generated content.</s><s id="21" sid="13"> The contributions of this paper are as follows: (1) We introduce the Denoised Web Treebank, a new Twitter dependency treebank with a normalization layer; (2) we propose a corresponding noise-aware evaluation metric; and (3) we use this dataset and the metric as a benchmark to evaluate the impact of text normalization on dependency parsing of user-generated content.</s></region>
      <section class="deo:RelatedWork">
        <h1 class="DoCO:SectionTitle" id="23" page="1" column="1">2. Related Work</h1>
      </section>
      <region class="DoCO:TextChunk" id="63" page="1" column="1"><s id="24" sid="14">For the domain of web data, various datasets and treebanks have been introduced.</s><s id="25" sid="15"> <xref ref-type="table" rid="T1" id="26" class="deo:Reference">Table 1</xref> provides an overview of all relevant English treebanks.</s><s id="27" sid="16"> The constituency treebanks mentioned here were created using the English Web Treebank annotation guidelines (<xref ref-type="bibr" rid="R3" id="28" class="deo:Reference">Bies et al., 2012</xref>), which are an addendum to the Penn Treebank guidelines (<xref ref-type="bibr" rid="R2" id="29" class="deo:Reference">Bies et al., 1995</xref>).</s><s id="30" sid="17"> These guidelines discuss domain-specific phenomena, including adaptations of existing labels as well as the addition of new labels for novel linguistic constructions.</s><s id="31" sid="18"> <xref ref-type="bibr" rid="R9" id="32" class="deo:Reference">Foster et al. (2011a)</xref> de- scribe a constituency treebank consisting of two domains; Twitter and sports forums.</s><s id="33" sid="19"> The Twitter part is of compa- rable size to our treebank and is described in more detail in <xref ref-type="bibr" rid="R10" id="34" class="deo:Reference">Foster et al. (2011b)</xref>.</s><s id="35" sid="20"> The dependency treebanks show greater diversity in annotation.</s><s id="36" sid="21"> The English Web Treebank (<xref ref-type="bibr" rid="R24" id="37" class="deo:Reference">Silveira et al., 2014</xref>) is annotated using the Universal Dependencies guidelines with additional relation types for the web domain.</s><s id="38" sid="22"> A very<marker type="column" number="2"/><marker type="block"/> different approach is taken for the annotation of the Tweebank (<xref ref-type="bibr" rid="R17" id="40" class="deo:Reference">Kong et al., 2014</xref>).</s><s id="41" sid="23"> In its format, individual words can be skipped in the annotation.</s><s id="42" sid="24"> This is motivated by the idea that not all words in a Tweet contribute significantly to the syntactic structure and their inclusion would lead to arbitrary decisions unhelpful for most downstream applications.</s><s id="43" sid="25"> Additionally, because Tweets are used as units instead of sentences, having multiple roots is allowed.</s><s id="44" sid="26"> This adjusted dependency format makes it harder to use existing parsers with this dataset.</s><s id="45" sid="27"> The Foreebank (<xref ref-type="bibr" rid="R14" id="46" class="deo:Reference">Kaljahi et al., 2015</xref>), a treebank focusing on forum text, is the only other treebank that includes normalization annotation.</s><s id="47" sid="28"> It includes manual normalizations of the raw text, and constituency trees of the normalized sentences.</s><s id="48" sid="29"> The normalization is kept as minimal as possible and is represented in the tree by appending an error suffix to the POS tags.</s><s id="49" sid="30"> The Foreebank allows analysis of the effect of different errors on the parsing performance of a constituency parser.</s><s id="50" sid="31"> Our contribution, the Denoised Web Treebank, fills the gap of a native (i.e., non-converted) dependency treebank including normalizations for the web domain.</s><s id="51" sid="32"> In the past, automatic conversions were used for this task (<xref ref-type="bibr" rid="R21" id="52" class="deo:Reference">Petrov and McDonald, 2012</xref>; <xref ref-type="bibr" rid="R9" id="53" class="deo:Reference">Foster et al., 2011a</xref>) using the Stanford Converter (<xref ref-type="bibr" rid="R6" id="54" class="deo:Reference">De Marneffe et al., 2006</xref>).</s><s id="55" sid="33"> But for the noisy web domain, the conversions might be of questionable quality.</s><s id="56" sid="34"> Previous work on the parsing of web data has mostly focused on complementing existing parsers with semi- supervised data The amount of training data can be artifi- cially enlarged by using self-training or up-training (<xref ref-type="bibr" rid="R21" id="57" class="deo:Reference">Petrov and McDonald, 2012</xref>; <xref ref-type="bibr" rid="R10" id="58" class="deo:Reference">Foster et al., 2011b</xref>).</s><s id="59" sid="35"> Another source of semi-supervised improvements can be gained from using features gathered from large amounts of unannotated texts (<xref ref-type="bibr" rid="R17" id="60" class="deo:Reference">Kong et al., 2014</xref>).</s><s id="61" sid="36"> A completely different approach is taken by <xref ref-type="bibr" rid="R15" id="62" class="deo:Reference">Khan et al. (2013)</xref>, where the most appropri- ate training trees are found in the train treebank for each sentence.</s></region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="64" page="1" column="2">3. Dataset</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="65" page="1" column="2">3.1. Data Preparation</h2>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="81" page="2" column="1">3.2. Normalization</h2>
          <region class="DoCO:TextChunk" id="94" page="2" column="1"><s id="82" sid="37">The goal of the normalization was to leave the original tokens intact and not to replace them by their normalized forms directly.</s><s id="83" sid="38"> Hence, we keep both the original tokens and the normalized version of the sentences with word alignments.</s><s id="84" sid="39"> <xref ref-type="fig" rid="F1" id="85" class="deo:Reference">Figure 1</xref> depicts a gold standard dependency graph including the alignments to the original tokens.</s><s id="86" sid="40"> Abbreviations Abbreviations and slang expressions are expanded whenever necessary for syntactic reasons.</s><s id="87" sid="41"> Exam- ples include instances such as “cu”, used as the short form of see and you, which as a single token would include both the verb and the object of the sentence.</s><s id="88" sid="42"> Punctuation Punctuation is inserted if it is necessary to disambiguate the sentence meaning.</s><s id="89" sid="43"> Emoticons, such as :), are kept intact.</s><s id="90" sid="44"> Zero copulas The data contains several cases of zero copula, i.e.</s><s id="91" sid="45"> a copula verb is not realized in the sentence.</s><s id="92" sid="46"> These occurrences are annotated by inserting the copula verb in the normalized version of the sentence (see <xref ref-type="fig" rid="F1" id="93" class="deo:Reference">Figure 1</xref>).</s></region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="95" page="2" column="1">3.3. Syntactic Annotation</h2>
          <region class="DoCO:TextChunk" id="111" page="2" column="1"><s id="96" sid="47">The normalized tokens were automatically parsed using a generative phrase structure parser 2 and then converted to dependencies.</s><s id="97" sid="48"> Both part-of-speech tags and dependency annotations were then manually corrected in two passes.</s><s id="98" sid="49"> The dependency annotations follow the format of the CoNLL-X shared task on multilingual dependency parsing (<xref ref-type="bibr" rid="R5" id="99" class="deo:Reference">Buchholz and Marsi, 2006</xref>), with the following adaptations: Emoticons Emoticons are kept intact, tagged with the part-of-speech tag UH (interjection) and are attached to the head of the sentence.</s><marker type="column" number="2"/><marker type="block"/> <s id="106" sid="50">Domain-specific elements Twitter-specific syntax was treated as follows: Usernames at the start of a sentence that do not fulfill a syntactic role, e.g.</s><s id="107" sid="51"> as the subject, are attached to the main verb using the DEP (unclassified) dependency relation.</s><s id="108" sid="52"> In all other cases, usernames are treated as proper nouns.</s><s id="109" sid="53"> RT and similar markers, as well as hashtags indicat- ing the topic of a Tweet (e.g.</s><s id="110" sid="54"> #worldcup) are attached to the main verb as DEP.</s></region>
          <region class="unknown" id="102" page="2" column="1">1 English Aspell dictionary: <ext-link ext-link-type="uri" href="http://aspell.net/" id="101">http://aspell.net/</ext-link> 2 https://code.google.com/p/ berkeleyparser/</region>
          <region class="unknown" id="103" page="2" column="2">P ROOT COORD P OBJ NMOD SBJ NMOD OBJ You . are . the . little . . guy . , . are . you . ? . U ∅ da lil guy , ru ?</region>
          <region class="DoCO:FigureBox" id="F1">
            <caption class="deo:Caption" id="105" page="2" column="2"><s id="104" sid="55">Figure 1: Aligned dependency graph.</s></caption>
          </region>
        </section>
      </section>
      <region class="DoCO:TextChunk" id="80" page="1" column="2"><s id="66" sid="56">We collected all Tweets within a period of 24 hours from January 07, 2012 00:00 until 23:59 GMT.</s><s id="67" sid="57"> To avoid possible<marker type="page" number="2"/><marker type="column" number="1"/><marker type="block"/> biases of automatic language identification tools towards well-formed language, we manually classified the Tweets in random order into English and non-English Tweets until we reached a reasonably-sized corpus of Tweets classified as English.</s><s id="75" sid="58"> We then manually split this corpus into sentences and randomly selected 250 sentences as a development set and 250 sentences as a test set.</s><s id="76" sid="59"> <xref ref-type="table" rid="T1" id="77" class="deo:Reference">Table 1</xref> compares some basic statistics of this treebank against other Web treebanks.</s><s id="78" sid="60"> Out-of-vocabulary rate is calculated against the English dictionary of the GNU Aspell spell checker.</s><s id="79" sid="61"> 1</s></region>
      <outsider class="DoCO:TextBox" type="page_nr" id="69" page="1" column="2">649</outsider>
      <region class="DoCO:TableBox" id="Tx70">
        <content>
          <table class="DoCO:Table" number="1" page="2">
            <thead class="table">
              <tr class="table">
                <th class="table"> Name</th>
                <th class="table"> Number</th>
                <th class="table"> OOV 1</th>
                <th class="table"> Average</th>
                <th class="table"> Annotation style</th>
                <th class="table"> Normal-</th>
                <th class="table"> Source</th>
              </tr>
            </thead>
            <tbody>
              <tr class="table.strange">
                <td class="table.strange"></td>
                <td class="table.strange"> of trees</td>
                <td class="table.strange"></td>
                <td class="table.strange"> sent. length</td>
                <td class="table.strange"></td>
                <td class="table.strange"> ization</td>
                <td class="table.strange"></td>
              </tr>
              <tr class="table">
                <td class="table"></td>
                <td class="table"></td>
                <td class="table"></td>
                <td class="table"></td>
                <td class="table"> Constituency and</td>
                <td class="table"></td>
                <td class="table"> Yahoo! answers, e-mails,</td>
              </tr>
              <tr class="table">
                <td class="table"> English Web Treebank</td>
                <td class="table"> 16,622</td>
                <td class="table"> 28%</td>
                <td class="table"> 16.4</td>
                <td class="table"> Dependency</td>
                <td class="table"> No</td>
                <td class="table"> newsgroups, reviews, blogs</td>
              </tr>
              <tr class="table">
                <td class="table"> Foster et al. (2011a)</td>
                <td class="table"> 1,000</td>
                <td class="table"> 25%</td>
                <td class="table"> 14.0</td>
                <td class="table"> Constituency</td>
                <td class="table"> No</td>
                <td class="table"> Twitter, sports forums</td>
              </tr>
              <tr class="table">
                <td class="table"> Foreebank</td>
                <td class="table"> 1,000</td>
                <td class="table"> 29%</td>
                <td class="table"> 15.6</td>
                <td class="table"> Constituency</td>
                <td class="table"> Yes</td>
                <td class="table"> Technical forums</td>
              </tr>
              <tr class="table">
                <td class="table"> Tweebank</td>
                <td class="table"> 929</td>
                <td class="table"> 48%</td>
                <td class="table"> 13.3</td>
                <td class="table"> Dependency</td>
                <td class="table"> No</td>
                <td class="table"> Twitter</td>
              </tr>
              <tr class="table">
                <td class="table"> Denoised Web Treebank</td>
                <td class="table"> 500</td>
                <td class="table"> 31%</td>
                <td class="table"> 10.4</td>
                <td class="table"> Dependency</td>
                <td class="table"> Yes</td>
                <td class="table"> Twitter</td>
              </tr>
            </tbody>
          </table>
        </content>
        <region class="TableInfo" id="72" confidence="possible" page="2" column="1">Name Number OOV 1 Average Annotation style Normal- Source of trees sent. length ization Constituency and Yahoo! answers, e-mails, English Web Treebank 16,622 28% 16.4 No Dependency newsgroups, reviews, blogs <xref ref-type="bibr" rid="R9" id="71" class="deo:Reference">Foster et al. (2011a)</xref> 1,000 25% 14.0 Constituency No Twitter, sports forums Foreebank 1,000 29% 15.6 Constituency Yes Technical forums Tweebank 929 48% 13.3 Dependency No Twitter Denoised Web Treebank 500 31% 10.4 Dependency Yes Twitter</region>
        <caption class="deo:Caption" id="74" page="2" column="1"><s id="73" sid="62">Table 1: English treebanks based on user-generated content.</s></caption>
      </region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="112" page="2" column="2">4. Evaluating Noise-Aware Parsing</h1>
        <region class="DoCO:TextChunk" id="125" page="2" column="2"><s id="113" sid="63">Our dataset provides alignments between the gold standard and the original tokens, allowing for insertions, deletions and modifications.</s><s id="114" sid="64"> Hence, the standard dependency parsing metrics, unlabeled and labeled attachment scores, are no longer sufficient.</s><s id="115" sid="65"> In our dataset, there may not be a di- rect one-to-one correspondence between the predicted tree and the gold tree.</s><s id="116" sid="66"> Hence, we allow the parser to make any insertions, deletions and modifications to the tokens under the assumption that it provides an alignment between the modified tokens and the original tokens.</s><s id="117" sid="67"> The evaluation is then performed using a metric based on precision and recall values calculated using these alignments.</s><marker type="block"/> <s id="119" sid="68">Aligned precision and recall Based on the normalized side of the gold standard and the parser’s aligned predictions, we calculate precision, recall and F 1 score for dependencies (Eq.</s><s id="120" sid="69"> 1–3).</s><s id="121" sid="70"> We base the evaluation metric on the standard definitions of precision and recall, which are widely used in natural language processing.</s><s id="122" sid="71"> In Eq.</s><s id="123" sid="72"> 1 and 2, TP , FP and FN are the numbers of true positive, false positive, and false negative results.</s><s id="124" sid="73"> The F 1 measure is the harmonic mean of precision and recall.</s></region>
        <outsider class="DoCO:TextBox" type="page_nr" id="126" page="2" column="2">650</outsider>
        <region class="DoCO:TextChunk" id="128" confidence="possible" page="3" column="1"><s id="127" sid="74">TP precision = (1) TP + FP TP recall = (2) TP + FN precision · recall F 1 = 2 · (3) precision + recall</s></region>
        <region class="DoCO:TextChunk" id="134" page="3" column="1"><s id="129" sid="75">Definition Formally, when comparing the parser output against the dataset, the following information is provided for each instance: − the original sentence S O − a predicted dependency tree D P = ⟨V P , E P ⟩ − a gold dependency tree D G = ⟨V G , E G ⟩ − alignment function a P for predicted tokens − alignment function a G for gold tokens For each parsed dependency tree, S O is the sequence of original, non-normalized tokens.</s><s id="130" sid="76"> The two alignment functions a G and a P map the gold tokens and the predicted tokens to the original tokens in S O .</s><s id="131" sid="77"> In the case of an insertion, the new token cannot be aligned to any of the original tokens in S O and, therefore, such insertions are mapped to an artificial NULL token.</s><s id="132" sid="78"> Unlabeled dependencies Based on all test instances, we calculate the total number of true positive, false positive and false negative dependency relations as follows: For each gold dependency tree D G = ⟨V G , E G ⟩ and each predicted dependency tree D P = ⟨V P , E P ⟩, let M G and M P be the set of dependency relations mapped to the original tokens in S O : M G = {⟨a G (w i ), a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P = {⟨a P (w i ), a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } The true positive, false positive and false negative dependency relations can then be calculated as: ∑ TP = |M G ∩ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FP = |M P \ M G | ⟨S O ,D P ,D G ,a P ,a G ⟩ ∑ FN = |M G \ M P | ⟨S O ,D P ,D G ,a P ,a G ⟩ Labeled dependencies To measure labeled dependencies, the dependency type is added to the head-modifier pair in M P and M G : M G ′ = {⟨a G (w i ), r, a G (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E G } M P ′ = {⟨a P (w i ), r, a P (w j )⟩ | ⟨w i , r, w j ⟩ ∈ E P } Relation to other metrics This metric can be seen as a generalization of the commonly used attachment score measure.</s><s id="133" sid="79"> If there is a one-to-one alignment between every predicted token and every gold token, the unlabeled and labeled aligned F 1 scores are equivalent to the unlabeled (UAS) and labeled attachment score (LAS).</s></region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="135" page="3" column="2">5. Experiments</h1>
        <region class="DoCO:TextChunk" id="137" page="3" column="2"><s id="136" sid="80">Having introduced our dataset and the corresponding evaluation metric, we can evaluate the impact of two methods commonly used to aid in the parsing of noisy content: noise-robust part-of-speech tagging and text normalization.</s></region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="138" page="3" column="2">5.1. Part-of-Speech Tagging</h2>
          <region class="DoCO:TextChunk" id="165" page="3" column="2"><s id="139" sid="81">POS tagging is a necessary preprocessing step for many parsing algorithms.</s><s id="140" sid="82"> Previous studies (e.g., <xref ref-type="bibr" rid="R10" id="141" class="deo:Reference">Foster et al. (2011b)</xref>) have shown that the accuracy of POS tagging can suffer significantly from noisy content.</s><s id="142" sid="83"> However, it is possible to adapt POS taggers to this type of input.</s><s id="143" sid="84"> In this ex- periment, we will briefly introduce approaches to adapting POS taggers and perform an evaluation on our dataset.</s><s id="144" sid="85"> Domain-specific tagging <xref ref-type="bibr" rid="R11" id="145" class="deo:Reference">Gimpel et al. (2011)</xref> present a domain-specific conditional random field POS tagger using a coarse part-of-speech tagset of 25 tags that was specifically designed for and trained on Twitter data.</s><s id="146" sid="86"> The tagset includes tags for social media-specific tokens, such as URLs, email addresses, emoticons, Twitter hashtags and usernames.</s><s id="147" sid="87"> Role of POS tags in the parser For our experiments, we use the discriminative graph-based maximum spanning tree (MST) parser (<xref ref-type="bibr" rid="R19" id="148" class="deo:Reference">McDonald et al., 2005</xref>).</s><s id="149" sid="88"> This dependency parser expects both fine- and coarse-grained tags as features in its well-established standard setting.</s><s id="150" sid="89"> Since we are interested in the influence of POS tagging on parse quality instead of the impact of individual features in the parser, we use this standard setting but combine the coarse- grained tags from the domain-specific tagger with the POS tags produced by a POS tagger with a less coarse-grained tagset.</s><s id="151" sid="90"> Both are combined by first determining n-best fine- grained tags for each token.</s><s id="152" sid="91"> For hidden Markov models, the probability of the tags occurring at a given position can be calculated using the forward-backward algorithm as P (t i = t) = α i (t)β i (t), where α i (t) is the total probability of all possible tag sequences ending in the tag t at the ith token and β i (t) is the total probability of all tag sequences starting from tag t at the ith token and contin- uing to the end of the sentence (<xref ref-type="bibr" rid="R13" id="153" class="deo:Reference">Jurafsky and Martin, 2000</xref>; <xref ref-type="bibr" rid="R22" id="154" class="deo:Reference">Prins, 2005</xref>).</s><s id="155" sid="92"> The n-best fine-grained tags are then combined with the coarse tags by a simple voting rule.</s><s id="156" sid="93"> Our experiments use a standard trigram HMM tagger 3 (<xref ref-type="bibr" rid="R4" id="157" class="deo:Reference">Brants, 2000</xref>) and the OpenNLP maximum entropy tagger.</s><s id="158" sid="94"> 4 Impact on parse quality <xref ref-type="table" rid="T2" id="159" class="deo:Reference">Table 2</xref> shows the influence of POS tagging on the performance of the MST parser on the development part of our dataset.</s><s id="160" sid="95"> Statistical significance testing is performed using bootstrap resampling (<xref ref-type="bibr" rid="R7" id="161" class="deo:Reference">Efron and Tibshirani, 1993</xref>).</s><s id="162" sid="96"> Except for the last row of the table, all tagging is performed without any text normalization.</s><s id="163" sid="97"> The last row demonstrates the upper bound performance on this task, by using both gold text normalization and gold part-of-speech tags.</s><s id="164" sid="98"> These results show that combining a generic part-of-speech tagger with a more coarse-grained domain-specific tagger can lead to measurable improvements in parse quality.</s></region>
          <region class="unknown" id="167" page="3" column="2">3 https://github.com/danieldk/jitar 4 <ext-link ext-link-type="uri" href="http://opennlp.apache.org/" id="166">http://opennlp.apache.org/</ext-link></region>
          <outsider class="DoCO:TextBox" type="page_nr" id="168" page="3" column="2">651</outsider>
          <region class="DoCO:TableBox" id="Tx169">
            <content>
              <table class="DoCO:Table" number="2" page="4">
                <thead class="table">
                  <tr class="table">
                    <th class="table"> Tagging method</th>
                    <th class="table"> Unlabeled F 1</th>
                    <th class="table"> Labeled F 1</th>
                    <th class="table"> Normalization method</th>
                    <th class="table"> Unlabeled F 1</th>
                    <th class="table"> Labeled F 1</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="table">
                    <td class="table"> HMM</td>
                    <td class="table"> 69.92</td>
                    <td class="table"> 57.60</td>
                    <td class="table"> No normalization</td>
                    <td class="table"> 72.41</td>
                    <td class="table"> 60.16</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Maximum entropy</td>
                    <td class="table"> 70.18</td>
                    <td class="table"> 58.47</td>
                    <td class="table"> + Twitter syntax rules</td>
                    <td class="table"> 76.17 *</td>
                    <td class="table"> 64.38 *</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Coarse + n-best HMM</td>
                    <td class="table"> 71.39 *</td>
                    <td class="table"> 58.39</td>
                    <td class="table"> Unsup. lexical</td>
                    <td class="table"> 76.36 *</td>
                    <td class="table"> 64.80 *</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Coarse + n-best MaxEnt</td>
                    <td class="table"> 72.41 *</td>
                    <td class="table"> 60.16 *</td>
                    <td class="table"> Machine translation</td>
                    <td class="table"> 76.85 *</td>
                    <td class="table"> 65.38 *</td>
                  </tr>
                  <tr class="table">
                    <td class="table"> Gold norm., gold tags</td>
                    <td class="table"> 79.28 *</td>
                    <td class="table"> 69.85 *</td>
                    <td class="table"> Unsup. lexical + MT</td>
                    <td class="table"> 77.08 *</td>
                    <td class="table"> 65.57 *</td>
                  </tr>
                  <tr class="table">
                    <td class="table"></td>
                    <td class="table"></td>
                    <td class="table"></td>
                    <td class="table"> Gold norm., predic. tags 6</td>
                    <td class="table"> 78.20 *</td>
                    <td class="table"> 68.02 *</td>
                  </tr>
                  <tr class="table.strange">
                    <td class="table.strange"> * indicates statistical significance</td>
                    <td class="table.strange"> against MaxEnt baseline</td>
                    <td class="table.strange"> at p-value &lt; 0.05.</td>
                    <td class="table.strange"> Gold norm., gold tags</td>
                    <td class="table.strange"> 79.28 *</td>
                    <td class="table.strange"> 69.85 *</td>
                  </tr>
                </tbody>
              </table>
            </content>
            <region class="TableInfo" id="170" confidence="possible" page="4" column="1">Tagging method Unlabeled F 1 Labeled F 1 Normalization method Unlabeled F 1 Labeled F 1 HMM 69.92 57.60 No normalization 72.41 60.16 Maximum entropy 70.18 58.47 + Twitter syntax rules 76.17 * 64.38 * Coarse + n-best HMM 71.39 * 58.39 Unsup. lexical 76.36 * 64.80 * Coarse + n-best MaxEnt 72.41 * 60.16 * Machine translation 76.85 * 65.38 * Gold norm., gold tags 79.28 * 69.85 * Unsup. lexical + MT 77.08 * 65.57 * Gold norm., predic. tags 6 78.20 * 68.02 * * indicates statistical significance against MaxEnt baseline at p-value &lt; 0.05. Gold norm., gold tags 79.28 * 69.85 *</region>
            <caption class="deo:Caption" id="172" page="4" column="1"><s id="171" sid="99">Table 2: POS tagging and parse quality.</s></caption>
          </region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="173" page="4" column="1">5.2. Text Normalization</h2>
          <region class="DoCO:TextChunk" id="211" page="4" column="1"><s id="174" sid="100">After considering the influence of the underlying POS tagger on parse quality, we now turn to the question of how much the parsing of noisy content is influenced by text normalization.</s><s id="175" sid="101"> For this, we evaluate two common text normalization methods: unsupervised normalization via lexical replacements and normalization based on machine translation.</s><s id="176" sid="102"> Unsupervised lexical normalization Various unsupervised methods for text normalization have been suggested in the relevant literature.</s><s id="177" sid="103"> A popular approach is to perform lexical normalization by correcting individual tokens.</s><s id="178" sid="104"> We implement the model for lexical normalization of text messages by <xref ref-type="bibr" rid="R12" id="179" class="deo:Reference">Han and Baldwin (2011)</xref>.</s><s id="180" sid="105"> This method works in analogy to spell checking, with the biggest difference that in short message data ill-formedness is often intentional, for example due to the message size limit.</s><s id="181" sid="106"> The model performs normalization only on the token level.</s><s id="182" sid="107"> Normalization as machine translation Research in short message normalization has shown that another effec- tive method is to treat the task as a machine translation problem.</s><s id="183" sid="108"> <xref ref-type="bibr" rid="R1" id="184" class="deo:Reference">Aw et al. (2006)</xref> and <xref ref-type="bibr" rid="R23" id="185" class="deo:Reference">Raghunathan and Krawczyk (2009)</xref> explore phrase-based statistical machine translation as a preprocessing step for various NLP tasks involving text messages.</s><s id="186" sid="109"> As part of this effort, they manually normalize a set of 5.000 and 2.500 messages respectively.</s><s id="187" sid="110"> While these corpora are not created for social media services such as Twitter, they nonetheless provide reasonable training corpora for our experiments as the restrictions of both domains are similar.</s><s id="188" sid="111"> Based on this corpus, we train a standard Moses baseline system 5 (<xref ref-type="bibr" rid="R16" id="189" class="deo:Reference">Koehn et al., 2007</xref>) using GIZA++ for word alignments and the grow-diag-final symmetrization heuristic.</s><s id="190" sid="112"> An n-gram language model is built on the English side of the news-commentary data set using IRSTLM (Federico and <xref ref-type="bibr" rid="R8" id="191" class="deo:Reference">Cettolo, 2007</xref>).</s><s id="192" sid="113"> Model weights are estimated using MERT (<xref ref-type="bibr" rid="R20" id="193" class="deo:Reference">Och, 2003</xref>).</s><s id="194" sid="114"> All experiments are performed on the development part of our dataset.</s><s id="195" sid="115"> Twitter-specific processing In order to isolate the influence of the text normalization, Twitter-specific syntax is parsed using a set of deterministic rules.</s><s id="196" sid="116"> Tokens such as retweet indicators and usernames at the start of a Tweet and URLs and hash tags at the end of a Tweet are removed from the text and pushed onto a stack.</s><s id="197" sid="117"> The remaining text is<marker type="column" number="2"/><marker type="block"/> then parsed using the underlying dependency parser and the Twitter-specific tokens are re-attached to the tree accord- ing to a fixed set of rules.</s><s id="204" sid="118"> This deterministic handling of Twitter-specific syntax is applied to all further experiments in <xref ref-type="table" rid="T3" id="205" class="deo:Reference">Table 3</xref>.</s><s id="206" sid="119"> Impact on parse quality <xref ref-type="table" rid="T3" id="207" class="deo:Reference">Table 3</xref> presents the results of the text normalization schemes on the development part of our dataset.</s><s id="208" sid="120"> The results show that a combination of lexical and MT-based normalization approaches leads to results close to the upper bound set by gold standard normalization.</s><s id="209" sid="121"> Although the machine translation system was trained on a different domain, its application leads to better parsing results.</s><s id="210" sid="122"> This improved performance is most likely due to the fact that the method is able to normalize sequences of words on the phrase level instead of being restricted to single-word replacements.</s></region>
          <region class="unknown" id="200" page="4" column="1">5 <ext-link ext-link-type="uri" href="http://statmt.org/moses/?n=Moses." id="199">http://statmt.org/moses/?n=Moses.</ext-link> Baseline</region>
          <region class="unknown" id="201" page="4" column="2">* statistically significant against non-normalized baseline at p-value &lt; 0.05.</region>
          <region class="DoCO:TableBox" id="T3">
            <caption class="deo:Caption" id="203" page="4" column="2"><s id="202" sid="123">Table 3: Text normalization and parse quality.</s></caption>
          </region>
        </section>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="212" page="4" column="2">6. Conclusion</h1>
        <region class="DoCO:TextChunk" id="221" page="4" column="2"><s id="213" sid="124">User-generated content on the web constitutes a rich and important source of information for many use cases.</s><s id="214" sid="125"> However, parsing of such noisy data still poses challenges for many parsing algorithms.</s><s id="215" sid="126"> In this paper, we have compared various strategies for adapting dependency parsing to noisy input conditions.</s><s id="216" sid="127"> In order to do so, we introduced a noise- aware benchmark for dependency parsing consisting of a treebank and a corresponding evaluation metric.</s><s id="217" sid="128"> Our experiments on this new dataset show that text normalization improves parse quality significantly, especially if the normalization method can go beyond the word level (e.g.</s><s id="218" sid="129"> using machine translation).</s><s id="219" sid="130"> To encourage future progress in this area, we make available both the Denoised Web Treebank and the newly introduced noise-aware evaluation metric.</s><s id="220" sid="131"> 7</s></region>
      </section>
      <section class="deo:Acknowledgements">
        <h1 class="DoCO:SectionTitle" id="222" page="4" column="2">Acknowledgments</h1>
        <region class="DoCO:TextChunk" id="227" page="4" column="2"><s id="223" sid="132">We thank Gertjan van Noord for his valuable feedback.</s><s id="224" sid="133"> Parts of this work were supported through the Erasmus Mundus European Masters Program in Language and Com- munication Technologies (EM-LCT).</s><s id="225" sid="134"> The first author is supported by the EXPERT (EXPloiting Empirical ap- pRoaches to Translation) Initial Training Network (ITN) of the European Union’s Seventh Framework Programme.</s><s id="226" sid="135"> The second author is supported by the Nuance Foundation.</s></region>
        <region class="unknown" id="229" page="4" column="2">6 Tags predicted by coarse + n-best MaxEnt. 7 <ext-link ext-link-type="uri" href="http://jodaiber.de/DenoisedWebTreebank" id="228">http://jodaiber.de/DenoisedWebTreebank</ext-link></region>
        <outsider class="DoCO:TextBox" type="page_nr" id="230" page="4" column="2">652</outsider>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="231" page="5" column="1">References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="232" page="5" column="1">Aw, A., Zhang, M., Xiao, J., and Su, J. (2006). A phrase- based statistical model for SMS text normalization. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 33–40.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="233" confidence="possible" page="5" column="1">Bies, A., Ferguson, M., Katz, K., MacIntyre, R., Tredin- nick, V., Kim, G., Marcinkiewicz, M. A., and Schas- berger, B. (1995). Bracketing guidelines for Treebank II style Penn Treebank project. Technical report, University of Pennsylvania.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="234" confidence="possible" page="5" column="1">Bies, A., Mott, J., Warner, C., and Kulick, S. (2012). Bracketing webtext: An addendum to Penn Treebank II guidelines. Technical report, Linguistic Data Consor- tium.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="235" confidence="possible" page="5" column="1">Brants, T. (2000). TnT: a statistical part-of-speech tagger. In Proceedings of the Sixth Conference on Applied Natural Language Processing, ANLC ’00, pages 224–231, Stroudsburg, PA, USA.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="236" confidence="possible" page="5" column="1">Buchholz, S. and Marsi, E. (2006). CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning, pages 149–164.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="237" confidence="possible" page="5" column="1">De Marneffe, M.-C., MacCartney, B., Manning, C. D., et al. (2006). Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="238" confidence="possible" page="5" column="1">Efron, B. and Tibshirani, R. (1993). An Introduction to the Bootstrap. Chapman &amp; Hall, New York.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="239" confidence="possible" page="5" column="1">Federico, M. and Cettolo, M. (2007). Efficient handling of n-gram language models for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 88–95.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="240" confidence="possible" page="5" column="1">Foster, J., C  ̧ etinoglu, O.,  ̈ Wagner, J., Le Roux, J., Nivre, J., Hogan, D., and van Genabith, J. (2011a). From news to comment: Resources and benchmarks for parsing the language of web 2.0. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 893–901, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="241" confidence="possible" page="5" column="1">Foster, J., C  ̧ etinoglu, O.,  ̈ Wagner, J., Le Roux, J., Hogan, S., Nivre, J., Hogan, D., and Van Genabith, J. (2011b). #hardtoparse: POS Tagging and Parsing the Twitter- verse. In AAAI 2011 Workshop On Analyzing Microtext, pages 20–25, United States.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="242" confidence="possible" page="5" column="1">Gimpel, K., Schneider, N., O’Connor, B., Das, D., Mills, D., Eisenstein, J., Heilman, M., Yogatama, D., Flanigan, J., and Smith, N. A. (2011). Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 42–47.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="243" confidence="possible" page="5" column="1">Han, B. and Baldwin, T. (2011). Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368–378.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="244" confidence="possible" page="5" column="1">Jurafsky, D. and Martin, J. H. (2000). Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech. Pearson Education.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="245" page="5" column="2">Kaljahi, R., Foster, J., Roturier, J., Ribeyre, C., Lynn, T., and Le Roux, J. (2015). Foreebank: Syntactic analysis of customer support forums. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1341–1347, Lisbon, Portugal, September.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="246" confidence="possible" page="5" column="2">Khan, M., Dickinson, M., and Kübler, S. (2013). Towards domain adaptation for parsing web data. In Proceedings of the International Conference Recent Advances in Natural Language Processing RANLP 2013, pages 357–364, Hissar, Bulgaria, September. INCOMA Ltd. Shoumen, Bulgaria.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="247" confidence="possible" page="5" column="2">Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., et al. (2007). Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="248" confidence="possible" page="5" column="2">Kong, L., Schneider, N., Swayamdipta, S., Bhatia, A., Dyer, C., and Smith, N. A. (2014). A dependency parser for Tweets. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001–1012, Doha, Qatar, Oc- tober.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="249" confidence="possible" page="5" column="2">Liu, X., Zhou, M., Wei, F., Fu, Z., and Zhou, X. (2012). Joint inference of named entity recognition and normalization for tweets. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 526–535.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="250" confidence="possible" page="5" column="2">McDonald, R. T., Pereira, F., Ribarov, K., and Hajic, J. (2005). Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="251" confidence="possible" page="5" column="2">Och, F. J. (2003). Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, volume 1, pages 160–167.</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="252" confidence="possible" page="5" column="2">Petrov, S. and McDonald, R. (2012). Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="253" confidence="possible" page="5" column="2">Prins, R. P. (2005). Finite-state pre-processing for natural language analysis. Ph.D. thesis, University of Groningen.</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="254" confidence="possible" page="5" column="2">Raghunathan, K. and Krawczyk, S. (2009). Investigating SMS text normalization using statistical machine translation. Technical report.</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="255" confidence="possible" page="5" column="2">Silveira, N., Dozat, T., de Marneffe, M.-C., Bowman, S., Connor, M., Bauer, J., and Manning, C. (2014). A gold standard dependency corpus for English. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2897–2904, Reykjavik, Iceland, May. European Language Resources Association (ELRA).</ref>
        </ref-list>
        <outsider class="DoCO:TextBox" type="page_nr" id="256" page="5" column="2">653</outsider>
      </section>
    </body>
  </article>
</pdfx>
