======== CAS 0 begin ==================================

-------- View _InitialView begin ----------------------------------

DocumentMetaData
   sofa: _InitialView
   begin: 0
   end: 20646
   language: "en"
   documentTitle: "Using Transfer Learning to Assist Exploratory Corpus Annotation"
   documentId: "Using Transfer Learning to Assist Exploratory Corpus Annotation"
   isLastSegment: false

CAS-Text:
Using Transfer Learning to Assist Exploratory Corpus AnnotationWe describe an under-studied problem in language resource management: that of providing automatic assistance to annotators working in exploratory settings. When no satisfactory tagset already exists, such as in under-resourced or undocumented languages, it must be developed iteratively while annotating data. This process naturally gives rise to a sequence of datasets, each annotated differently. We argue that this problem is best regarded as a transfer learning problem with multiple source tasks. Using part-of-speech tagging data with simulated exploratory tagsets, we demonstrate that even simple transfer learning techniques can significantly improve the quality of pre-annotations in an exploratory annotation. Keywords: corpus annotation, transfer learning, machine learningPaul Felt, Eric K. Ringger, Kevin D. Seppi, Kristian HealBrigham Young University Department of Computer Science, † Neal A. Maxwell Institute Provo, UT 84602 USA paul felt@byu.edu, {ringger, kseppi}@cs.byu.edu, kristian heal@byu.edu1. Exploratory Corpus Annotation (ECA)Because corpora are useful for investigating the struc- ture of language, studying the way that languages change over time, testing linguistic hypotheses, charting the move- ment of ideas and historical trends, and even improving the effectiveness of language teaching and acquisition, they are an essential linguistic resource (Kroch, 1989; Sinclair, 2004; Nesselhauf, 2004). One of the most urgent needs for annotated corpora is in the realm of under-resourced and endangered language documentation (Grenoble and Whaley, 1998; Crystal, 2002; Bird and Simons, 2003; Gippert et al., 2006). In domains such as under-resourced language documentation, annotation is unavoidably exploratory and iterative in nature (Hovy and Lavid, 2010). The annotator proposes an annotation scheme, annotates data, and then revises that annotation scheme in light of insights gener- ated by applying the annotation scheme to real world data (Figure 1.), a process which for brevity we refer to as ECA (exploratory corpus annotation). ECA results in a sequence of possibly disjoint annotation sets, or “versions”, V 1 ⊕ . . . ⊕ V K = V , where each V v consists of data and annotations, {(x i , y i )} N i=1 v , produced according to V v ’s annotation scheme. Each time the annotation scheme changes, some cost is incurred as existing annotations are invalidated and must be updated before the corpus is complete. The cost associated with evolving annotation schemes is largely a hidden cost, since few annotation projects record or report internal changes. For example, the Natural Language Processing (NLP) Lab at BYU is collaborating with scholars of ancient languages at the Neal A. Maxwell Institute for Religious Scholarship to create a large corpus of annotated Classical Syriac. 1 Although significant time was spent at the outset defining the annotation scheme that would be used, as pre- liminary data has been annotated at least a dozen updates have already been made to the annotation scheme. Since1 http://cpart.maxwellinstitute.byu.edu/ home/sec/†Healwe are starting with a sizable body of already annotated text, some of these changes have required considerable time and effort to implement (via re-annotation). Annotation scheme revisions are especially likely in exploratory annotation scenarios dealing with languages or linguistic theories that have not previously been codified into annotation schemes. However, revisions can occur even in well established annotation tasks such as English part of speech tagging and parsing. When creating the Penn Treebank corpus, Marcus et al. (1993) re-annotated the Brown corpus data with revised part-of-speech tags. Addi- tionally, Marcus et al. report that after publishing the Penn Treebank, they identified a variety of limitations and incon- sistencies in their annotation scheme for English syntactic parsing and subsequently spent a good deal of effort repair- ing the parsing scheme and re-annotating data for future releases (Marcus et al., 1995). A more extreme case comes from the SUSANNE corpus, another derivative of the Brown Corpus, annotated with very detailed parsing information. The 512-page book describing the SUSANNE annotation scheme required twelve years of work to finish, and the author describes the accompanying 130,000-word corpus as a “by- product of the work of creating the SUSANNE annotation scheme” (Sampson, 2008). These examples underscore the effort involved in developing a satisfactory annotation scheme, even for mainstream languages and linguistic annotation tasks. The costs involved in iteratively improving an annotation scheme mean that budget-constrained corpus developers often must choose between developing a linguistically optimal annotation scheme and generating useful amounts of annotated data. To make matters worse, statistical pre- annotation—the traditional method of reducing annotation overhead—is hampered by the lack of a self-consistent training set.2. Previous Work2.1. Transfer LearningUsing knowledge from one or more source tasks to improve performance on a target task, as the Penn Treebank developers did, is known as transfer learning, and is an area of active research within machine learning. Providing pre- annotations for ECA fits naturally into the transfer learning framework. The following definition of transfer learning borrows notation and ideas from Pan and Yang (2010), but with minor changes to highlight connections between transfer learning and the motivation presented in Section 1..Definition 1 (Transfer Learning) Let D denote a domain comprising the feature space X and a distribution p(x) over data x ∈ X . Let T denote a task, or annotation scheme, comprising a feature space X , a label space Y, and a labeling function f : x → y where x ∈ X and y ∈ Y. 2 Finally, let version V t be the set of annotations produced according to the annotation scheme of task T t . Then the goal of transfer learning is to use data from all source versions V 1..t−1 to improve our ability to model the target annotation scheme T t .Definition 1 encompasses a large number of scenarios. There may be one or many source versions. Differ- ent quantities of data and annotations may be available in any given version. Furthermore, each version is associated with an annotator, domain, and task, and therefore may differ from other versions in terms of X , p(x), Y, or f . Each of these differences can be understood via simple examples. Text and images come from domains D 1 , D 2 where X 1 = X 2 . Poetry and newswire text come from domains where p 1 (x) = p 2 (x). When two part-of-speech tagging tasks use different tagsets, then Y 1 = Y 2 ; when they assign different meanings to the same tag, f 1 = f 2 . Each setting2 Although f may be approximately described in annotation manuals, the true f is generally unseen. In probabilistic ap- proaches, f is often modeled as p(y|x)of these variables in a versioned dataset V 1...t corresponds to a different transfer learning scenario. Much work on transfer learning for NLP is currently in domain adaptation, the transfer setting in which T s = T t , D s = D t , and the domains differ only in the marginal distribution of the data, p s (X) and p t (X). An example of domain adaptation would be using Wikipedia text in which named entities (people, places, events, etc.) have been labeled in order to improve named entity recognition in movie reviews. Other notable related work includes multi-task learning, a transfer setting in which D s = D t and there are multiple tasks that all differ from one another. Multi-task learning is unusual in that no source/destination distinctions made among tasks (Caruana, 1997). For example, Collobert et al. (2011) construct a system that simultaneously learns part- of-speech tagging, named entity recognition, chunking, and semantic role labeling. Each task helps to inform the others, leading to higher performance on all tasks learned jointly than was possible for any individual task when learned in- dividually.Numerous annotation projects have shown that assist- ing annotators with good pre-annotations is essential to an-(a) Corpus annotation with a pre-defined, un- changing annotation schemenotator efficiency and accuracy. Studies in English part- of-speech tagging, Chinese parsing, information extraction, named entity recognition, and Semitic morphological analysis all demonstrate that high accuracy pre-annotations cor- relate strongly with low annotation cost (Marcus et al., 1993; Chiou et al., 2001; Culotta and McCallum, 2005; Ganchev et al., 2007; Felt et al., 2012). This point is critical to our future decision (see Section 4.) to focus on increasing model accuracy as a stand-in for reduced cost. Because accurate pre-annotation models are so effective in reducing annotation effort, much work has been done to train high quality models with as little data as possible. The active learning literature aims to reduce the cost required to train a model by selecting instances for annotation that are likely to be most informative (Settles, 2010). Weakly supervised techniques attempt to speed model training by learning from unlabeled instances, or by allowing annotators to communicate their knowledge to the model by specifying labels or constraints that are applicable to large classes of data instances (Roth and Yih, 2004; Druck et al., 2009; Liang et al., 2009; Ganchev et al., 2010). We know of no previous work that explicitly addresses the problem of providing automatic assistance for ECA; however, corpus developers have naturally gravitated to- wards the solution of adapting knowledge from the data in out-of-date versions. For example, the creators of the Penn Treebank corpus used an altered version of the Brown Corpus’s annotation scheme, which can be seen as an example of a single large step in the iterative process of ECA (Marcus et al., 1993). Although the existing Brown Corpus annotations were unsuitable for direct use, the creators of the Penn Treebank used an automatic tagging model trained on heuristically modified Brown Corpus data to automatically pre-annotate Penn Treebank data. Although im- perfect, these pre-annotations effectively doubled annotation speed, greatly reducing annotation cost (Francis and Kucera, 1979; Church, 1988).(b) Exploratory corpus annotation (ECA) Ann = annotation generation Rev = revising the annotation scheme3. ECA as Transfer LearningWe formally define the problem of providing machine assistance in the setting of exploratory corpus annotation as a transfer learning problem and introduce some simple solutions adapted from previous work. The solutions described below are appealing since they are conceptually simple and easy to implement using existing models as building blocks.Definition 2 (Exploratory Corpus Annotation) The transfer learning setting in which the following are true. There are multiple source tasks T 1..t−1 . For each pair i, j of source tasks, D i = D j and T i = T j . Finally, each source version has at least some labeled data. Little or no labeled data is available for the target version V t .Only a few of the possible transfer learning settings are commonly studied, and none of those match Definition 2. ECA is unusual and interesting from a transfer learning point of view because it has multiple source tasks and there is often a sequential relationship among the tasks.3.1. Baselines: TGTTRAIN and ALLTRAINLet TGTTRAIN be the approach that ignores all data from source tasks and trains a traditional supervised classification model only on the current target data V t . We can expect TGTTRAIN to do well when V t is large and badly when V t is small, such as at the beginning of a project or just after a change is made to the annotation scheme. TGTTRAIN corresponds to annotation projects that discard out- dated annotations when a new version is introduced. In practice, this tends to happen during the initial stages of a project, when the perceived value of the information being lost is low. Let ALLTRAIN be the algorithm that trains a traditional supervised classification model on all available data V 1...t , ignoring version boundaries. We would expect ALLTRAIN to do well when there are few differences between the source and target datasets, and badly when there are large differences.3.2. STACKSTACK refers to an adaptation of stacked generalization, in which traditional supervised models are trained on each of the datasets, and a higher level model is trained to accomplish the target task using the predictions of the lower level models as features (Wolpert, 1992). The higher level model can potentially discover patterns in the errors of the underlying models in order to know which are trustworthy in which contexts, and whether their guesses are wrong in ways that can be predictably mapped to the correct answer.3.3. AUGMENTAUGMENT is a simple and effective domain adaptation technique proposed by Daumé (2007). AUGMENT moves the data into a feature space that allows traditional supervised learning techniques to find commonalities and differences among data from different domains. It is assumed that there are two datasets: the source X s and the target X t . Each source feature vector is mapped into the new feature space by the kernel function Φ s (x) = x, x, 0 , and each target feature vector is mapped by the function Φ t (x) = x, 0, x . Thus each feature has a source-specific version, a target-specific version, and a general version in the new feature space. This can be generalized to the context of multiple source domains by defining Φ s1 (x) = x, 0, 0, ..., x , Φ s2 (x) = 0, x, 0, ..., x ,...,Φ t (x) = 0, 0, ..., x, x .4. ExperimentsWe would like to test the hypothesis that transfer learning can improve pre-annotations for ECA. However, evaluating a model in this setting requires access to corpora that recorded every version of the data V since the beginning of the project. We are aware of no such datasets. However, we can simulate such corpora by starting with an existing annotated corpus and probabilistically generating sequences of intermediate versions that explain how that corpus’s annotation scheme might have come to be. For example, to start in familiar territory, we use the following process to create versioned datasets explaining the derivation of the Penn Treebank’s part-of-speech tagged data.Algorithm 1 Simulate Versioned POS Datasets Given: GoldData is the reference dataset Given: GoldT ags is the reference tagset 1: T ags ← CompositeT ag(GoldT ags) 2: dataset ← {} 3: while T ags = GoldTags do 4: op ← sample({SPLIT,MOVE,MERGE}) 5: T ags ← apply(op, T ags) 6: κ ← sampleV ersionSize() 7: dataset ← annotate(κ, T ags, GoldData) 8: return datasetAlgorithm 1 starts by grouping all the reference tags into a single composite tagset, then iterates between alter- ing the tagset and annotating data until the original tagset is reached. A SPLIT represents an annotator deciding that the largest composite tag in the tagset is too broad and di- viding it. A MERGE represents an annotator deciding that the distinctions between two tags are too fine and lumping them together. A MOVE represents an annotator moving one of the reference tags out of one composite tag and into another; in other words, deciding that a set of words that was previously labeled as something would be better labeled something else. In order to be linguistically reasonable, splits are de- termined by finding a min-cut in the graph of reference tags, where reference tags are connected with strong ad- hoc weights if they are in the same family of tags (e.g., nouns, verbs, punctuation, etc), and weak weights other- wise. Merges and moves are chosen by sampling from a distribution over reference tag pairs where pairs that are identified by the Penn Treebank tagging guidelines as confusable (25 of these) or very confusable (9 of these) are more probable (Santorini, 1990). Finally, sampleV ersionSize() is implemented by hypothesizing that annotation projects alternate between small annotation batches for development and large batches for production approaching the size of the desired corpus, N , as the tagset converges on the reference set. The final mix favors development mode, since production mode in- volves heavy costs in the later stages (Figure 2).This simulated data is clearly not ideal, and we and are actively working on developing real-world ECA data based on annotation projects we are involved in (Felt et al., In Press). However, in the meantime, simulated data allows us to make cautious observations about the characteristics of the problem and projections about the potential of transfer learning models to improve pre-annotation for ECA. We used Algorithm 1 to generate 30 diverse datasets, choosing values for the simulation parameters at random. We used maximum-entropy Markov models (“maxent tag- gers”) with standard features (Toutanova et al., 2003) to implement the transfer algorithms described in Section 3.. Figure 3 shows the learning curve of each algorithm on a single dataset. TGTTRAIN’s learning curve shows deep valleys at each version transition, because it is equivalent to beginning an entirely new learning curve at the beginning of each version. ALLTRAIN, on the other hand, shows a much smoother pattern. Using old data allows it to avoid the valleys seen in TGTTRAIN, but hurts its ability to reach high peaks quickly. STACK and AUGMENT both have peaks similar to TGTTRAIN, but manage to recover more quickly from version changes and avoid the low valleys.Because we are interested in a model that performs well at all stages of the ECA process, we need to compare entire learning curves rather than just final accuracy. A natural summary statistic for the quality of a learning curve is average area under the curve (AAUC). An accurate estimate of AAUC requires good resolution on the learning curve, so we evaluate between 500 and 1000 points on each learning curve, sampling more densely around version transitions. In Table 1 we report the average AAUC of each algorithm over all the simulated datasets, along with model timing statistics. STACK and AUGMENT significantly outperform both TGTTRAIN and ALLTRAIN. Notice that AUGMENT is particularly slow to train, since its feature space has been expanded linearly in K, the number of versions. STACK is unusually slow at inference time, since it must solicit predictions from K subordinate models as features. The fact that TGTTRAIN was relatively easy to beat is encour- aging, suggesting that there is room for more sophisticated transfer learning to make significant improvements over the baseline approach.(a) Development: Noisy Gaussian Size N Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqqqqqqq qq qq qqqqq 5 10 15 20 25 30 35 40 45 (b) Production: Number Noisy of Tags Sigmoid Size N qq qq qq qq q q Dataset qqq qq q q q qqqqqqqqqqqqqqqqqqqqqqqqqqqq 0 5 10 15 20 25 30 35 40 45 Number (c) Mixed of Tags Size N q q q Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqq q qqqq q qq qq q 5 10 15 20 25 30 35 40 45 Number of Simulated Composite Tags5. Conclusions and Future WorkWe have described the problem of providing automatic assistance to annotators working in exploratory settings. We have argued that this problem should be regarded as a transfer learning problem, and shown that existing transfer learning techniques can be adapted to significantly improve the quality of pre-annotations in simulated exploratory part- of-speech tagging. Corpus annotators working in novel an-(a) TGTTRAIN 1 Accuracy 0 50 200 500 2000 10000 (b) Words ALLTRAIN (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (c) STACK (logscale) 1 Accuracy 0 50 200 500 2000 10000 (d) Words AUGMENT (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (logscale) Figure 3: An example learning curve for each algorithm on the same dataset.notation domains should be encouraged by these results and by the existence of a rich body of transfer learning work to draw on. We plan to develop models that leverage the sequential nature of the versions. We also plan to apply the insights developed in this paper to improve pre-annotations for annotators engaged in real-world annotation projects. Finally, in order to apply these techniques seamlessly in annotation projects, it would be beneficial to discover a way of learning to automatically identify the boundaries between versions so that annotators need not manually identify annotation scheme changes.6. References50000500005000050000
[Using Transfer Learning to Assist Exploratory Corpus Annotation]
Paragraph
   sofa: _InitialView
   begin: 0
   end: 63
[Using Transfer Learning to Assist Exploratory Corpus Annotation]
Sentence
   sofa: _InitialView
   begin: 0
   end: 63
[We describe an under-studied problem in language resource management: that of providing automatic assistance to annotators working in exploratory settings. When no satisfactory tagset already exists, such as in under-resourced or undocumented languages, it must be developed iteratively while annotating data. This process naturally gives rise to a sequence of datasets, each annotated differently. We argue that this problem is best regarded as a transfer learning problem with multiple source tasks. Using part-of-speech tagging data with simulated exploratory tagsets, we demonstrate that even simple transfer learning techniques can significantly improve the quality of pre-annotations in an exploratory annotation. Keywords: corpus annotation, transfer learning, machine learning]
Paragraph
   sofa: _InitialView
   begin: 63
   end: 847
[We describe an under-studied problem in language resource management: that of providing automatic assistance to annotators working in exploratory settings.]
Sentence
   sofa: _InitialView
   begin: 63
   end: 218
[ When no satisfactory tagset already exists, such as in under-resourced or undocumented languages, it must be developed iteratively while annotating data.]
Sentence
   sofa: _InitialView
   begin: 218
   end: 372
[ This process naturally gives rise to a sequence of datasets, each annotated differently.]
Sentence
   sofa: _InitialView
   begin: 372
   end: 461
[ We argue that this problem is best regarded as a transfer learning problem with multiple source tasks.]
Sentence
   sofa: _InitialView
   begin: 461
   end: 564
[ Using part-of-speech tagging data with simulated exploratory tagsets, we demonstrate that even simple transfer learning techniques can significantly improve the quality of pre-annotations in an exploratory annotation.]
Sentence
   sofa: _InitialView
   begin: 564
   end: 782
[ Keywords: corpus annotation, transfer learning, machine learning]
Sentence
   sofa: _InitialView
   begin: 782
   end: 847
[Paul Felt, Eric K. Ringger, Kevin D. Seppi, Kristian Heal]
Paragraph
   sofa: _InitialView
   begin: 847
   end: 904
[Paul Felt, Eric K. Ringger, Kevin D. Seppi, Kristian Heal]
Sentence
   sofa: _InitialView
   begin: 847
   end: 904
[Brigham Young University Department of Computer Science, † Neal A. Maxwell Institute Provo, UT 84602 USA paul felt@byu.edu, {ringger, kseppi}@cs.byu.edu, kristian heal@byu.edu]
Paragraph
   sofa: _InitialView
   begin: 904
   end: 1079
[Brigham Young University Department of Computer Science, † Neal A. Maxwell Institute Provo, UT 84602 USA paul felt@byu.edu, {ringger, kseppi}@cs.byu.edu, kristian heal@byu.edu]
Sentence
   sofa: _InitialView
   begin: 904
   end: 1079
[1. Exploratory Corpus Annotation (ECA)]
Paragraph
   sofa: _InitialView
   begin: 1079
   end: 1117
[1. Exploratory Corpus Annotation (ECA)]
Sentence
   sofa: _InitialView
   begin: 1079
   end: 1117
[Because corpora are useful for investigating the struc- ture of language, studying the way that languages change over time, testing linguistic hypotheses, charting the move- ment of ideas and historical trends, and even improving the effectiveness of language teaching and acquisition, they are an essential linguistic resource (Kroch, 1989; Sinclair, 2004; Nesselhauf, 2004). One of the most urgent needs for annotated corpora is in the realm of under-resourced and endangered language documentation (Grenoble and Whaley, 1998; Crystal, 2002; Bird and Simons, 2003; Gippert et al., 2006). In domains such as under-resourced language documentation, annotation is unavoidably exploratory and iterative in nature (Hovy and Lavid, 2010). The annotator proposes an annotation scheme, annotates data, and then revises that annotation scheme in light of insights gener- ated by applying the annotation scheme to real world data (Figure 1.), a process which for brevity we refer to as ECA (exploratory corpus annotation). ECA results in a sequence of possibly disjoint annotation sets, or “versions”, V 1 ⊕ . . . ⊕ V K = V , where each V v consists of data and annotations, {(x i , y i )} N i=1 v , produced according to V v ’s annotation scheme. Each time the annotation scheme changes, some cost is incurred as existing annotations are invalidated and must be updated before the corpus is complete. The cost associated with evolving annotation schemes is largely a hidden cost, since few annotation projects record or report internal changes. For example, the Natural Language Processing (NLP) Lab at BYU is collaborating with scholars of ancient languages at the Neal A. Maxwell Institute for Religious Scholarship to create a large corpus of annotated Classical Syriac. 1 Although significant time was spent at the outset defining the annotation scheme that would be used, as pre- liminary data has been annotated at least a dozen updates have already been made to the annotation scheme. Since]
Paragraph
   sofa: _InitialView
   begin: 1117
   end: 3107
[Because corpora are useful for investigating the struc- ture of language, studying the way that languages change over time, testing linguistic hypotheses, charting the move- ment of ideas and historical trends, and even improving the effectiveness of language teaching and acquisition, they are an essential linguistic resource (Kroch, 1989; Sinclair, 2004; Nesselhauf, 2004).]
Sentence
   sofa: _InitialView
   begin: 1117
   end: 1493
[Kroch, 1989]
Reference
   sofa: _InitialView
   begin: 1446
   end: 1457
   refId: "R17"
   refType: "bibr"
[Sinclair, 2004]
Reference
   sofa: _InitialView
   begin: 1459
   end: 1473
   refId: "R27"
   refType: "bibr"
[Nesselhauf, 2004]
Reference
   sofa: _InitialView
   begin: 1475
   end: 1491
   refId: "R21"
   refType: "bibr"
[ One of the most urgent needs for annotated corpora is in the realm of under-resourced and endangered language documentation (Grenoble and Whaley, 1998; Crystal, 2002; Bird and Simons, 2003; Gippert et al., 2006).]
Sentence
   sofa: _InitialView
   begin: 1493
   end: 1706
[Crystal, 2002]
Reference
   sofa: _InitialView
   begin: 1646
   end: 1659
   refId: "R6"
   refType: "bibr"
[Bird and Simons, 2003]
Reference
   sofa: _InitialView
   begin: 1661
   end: 1682
   refId: "R1"
   refType: "bibr"
[Gippert et al., 2006]
Reference
   sofa: _InitialView
   begin: 1684
   end: 1704
   refId: "R14"
   refType: "bibr"
[ In domains such as under-resourced language documentation, annotation is unavoidably exploratory and iterative in nature (Hovy and Lavid, 2010).]
Sentence
   sofa: _InitialView
   begin: 1706
   end: 1851
[Hovy and Lavid, 2010]
Reference
   sofa: _InitialView
   begin: 1829
   end: 1849
   refId: "R16"
   refType: "bibr"
[ The annotator proposes an annotation scheme, annotates data, and then revises that annotation scheme in light of insights gener- ated by applying the annotation scheme to real world data (Figure 1.),]
Sentence
   sofa: _InitialView
   begin: 1851
   end: 2051
[Figure 1]
Reference
   sofa: _InitialView
   begin: 2040
   end: 2048
   refId: "F1"
   refType: "fig"
[ a process which for brevity we refer to as ECA (exploratory corpus annotation).]
Sentence
   sofa: _InitialView
   begin: 2051
   end: 2131
[ ECA results in a sequence of possibly disjoint annotation sets, or “versions”, V 1 ⊕ .]
Sentence
   sofa: _InitialView
   begin: 2131
   end: 2218
[ .]
Sentence
   sofa: _InitialView
   begin: 2218
   end: 2220
[ .]
Sentence
   sofa: _InitialView
   begin: 2220
   end: 2222
[ ⊕ V K = V , where each V v consists of data and annotations, {(x i , y i )} N i=1 v , produced according to V v ’s annotation scheme.]
Sentence
   sofa: _InitialView
   begin: 2222
   end: 2356
[ Each time the annotation scheme changes, some cost is incurred as existing annotations are invalidated and must be updated before the corpus is complete.]
Sentence
   sofa: _InitialView
   begin: 2356
   end: 2510
[ The cost associated with evolving annotation schemes is largely a hidden cost, since few annotation projects record or report internal changes.]
Sentence
   sofa: _InitialView
   begin: 2510
   end: 2654
[ For example, the Natural Language Processing (NLP) Lab at BYU is collaborating with scholars of ancient languages at the Neal A. Maxwell Institute for Religious Scholarship to create a large corpus of annotated Classical Syriac.]
Sentence
   sofa: _InitialView
   begin: 2654
   end: 2883
[ 1 Although significant time was spent at the outset defining the annotation scheme that would be used, as pre- liminary data has been annotated at least a dozen updates have already been made to the annotation scheme.]
Sentence
   sofa: _InitialView
   begin: 2883
   end: 3101
[ Since]
Sentence
   sofa: _InitialView
   begin: 3101
   end: 3107
[1 http://cpart.maxwellinstitute.byu.edu/ home/sec/]
Paragraph
   sofa: _InitialView
   begin: 3107
   end: 3157
[1 http://cpart.maxwellinstitute.byu.edu/ home/sec/]
Sentence
   sofa: _InitialView
   begin: 3107
   end: 3157
[†]
Paragraph
   sofa: _InitialView
   begin: 3157
   end: 3158
[†]
Sentence
   sofa: _InitialView
   begin: 3157
   end: 3158
[Heal]
Paragraph
   sofa: _InitialView
   begin: 3158
   end: 3162
[Heal]
Sentence
   sofa: _InitialView
   begin: 3158
   end: 3162
[we are starting with a sizable body of already annotated text, some of these changes have required considerable time and effort to implement (via re-annotation). Annotation scheme revisions are especially likely in exploratory annotation scenarios dealing with languages or linguistic theories that have not previously been codified into annotation schemes. However, revisions can occur even in well established annotation tasks such as English part of speech tagging and parsing. When creating the Penn Treebank corpus, Marcus et al. (1993) re-annotated the Brown corpus data with revised part-of-speech tags. Addi- tionally, Marcus et al. report that after publishing the Penn Treebank, they identified a variety of limitations and incon- sistencies in their annotation scheme for English syntactic parsing and subsequently spent a good deal of effort repair- ing the parsing scheme and re-annotating data for future releases (Marcus et al., 1995). A more extreme case comes from the SUSANNE corpus, another derivative of the Brown Corpus, annotated with very detailed parsing information. The 512-page book describing the SUSANNE annotation scheme required twelve years of work to finish, and the author describes the accompanying 130,000-word corpus as a “by- product of the work of creating the SUSANNE annotation scheme” (Sampson, 2008). These examples underscore the effort involved in developing a satisfactory annotation scheme, even for mainstream languages and linguistic annotation tasks. The costs involved in iteratively improving an annotation scheme mean that budget-constrained corpus developers often must choose between developing a linguistically optimal annotation scheme and generating useful amounts of annotated data. To make matters worse, statistical pre- annotation—the traditional method of reducing annotation overhead—is hampered by the lack of a self-consistent training set.]
Paragraph
   sofa: _InitialView
   begin: 3162
   end: 5068
[we are starting with a sizable body of already annotated text, some of these changes have required considerable time and effort to implement (via re-annotation).]
Sentence
   sofa: _InitialView
   begin: 3162
   end: 3323
[ Annotation scheme revisions are especially likely in exploratory annotation scenarios dealing with languages or linguistic theories that have not previously been codified into annotation schemes.]
Sentence
   sofa: _InitialView
   begin: 3323
   end: 3519
[ However, revisions can occur even in well established annotation tasks such as English part of speech tagging and parsing.]
Sentence
   sofa: _InitialView
   begin: 3519
   end: 3642
[ When creating the Penn Treebank corpus, Marcus et al. (1993) re-annotated the Brown corpus data with revised part-of-speech tags.]
Sentence
   sofa: _InitialView
   begin: 3642
   end: 3772
[Marcus et al. (1993)]
Reference
   sofa: _InitialView
   begin: 3683
   end: 3703
   refId: "R19"
   refType: "bibr"
[ Addi- tionally, Marcus et al.]
Sentence
   sofa: _InitialView
   begin: 3772
   end: 3802
[ report that after publishing the Penn Treebank, they identified a variety of limitations and incon- sistencies in their annotation scheme for English syntactic parsing and subsequently spent a good deal of effort repair- ing the parsing scheme and re-annotating data for future releases (Marcus et al., 1995).]
Sentence
   sofa: _InitialView
   begin: 3802
   end: 4112
[Marcus et al., 1995]
Reference
   sofa: _InitialView
   begin: 4091
   end: 4110
   refId: "R20"
   refType: "bibr"
[ A more extreme case comes from the SUSANNE corpus, another derivative of the Brown Corpus, annotated with very detailed parsing information.]
Sentence
   sofa: _InitialView
   begin: 4112
   end: 4253
[ The 512-page book describing the SUSANNE annotation scheme required twelve years of work to finish, and the author describes the accompanying 130,000-word corpus as a “by- product of the work of creating the SUSANNE annotation scheme” (Sampson, 2008).]
Sentence
   sofa: _InitialView
   begin: 4253
   end: 4505
[Sampson, 2008]
Reference
   sofa: _InitialView
   begin: 4490
   end: 4503
   refId: "R24"
   refType: "bibr"
[ These examples underscore the effort involved in developing a satisfactory annotation scheme, even for mainstream languages and linguistic annotation tasks.]
Sentence
   sofa: _InitialView
   begin: 4505
   end: 4662
[ The costs involved in iteratively improving an annotation scheme mean that budget-constrained corpus developers often must choose between developing a linguistically optimal annotation scheme and generating useful amounts of annotated data.]
Sentence
   sofa: _InitialView
   begin: 4662
   end: 4903
[ To make matters worse, statistical pre- annotation—the traditional method of reducing annotation overhead—is hampered by the lack of a self-consistent training set.]
Sentence
   sofa: _InitialView
   begin: 4903
   end: 5068
[2. Previous Work]
Paragraph
   sofa: _InitialView
   begin: 5068
   end: 5084
[2. Previous Work]
Sentence
   sofa: _InitialView
   begin: 5068
   end: 5084
[2.1. Transfer Learning]
Paragraph
   sofa: _InitialView
   begin: 5084
   end: 5106
[2.1. Transfer Learning]
Sentence
   sofa: _InitialView
   begin: 5084
   end: 5106
[Using knowledge from one or more source tasks to improve performance on a target task, as the Penn Treebank developers did, is known as transfer learning, and is an area of active research within machine learning. Providing pre- annotations for ECA fits naturally into the transfer learning framework. The following definition of transfer learning borrows notation and ideas from Pan and Yang (2010), but with minor changes to highlight connections between transfer learning and the motivation presented in Section 1..]
Paragraph
   sofa: _InitialView
   begin: 5106
   end: 5624
[Using knowledge from one or more source tasks to improve performance on a target task, as the Penn Treebank developers did, is known as transfer learning, and is an area of active research within machine learning.]
Sentence
   sofa: _InitialView
   begin: 5106
   end: 5319
[ Providing pre- annotations for ECA fits naturally into the transfer learning framework.]
Sentence
   sofa: _InitialView
   begin: 5319
   end: 5407
[ The following definition of transfer learning borrows notation and ideas from Pan and Yang (2010), but with minor changes to highlight connections between transfer learning and the motivation presented in Section 1..]
Sentence
   sofa: _InitialView
   begin: 5407
   end: 5624
[Pan and Yang (2010)]
Reference
   sofa: _InitialView
   begin: 5486
   end: 5505
   refId: "R22"
   refType: "bibr"
[Definition 1 (Transfer Learning) Let D denote a domain comprising the feature space X and a distribution p(x) over data x ∈ X . Let T denote a task, or annotation scheme, comprising a feature space X , a label space Y, and a labeling function f : x → y where x ∈ X and y ∈ Y. 2 Finally, let version V t be the set of annotations produced according to the annotation scheme of task T t . Then the goal of transfer learning is to use data from all source versions V 1..t−1 to improve our ability to model the target annotation scheme T t .]
Paragraph
   sofa: _InitialView
   begin: 5624
   end: 6161
[Definition 1 (Transfer Learning) Let D denote a domain comprising the feature space X and a distribution p(x) over data x ∈ X .]
Sentence
   sofa: _InitialView
   begin: 5624
   end: 5751
[ Let T denote a task, or annotation scheme, comprising a feature space X , a label space Y, and a labeling function f : x → y where x ∈ X and y ∈ Y.]
Sentence
   sofa: _InitialView
   begin: 5751
   end: 5899
[ 2 Finally, let version V t be the set of annotations produced according to the annotation scheme of task T t .]
Sentence
   sofa: _InitialView
   begin: 5899
   end: 6010
[ Then the goal of transfer learning is to use data from all source versions V 1..t−1 to improve our ability to model the target annotation scheme T t .]
Sentence
   sofa: _InitialView
   begin: 6010
   end: 6161
[Definition 1 encompasses a large number of scenarios. There may be one or many source versions. Differ- ent quantities of data and annotations may be available in any given version. Furthermore, each version is associated with an annotator, domain, and task, and therefore may differ from other versions in terms of X , p(x), Y, or f . Each of these differences can be understood via simple examples. Text and images come from domains D 1 , D 2 where X 1 = X 2 . Poetry and newswire text come from domains where p 1 (x) = p 2 (x). When two part-of-speech tagging tasks use different tagsets, then Y 1 = Y 2 ; when they assign different meanings to the same tag, f 1 = f 2 . Each setting]
Paragraph
   sofa: _InitialView
   begin: 6161
   end: 6847
[Definition 1 encompasses a large number of scenarios.]
Sentence
   sofa: _InitialView
   begin: 6161
   end: 6214
[ There may be one or many source versions.]
Sentence
   sofa: _InitialView
   begin: 6214
   end: 6256
[ Differ- ent quantities of data and annotations may be available in any given version.]
Sentence
   sofa: _InitialView
   begin: 6256
   end: 6342
[ Furthermore, each version is associated with an annotator, domain, and task, and therefore may differ from other versions in terms of X , p(x), Y, or f .]
Sentence
   sofa: _InitialView
   begin: 6342
   end: 6496
[ Each of these differences can be understood via simple examples.]
Sentence
   sofa: _InitialView
   begin: 6496
   end: 6561
[ Text and images come from domains D 1 , D 2 where X 1 = X 2 .]
Sentence
   sofa: _InitialView
   begin: 6561
   end: 6623
[ Poetry and newswire text come from domains where p 1 (x) = p 2 (x).]
Sentence
   sofa: _InitialView
   begin: 6623
   end: 6691
[ When two part-of-speech tagging tasks use different tagsets, then Y 1 = Y 2 ; when they assign different meanings to the same tag, f 1 = f 2 .]
Sentence
   sofa: _InitialView
   begin: 6691
   end: 6834
[ Each setting]
Sentence
   sofa: _InitialView
   begin: 6834
   end: 6847
[2 Although f may be approximately described in annotation manuals, the true f is generally unseen. In probabilistic ap- proaches, f is often modeled as p(y|x)]
Paragraph
   sofa: _InitialView
   begin: 6847
   end: 7005
[2 Although f may be approximately described in annotation manuals, the true f is generally unseen.]
Sentence
   sofa: _InitialView
   begin: 6847
   end: 6945
[ In probabilistic ap- proaches, f is often modeled as p(y|x)]
Sentence
   sofa: _InitialView
   begin: 6945
   end: 7005
[of these variables in a versioned dataset V 1...t corresponds to a different transfer learning scenario. Much work on transfer learning for NLP is currently in domain adaptation, the transfer setting in which T s = T t , D s = D t , and the domains differ only in the marginal distribution of the data, p s (X) and p t (X). An example of domain adaptation would be using Wikipedia text in which named entities (people, places, events, etc.) have been labeled in order to improve named entity recognition in movie reviews. Other notable related work includes multi-task learning, a transfer setting in which D s = D t and there are multiple tasks that all differ from one another. Multi-task learning is unusual in that no source/destination distinctions made among tasks (Caruana, 1997). For example, Collobert et al. (2011) construct a system that simultaneously learns part- of-speech tagging, named entity recognition, chunking, and semantic role labeling. Each task helps to inform the others, leading to higher performance on all tasks learned jointly than was possible for any individual task when learned in- dividually.]
Paragraph
   sofa: _InitialView
   begin: 7005
   end: 8132
[of these variables in a versioned dataset V 1...t corresponds to a different transfer learning scenario.]
Sentence
   sofa: _InitialView
   begin: 7005
   end: 7109
[ Much work on transfer learning for NLP is currently in domain adaptation, the transfer setting in which T s = T t , D s = D t , and the domains differ only in the marginal distribution of the data, p s (X) and p t (X).]
Sentence
   sofa: _InitialView
   begin: 7109
   end: 7328
[ An example of domain adaptation would be using Wikipedia text in which named entities (people, places, events, etc.)]
Sentence
   sofa: _InitialView
   begin: 7328
   end: 7445
[ have been labeled in order to improve named entity recognition in movie reviews.]
Sentence
   sofa: _InitialView
   begin: 7445
   end: 7526
[ Other notable related work includes multi-task learning, a transfer setting in which D s = D t and there are multiple tasks that all differ from one another.]
Sentence
   sofa: _InitialView
   begin: 7526
   end: 7684
[ Multi-task learning is unusual in that no source/destination distinctions made among tasks (Caruana, 1997).]
Sentence
   sofa: _InitialView
   begin: 7684
   end: 7792
[Caruana, 1997]
Reference
   sofa: _InitialView
   begin: 7777
   end: 7790
   refId: "R2"
   refType: "bibr"
[ For example, Collobert et al. (2011) construct a system that simultaneously learns part- of-speech tagging, named entity recognition, chunking, and semantic role labeling.]
Sentence
   sofa: _InitialView
   begin: 7792
   end: 7964
[Collobert et al. (2011)]
Reference
   sofa: _InitialView
   begin: 7806
   end: 7829
   refId: "R5"
   refType: "bibr"
[ Each task helps to inform the others, leading to higher performance on all tasks learned jointly than was possible for any individual task when learned in- dividually.]
Sentence
   sofa: _InitialView
   begin: 7964
   end: 8132
[Numerous annotation projects have shown that assist- ing annotators with good pre-annotations is essential to an-]
Paragraph
   sofa: _InitialView
   begin: 8132
   end: 8245
[Numerous annotation projects have shown that assist- ing annotators with good pre-annotations is essential to an-]
Sentence
   sofa: _InitialView
   begin: 8132
   end: 8245
[(a) Corpus annotation with a pre-defined, un- changing annotation scheme]
Paragraph
   sofa: _InitialView
   begin: 8245
   end: 8317
[(a) Corpus annotation with a pre-defined, un- changing annotation scheme]
Sentence
   sofa: _InitialView
   begin: 8245
   end: 8317
[notator efficiency and accuracy. Studies in English part- of-speech tagging, Chinese parsing, information extraction, named entity recognition, and Semitic morphological analysis all demonstrate that high accuracy pre-annotations cor- relate strongly with low annotation cost (Marcus et al., 1993; Chiou et al., 2001; Culotta and McCallum, 2005; Ganchev et al., 2007; Felt et al., 2012). This point is critical to our future decision (see Section 4.) to focus on increasing model accuracy as a stand-in for reduced cost. Because accurate pre-annotation models are so effective in reducing annotation effort, much work has been done to train high quality models with as little data as possible. The active learning literature aims to reduce the cost required to train a model by selecting instances for annotation that are likely to be most informative (Settles, 2010). Weakly supervised techniques attempt to speed model training by learning from unlabeled instances, or by allowing annotators to communicate their knowledge to the model by specifying labels or constraints that are applicable to large classes of data instances (Roth and Yih, 2004; Druck et al., 2009; Liang et al., 2009; Ganchev et al., 2010). We know of no previous work that explicitly addresses the problem of providing automatic assistance for ECA; however, corpus developers have naturally gravitated to- wards the solution of adapting knowledge from the data in out-of-date versions. For example, the creators of the Penn Treebank corpus used an altered version of the Brown Corpus’s annotation scheme, which can be seen as an example of a single large step in the iterative process of ECA (Marcus et al., 1993). Although the existing Brown Corpus annotations were unsuitable for direct use, the creators of the Penn Treebank used an automatic tagging model trained on heuristically modified Brown Corpus data to automatically pre-annotate Penn Treebank data. Although im- perfect, these pre-annotations effectively doubled annotation speed, greatly reducing annotation cost (Francis and Kucera, 1979; Church, 1988).]
Paragraph
   sofa: _InitialView
   begin: 8317
   end: 10408
[notator efficiency and accuracy.]
Sentence
   sofa: _InitialView
   begin: 8317
   end: 8349
[ Studies in English part- of-speech tagging, Chinese parsing, information extraction, named entity recognition, and Semitic morphological analysis all demonstrate that high accuracy pre-annotations cor- relate strongly with low annotation cost (Marcus et al., 1993; Chiou et al., 2001; Culotta and McCallum, 2005; Ganchev et al., 2007; Felt et al., 2012).]
Sentence
   sofa: _InitialView
   begin: 8349
   end: 8704
[Marcus et al., 1993]
Reference
   sofa: _InitialView
   begin: 8594
   end: 8613
   refId: "R19"
   refType: "bibr"
[Chiou et al., 2001]
Reference
   sofa: _InitialView
   begin: 8615
   end: 8633
   refId: "R3"
   refType: "bibr"
[Culotta and McCallum, 2005]
Reference
   sofa: _InitialView
   begin: 8635
   end: 8661
   refId: "R7"
   refType: "bibr"
[Ganchev et al., 2007]
Reference
   sofa: _InitialView
   begin: 8663
   end: 8683
   refId: "R12"
   refType: "bibr"
[Felt et al., 2012]
Reference
   sofa: _InitialView
   begin: 8685
   end: 8702
   refId: "R10"
   refType: "bibr"
[ This point is critical to our future decision (see Section 4.)]
Sentence
   sofa: _InitialView
   begin: 8704
   end: 8767
[ to focus on increasing model accuracy as a stand-in for reduced cost.]
Sentence
   sofa: _InitialView
   begin: 8767
   end: 8837
[ Because accurate pre-annotation models are so effective in reducing annotation effort, much work has been done to train high quality models with as little data as possible.]
Sentence
   sofa: _InitialView
   begin: 8837
   end: 9010
[ The active learning literature aims to reduce the cost required to train a model by selecting instances for annotation that are likely to be most informative (Settles, 2010).]
Sentence
   sofa: _InitialView
   begin: 9010
   end: 9185
[Settles, 2010]
Reference
   sofa: _InitialView
   begin: 9170
   end: 9183
   refId: "R26"
   refType: "bibr"
[ Weakly supervised techniques attempt to speed model training by learning from unlabeled instances, or by allowing annotators to communicate their knowledge to the model by specifying labels or constraints that are applicable to large classes of data instances (Roth and Yih, 2004; Druck et al., 2009; Liang et al., 2009; Ganchev et al., 2010).]
Sentence
   sofa: _InitialView
   begin: 9185
   end: 9529
[Roth and Yih, 2004]
Reference
   sofa: _InitialView
   begin: 9447
   end: 9465
   refId: "R23"
   refType: "bibr"
[Druck et al., 2009]
Reference
   sofa: _InitialView
   begin: 9467
   end: 9485
   refId: "R9"
   refType: "bibr"
[Liang et al., 2009]
Reference
   sofa: _InitialView
   begin: 9487
   end: 9505
   refId: "R18"
   refType: "bibr"
[Ganchev et al., 2010]
Reference
   sofa: _InitialView
   begin: 9507
   end: 9527
   refId: "R13"
   refType: "bibr"
[ We know of no previous work that explicitly addresses the problem of providing automatic assistance for ECA; however, corpus developers have naturally gravitated to- wards the solution of adapting knowledge from the data in out-of-date versions.]
Sentence
   sofa: _InitialView
   begin: 9529
   end: 9775
[ For example, the creators of the Penn Treebank corpus used an altered version of the Brown Corpus’s annotation scheme, which can be seen as an example of a single large step in the iterative process of ECA (Marcus et al., 1993).]
Sentence
   sofa: _InitialView
   begin: 9775
   end: 10004
[Marcus et al., 1993]
Reference
   sofa: _InitialView
   begin: 9983
   end: 10002
   refId: "R19"
   refType: "bibr"
[ Although the existing Brown Corpus annotations were unsuitable for direct use, the creators of the Penn Treebank used an automatic tagging model trained on heuristically modified Brown Corpus data to automatically pre-annotate Penn Treebank data.]
Sentence
   sofa: _InitialView
   begin: 10004
   end: 10251
[ Although im- perfect, these pre-annotations effectively doubled annotation speed, greatly reducing annotation cost (Francis and Kucera, 1979; Church, 1988).]
Sentence
   sofa: _InitialView
   begin: 10251
   end: 10408
[Francis and Kucera, 1979]
Reference
   sofa: _InitialView
   begin: 10368
   end: 10392
   refId: "R11"
   refType: "bibr"
[Church, 1988]
Reference
   sofa: _InitialView
   begin: 10394
   end: 10406
   refId: "R4"
   refType: "bibr"
[(b) Exploratory corpus annotation (ECA) Ann = annotation generation Rev = revising the annotation scheme]
Paragraph
   sofa: _InitialView
   begin: 10408
   end: 10512
[(b) Exploratory corpus annotation (ECA) Ann = annotation generation Rev = revising the annotation scheme]
Sentence
   sofa: _InitialView
   begin: 10408
   end: 10512
[3. ECA as Transfer Learning]
Paragraph
   sofa: _InitialView
   begin: 10512
   end: 10539
[3. ECA as Transfer Learning]
Sentence
   sofa: _InitialView
   begin: 10512
   end: 10539
[We formally define the problem of providing machine assistance in the setting of exploratory corpus annotation as a transfer learning problem and introduce some simple solutions adapted from previous work. The solutions described below are appealing since they are conceptually simple and easy to implement using existing models as building blocks.]
Paragraph
   sofa: _InitialView
   begin: 10539
   end: 10887
[We formally define the problem of providing machine assistance in the setting of exploratory corpus annotation as a transfer learning problem and introduce some simple solutions adapted from previous work.]
Sentence
   sofa: _InitialView
   begin: 10539
   end: 10744
[ The solutions described below are appealing since they are conceptually simple and easy to implement using existing models as building blocks.]
Sentence
   sofa: _InitialView
   begin: 10744
   end: 10887
[Definition 2 (Exploratory Corpus Annotation) The transfer learning setting in which the following are true. There are multiple source tasks T 1..t−1 . For each pair i, j of source tasks, D i = D j and T i = T j . Finally, each source version has at least some labeled data. Little or no labeled data is available for the target version V t .]
Paragraph
   sofa: _InitialView
   begin: 10887
   end: 11228
[Definition 2 (Exploratory Corpus Annotation) The transfer learning setting in which the following are true.]
Sentence
   sofa: _InitialView
   begin: 10887
   end: 10994
[ There are multiple source tasks T 1..t−1 .]
Sentence
   sofa: _InitialView
   begin: 10994
   end: 11037
[ For each pair i, j of source tasks, D i = D j and T i = T j .]
Sentence
   sofa: _InitialView
   begin: 11037
   end: 11099
[ Finally, each source version has at least some labeled data.]
Sentence
   sofa: _InitialView
   begin: 11099
   end: 11160
[ Little or no labeled data is available for the target version V t .]
Sentence
   sofa: _InitialView
   begin: 11160
   end: 11228
[Only a few of the possible transfer learning settings are commonly studied, and none of those match Definition 2. ECA is unusual and interesting from a transfer learning point of view because it has multiple source tasks and there is often a sequential relationship among the tasks.]
Paragraph
   sofa: _InitialView
   begin: 11228
   end: 11510
[Only a few of the possible transfer learning settings are commonly studied, and none of those match Definition 2.]
Sentence
   sofa: _InitialView
   begin: 11228
   end: 11341
[ ECA is unusual and interesting from a transfer learning point of view because it has multiple source tasks and there is often a sequential relationship among the tasks.]
Sentence
   sofa: _InitialView
   begin: 11341
   end: 11510
[3.1. Baselines: TGTTRAIN and ALLTRAIN]
Paragraph
   sofa: _InitialView
   begin: 11510
   end: 11547
[3.1. Baselines: TGTTRAIN and ALLTRAIN]
Sentence
   sofa: _InitialView
   begin: 11510
   end: 11547
[Let TGTTRAIN be the approach that ignores all data from source tasks and trains a traditional supervised classification model only on the current target data V t . We can expect TGTTRAIN to do well when V t is large and badly when V t is small, such as at the beginning of a project or just after a change is made to the annotation scheme. TGTTRAIN corresponds to annotation projects that discard out- dated annotations when a new version is introduced. In practice, this tends to happen during the initial stages of a project, when the perceived value of the information being lost is low. Let ALLTRAIN be the algorithm that trains a traditional supervised classification model on all available data V 1...t , ignoring version boundaries. We would expect ALLTRAIN to do well when there are few differences between the source and target datasets, and badly when there are large differences.]
Paragraph
   sofa: _InitialView
   begin: 11547
   end: 12437
[Let TGTTRAIN be the approach that ignores all data from source tasks and trains a traditional supervised classification model only on the current target data V t .]
Sentence
   sofa: _InitialView
   begin: 11547
   end: 11710
[ We can expect TGTTRAIN to do well when V t is large and badly when V t is small, such as at the beginning of a project or just after a change is made to the annotation scheme.]
Sentence
   sofa: _InitialView
   begin: 11710
   end: 11886
[ TGTTRAIN corresponds to annotation projects that discard out- dated annotations when a new version is introduced.]
Sentence
   sofa: _InitialView
   begin: 11886
   end: 12000
[ In practice, this tends to happen during the initial stages of a project, when the perceived value of the information being lost is low.]
Sentence
   sofa: _InitialView
   begin: 12000
   end: 12137
[ Let ALLTRAIN be the algorithm that trains a traditional supervised classification model on all available data V 1...t , ignoring version boundaries.]
Sentence
   sofa: _InitialView
   begin: 12137
   end: 12286
[ We would expect ALLTRAIN to do well when there are few differences between the source and target datasets, and badly when there are large differences.]
Sentence
   sofa: _InitialView
   begin: 12286
   end: 12437
[3.2. STACK]
Paragraph
   sofa: _InitialView
   begin: 12437
   end: 12447
[3.2. STACK]
Sentence
   sofa: _InitialView
   begin: 12437
   end: 12447
[STACK refers to an adaptation of stacked generalization, in which traditional supervised models are trained on each of the datasets, and a higher level model is trained to accomplish the target task using the predictions of the lower level models as features (Wolpert, 1992). The higher level model can potentially discover patterns in the errors of the underlying models in order to know which are trustworthy in which contexts, and whether their guesses are wrong in ways that can be predictably mapped to the correct answer.]
Paragraph
   sofa: _InitialView
   begin: 12447
   end: 12974
[STACK refers to an adaptation of stacked generalization, in which traditional supervised models are trained on each of the datasets, and a higher level model is trained to accomplish the target task using the predictions of the lower level models as features (Wolpert, 1992).]
Sentence
   sofa: _InitialView
   begin: 12447
   end: 12722
[Wolpert, 1992]
Reference
   sofa: _InitialView
   begin: 12707
   end: 12720
   refId: "R29"
   refType: "bibr"
[ The higher level model can potentially discover patterns in the errors of the underlying models in order to know which are trustworthy in which contexts, and whether their guesses are wrong in ways that can be predictably mapped to the correct answer.]
Sentence
   sofa: _InitialView
   begin: 12722
   end: 12974
[3.3. AUGMENT]
Paragraph
   sofa: _InitialView
   begin: 12974
   end: 12986
[3.3. AUGMENT]
Sentence
   sofa: _InitialView
   begin: 12974
   end: 12986
[AUGMENT is a simple and effective domain adaptation technique proposed by Daumé (2007). AUGMENT moves the data into a feature space that allows traditional supervised learning techniques to find commonalities and differences among data from different domains. It is assumed that there are two datasets: the source X s and the target X t . Each source feature vector is mapped into the new feature space by the kernel function Φ s (x) = x, x, 0 , and each target feature vector is mapped by the function Φ t (x) = x, 0, x . Thus each feature has a source-specific version, a target-specific version, and a general version in the new feature space. This can be generalized to the context of multiple source domains by defining Φ s1 (x) = x, 0, 0, ..., x , Φ s2 (x) = 0, x, 0, ..., x ,...,Φ t (x) = 0, 0, ..., x, x .]
Paragraph
   sofa: _InitialView
   begin: 12986
   end: 13799
[AUGMENT is a simple and effective domain adaptation technique proposed by Daumé (2007).]
Sentence
   sofa: _InitialView
   begin: 12986
   end: 13073
[Daumé (2007)]
Reference
   sofa: _InitialView
   begin: 13060
   end: 13072
   refId: "R8"
   refType: "bibr"
[ AUGMENT moves the data into a feature space that allows traditional supervised learning techniques to find commonalities and differences among data from different domains.]
Sentence
   sofa: _InitialView
   begin: 13073
   end: 13245
[ It is assumed that there are two datasets: the source X s and the target X t .]
Sentence
   sofa: _InitialView
   begin: 13245
   end: 13324
[ Each source feature vector is mapped into the new feature space by the kernel function Φ s (x) = x, x, 0 , and each target feature vector is mapped by the function Φ t (x) = x, 0, x .]
Sentence
   sofa: _InitialView
   begin: 13324
   end: 13508
[ Thus each feature has a source-specific version, a target-specific version, and a general version in the new feature space.]
Sentence
   sofa: _InitialView
   begin: 13508
   end: 13632
[ This can be generalized to the context of multiple source domains by defining Φ s1 (x) = x, 0, 0, ..., x , Φ s2 (x) = 0, x, 0, ..., x ,...,Φ t (x) = 0, 0, ..., x, x .]
Sentence
   sofa: _InitialView
   begin: 13632
   end: 13799
[4. Experiments]
Paragraph
   sofa: _InitialView
   begin: 13799
   end: 13813
[4. Experiments]
Sentence
   sofa: _InitialView
   begin: 13799
   end: 13813
[We would like to test the hypothesis that transfer learning can improve pre-annotations for ECA. However, evaluating a model in this setting requires access to corpora that recorded every version of the data V since the beginning of the project. We are aware of no such datasets. However, we can simulate such corpora by starting with an existing annotated corpus and probabilistically generating sequences of intermediate versions that explain how that corpus’s annotation scheme might have come to be. For example, to start in familiar territory, we use the following process to create versioned datasets explaining the derivation of the Penn Treebank’s part-of-speech tagged data.]
Paragraph
   sofa: _InitialView
   begin: 13813
   end: 14496
[We would like to test the hypothesis that transfer learning can improve pre-annotations for ECA.]
Sentence
   sofa: _InitialView
   begin: 13813
   end: 13909
[ However, evaluating a model in this setting requires access to corpora that recorded every version of the data V since the beginning of the project.]
Sentence
   sofa: _InitialView
   begin: 13909
   end: 14058
[ We are aware of no such datasets.]
Sentence
   sofa: _InitialView
   begin: 14058
   end: 14092
[ However, we can simulate such corpora by starting with an existing annotated corpus and probabilistically generating sequences of intermediate versions that explain how that corpus’s annotation scheme might have come to be.]
Sentence
   sofa: _InitialView
   begin: 14092
   end: 14316
[ For example, to start in familiar territory, we use the following process to create versioned datasets explaining the derivation of the Penn Treebank’s part-of-speech tagged data.]
Sentence
   sofa: _InitialView
   begin: 14316
   end: 14496
[Algorithm 1 Simulate Versioned POS Datasets Given: GoldData is the reference dataset Given: GoldT ags is the reference tagset 1: T ags ← CompositeT ag(GoldT ags) 2: dataset ← {} 3: while T ags = GoldTags do 4: op ← sample({SPLIT,MOVE,MERGE}) 5: T ags ← apply(op, T ags) 6: κ ← sampleV ersionSize() 7: dataset ← annotate(κ, T ags, GoldData) 8: return dataset]
Paragraph
   sofa: _InitialView
   begin: 14496
   end: 14853
[Algorithm 1 Simulate Versioned POS Datasets Given: GoldData is the reference dataset Given: GoldT ags is the reference tagset 1: T ags ← CompositeT ag(GoldT ags) 2: dataset ← {} 3: while T ags = GoldTags do 4: op ← sample({SPLIT,MOVE,MERGE}) 5: T ags ← apply(op, T ags) 6: κ ← sampleV ersionSize() 7: dataset ← annotate(κ, T ags, GoldData) 8: return dataset]
Sentence
   sofa: _InitialView
   begin: 14496
   end: 14853
[Algorithm 1 starts by grouping all the reference tags into a single composite tagset, then iterates between alter- ing the tagset and annotating data until the original tagset is reached. A SPLIT represents an annotator deciding that the largest composite tag in the tagset is too broad and di- viding it. A MERGE represents an annotator deciding that the distinctions between two tags are too fine and lumping them together. A MOVE represents an annotator moving one of the reference tags out of one composite tag and into another; in other words, deciding that a set of words that was previously labeled as something would be better labeled something else. In order to be linguistically reasonable, splits are de- termined by finding a min-cut in the graph of reference tags, where reference tags are connected with strong ad- hoc weights if they are in the same family of tags (e.g., nouns, verbs, punctuation, etc), and weak weights other- wise. Merges and moves are chosen by sampling from a distribution over reference tag pairs where pairs that are identified by the Penn Treebank tagging guidelines as confusable (25 of these) or very confusable (9 of these) are more probable (Santorini, 1990). Finally, sampleV ersionSize() is implemented by hypothesizing that annotation projects alternate between small annotation batches for development and large batches for production approaching the size of the desired corpus, N , as the tagset converges on the reference set. The final mix favors development mode, since production mode in- volves heavy costs in the later stages (Figure 2).]
Paragraph
   sofa: _InitialView
   begin: 14853
   end: 16445
[Algorithm 1 starts by grouping all the reference tags into a single composite tagset, then iterates between alter- ing the tagset and annotating data until the original tagset is reached.]
Sentence
   sofa: _InitialView
   begin: 14853
   end: 15040
[ A SPLIT represents an annotator deciding that the largest composite tag in the tagset is too broad and di- viding it.]
Sentence
   sofa: _InitialView
   begin: 15040
   end: 15158
[ A MERGE represents an annotator deciding that the distinctions between two tags are too fine and lumping them together.]
Sentence
   sofa: _InitialView
   begin: 15158
   end: 15278
[ A MOVE represents an annotator moving one of the reference tags out of one composite tag and into another; in other words, deciding that a set of words that was previously labeled as something would be better labeled something else.]
Sentence
   sofa: _InitialView
   begin: 15278
   end: 15511
[ In order to be linguistically reasonable, splits are de- termined by finding a min-cut in the graph of reference tags, where reference tags are connected with strong ad- hoc weights if they are in the same family of tags (e.g., nouns, verbs, punctuation, etc), and weak weights other- wise.]
Sentence
   sofa: _InitialView
   begin: 15511
   end: 15802
[ Merges and moves are chosen by sampling from a distribution over reference tag pairs where pairs that are identified by the Penn Treebank tagging guidelines as confusable (25 of these) or very confusable (9 of these) are more probable (Santorini, 1990).]
Sentence
   sofa: _InitialView
   begin: 15802
   end: 16056
[Santorini, 1990]
Reference
   sofa: _InitialView
   begin: 16039
   end: 16054
   refId: "R25"
   refType: "bibr"
[ Finally, sampleV ersionSize() is implemented by hypothesizing that annotation projects alternate between small annotation batches for development and large batches for production approaching the size of the desired corpus, N , as the tagset converges on the reference set.]
Sentence
   sofa: _InitialView
   begin: 16056
   end: 16329
[ The final mix favors development mode, since production mode in- volves heavy costs in the later stages (Figure 2).]
Sentence
   sofa: _InitialView
   begin: 16329
   end: 16445
[Figure 2]
Reference
   sofa: _InitialView
   begin: 16435
   end: 16443
   refId: "F2"
   refType: "fig"
[This simulated data is clearly not ideal, and we and are actively working on developing real-world ECA data based on annotation projects we are involved in (Felt et al., In Press). However, in the meantime, simulated data allows us to make cautious observations about the characteristics of the problem and projections about the potential of transfer learning models to improve pre-annotation for ECA. We used Algorithm 1 to generate 30 diverse datasets, choosing values for the simulation parameters at random. We used maximum-entropy Markov models (“maxent tag- gers”) with standard features (Toutanova et al., 2003) to implement the transfer algorithms described in Section 3.. Figure 3 shows the learning curve of each algorithm on a single dataset. TGTTRAIN’s learning curve shows deep valleys at each version transition, because it is equivalent to beginning an entirely new learning curve at the beginning of each version. ALLTRAIN, on the other hand, shows a much smoother pattern. Using old data allows it to avoid the valleys seen in TGTTRAIN, but hurts its ability to reach high peaks quickly. STACK and AUGMENT both have peaks similar to TGTTRAIN, but manage to recover more quickly from version changes and avoid the low valleys.]
Paragraph
   sofa: _InitialView
   begin: 16445
   end: 17687
[This simulated data is clearly not ideal, and we and are actively working on developing real-world ECA data based on annotation projects we are involved in (Felt et al., In Press).]
Sentence
   sofa: _InitialView
   begin: 16445
   end: 16625
[ However, in the meantime, simulated data allows us to make cautious observations about the characteristics of the problem and projections about the potential of transfer learning models to improve pre-annotation for ECA.]
Sentence
   sofa: _InitialView
   begin: 16625
   end: 16846
[ We used Algorithm 1 to generate 30 diverse datasets, choosing values for the simulation parameters at random.]
Sentence
   sofa: _InitialView
   begin: 16846
   end: 16956
[We used maximum-entropy Markov models (“maxent tag- gers”) with standard features (Toutanova et al., 2003) to implement the transfer algorithms described in Section 3..]
Sentence
   sofa: _InitialView
   begin: 16957
   end: 17125
[Toutanova et al., 2003]
Reference
   sofa: _InitialView
   begin: 17040
   end: 17062
   refId: "R28"
   refType: "bibr"
[ Figure 3 shows the learning curve of each algorithm on a single dataset.]
Sentence
   sofa: _InitialView
   begin: 17125
   end: 17198
[Figure 3]
Reference
   sofa: _InitialView
   begin: 17126
   end: 17134
   refId: "F3"
   refType: "fig"
[ TGTTRAIN’s learning curve shows deep valleys at each version transition, because it is equivalent to beginning an entirely new learning curve at the beginning of each version.]
Sentence
   sofa: _InitialView
   begin: 17198
   end: 17374
[ ALLTRAIN, on the other hand, shows a much smoother pattern.]
Sentence
   sofa: _InitialView
   begin: 17374
   end: 17434
[ Using old data allows it to avoid the valleys seen in TGTTRAIN, but hurts its ability to reach high peaks quickly.]
Sentence
   sofa: _InitialView
   begin: 17434
   end: 17549
[ STACK and AUGMENT both have peaks similar to TGTTRAIN, but manage to recover more quickly from version changes and avoid the low valleys.]
Sentence
   sofa: _InitialView
   begin: 17549
   end: 17687
[Because we are interested in a model that performs well at all stages of the ECA process, we need to compare entire learning curves rather than just final accuracy. A natural summary statistic for the quality of a learning curve is average area under the curve (AAUC). An accurate estimate of AAUC requires good resolution on the learning curve, so we evaluate between 500 and 1000 points on each learning curve, sampling more densely around version transitions. In Table 1 we report the average AAUC of each algorithm over all the simulated datasets, along with model timing statistics. STACK and AUGMENT significantly outperform both TGTTRAIN and ALLTRAIN. Notice that AUGMENT is particularly slow to train, since its feature space has been expanded linearly in K, the number of versions. STACK is unusually slow at inference time, since it must solicit predictions from K subordinate models as features. The fact that TGTTRAIN was relatively easy to beat is encour- aging, suggesting that there is room for more sophisticated transfer learning to make significant improvements over the baseline approach.]
Paragraph
   sofa: _InitialView
   begin: 17687
   end: 18794
[Because we are interested in a model that performs well at all stages of the ECA process, we need to compare entire learning curves rather than just final accuracy.]
Sentence
   sofa: _InitialView
   begin: 17687
   end: 17851
[ A natural summary statistic for the quality of a learning curve is average area under the curve (AAUC).]
Sentence
   sofa: _InitialView
   begin: 17851
   end: 17955
[ An accurate estimate of AAUC requires good resolution on the learning curve, so we evaluate between 500 and 1000 points on each learning curve, sampling more densely around version transitions.]
Sentence
   sofa: _InitialView
   begin: 17955
   end: 18149
[ In Table 1 we report the average AAUC of each algorithm over all the simulated datasets, along with model timing statistics.]
Sentence
   sofa: _InitialView
   begin: 18149
   end: 18274
[Table 1]
Reference
   sofa: _InitialView
   begin: 18153
   end: 18160
   refId: "T1"
   refType: "table"
[ STACK and AUGMENT significantly outperform both TGTTRAIN and ALLTRAIN.]
Sentence
   sofa: _InitialView
   begin: 18274
   end: 18345
[ Notice that AUGMENT is particularly slow to train, since its feature space has been expanded linearly in K, the number of versions.]
Sentence
   sofa: _InitialView
   begin: 18345
   end: 18477
[ STACK is unusually slow at inference time, since it must solicit predictions from K subordinate models as features.]
Sentence
   sofa: _InitialView
   begin: 18477
   end: 18593
[ The fact that TGTTRAIN was relatively easy to beat is encour- aging, suggesting that there is room for more sophisticated transfer learning to make significant improvements over the baseline approach.]
Sentence
   sofa: _InitialView
   begin: 18593
   end: 18794
[(a) Development: Noisy Gaussian Size N Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqqqqqqq qq qq qqqqq 5 10 15 20 25 30 35 40 45 (b) Production: Number Noisy of Tags Sigmoid Size N qq qq qq qq q q Dataset qqq qq q q q qqqqqqqqqqqqqqqqqqqqqqqqqqqq 0 5 10 15 20 25 30 35 40 45 Number (c) Mixed of Tags Size N q q q Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqq q qqqq q qq qq q 5 10 15 20 25 30 35 40 45 Number of Simulated Composite Tags]
Paragraph
   sofa: _InitialView
   begin: 18794
   end: 19231
[(a) Development: Noisy Gaussian Size N Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqqqqqqq qq qq qqqqq 5 10 15 20 25 30 35 40 45 (b) Production: Number Noisy of Tags Sigmoid Size N qq qq qq qq q q Dataset qqq qq q q q qqqqqqqqqqqqqqqqqqqqqqqqqqqq 0 5 10 15 20 25 30 35 40 45 Number (c) Mixed of Tags Size N q q q Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqq q qqqq q qq qq q 5 10 15 20 25 30 35 40 45 Number of Simulated Composite Tags]
Sentence
   sofa: _InitialView
   begin: 18794
   end: 19231
[5. Conclusions and Future Work]
Paragraph
   sofa: _InitialView
   begin: 19231
   end: 19261
[5. Conclusions and Future Work]
Sentence
   sofa: _InitialView
   begin: 19231
   end: 19261
[We have described the problem of providing automatic assistance to annotators working in exploratory settings. We have argued that this problem should be regarded as a transfer learning problem, and shown that existing transfer learning techniques can be adapted to significantly improve the quality of pre-annotations in simulated exploratory part- of-speech tagging. Corpus annotators working in novel an-]
Paragraph
   sofa: _InitialView
   begin: 19261
   end: 19668
[We have described the problem of providing automatic assistance to annotators working in exploratory settings.]
Sentence
   sofa: _InitialView
   begin: 19261
   end: 19371
[ We have argued that this problem should be regarded as a transfer learning problem, and shown that existing transfer learning techniques can be adapted to significantly improve the quality of pre-annotations in simulated exploratory part- of-speech tagging.]
Sentence
   sofa: _InitialView
   begin: 19371
   end: 19629
[ Corpus annotators working in novel an-]
Sentence
   sofa: _InitialView
   begin: 19629
   end: 19668
[(a) TGTTRAIN 1 Accuracy 0 50 200 500 2000 10000 (b) Words ALLTRAIN (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (c) STACK (logscale) 1 Accuracy 0 50 200 500 2000 10000 (d) Words AUGMENT (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (logscale) Figure 3: An example learning curve for each algorithm on the same dataset.]
Paragraph
   sofa: _InitialView
   begin: 19668
   end: 19999
[(a) TGTTRAIN 1 Accuracy 0 50 200 500 2000 10000 (b) Words ALLTRAIN (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (c) STACK (logscale) 1 Accuracy 0 50 200 500 2000 10000 (d) Words AUGMENT (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (logscale) Figure 3: An example learning curve for each algorithm on the same dataset.]
Sentence
   sofa: _InitialView
   begin: 19668
   end: 19999
[Figure 3]
Reference
   sofa: _InitialView
   begin: 19924
   end: 19932
   refId: "F3"
   refType: "fig"
[notation domains should be encouraged by these results and by the existence of a rich body of transfer learning work to draw on. We plan to develop models that leverage the sequential nature of the versions. We also plan to apply the insights developed in this paper to improve pre-annotations for annotators engaged in real-world annotation projects. Finally, in order to apply these techniques seamlessly in annotation projects, it would be beneficial to discover a way of learning to automatically identify the boundaries between versions so that annotators need not manually identify annotation scheme changes.]
Paragraph
   sofa: _InitialView
   begin: 19999
   end: 20613
[notation domains should be encouraged by these results and by the existence of a rich body of transfer learning work to draw on.]
Sentence
   sofa: _InitialView
   begin: 19999
   end: 20127
[ We plan to develop models that leverage the sequential nature of the versions.]
Sentence
   sofa: _InitialView
   begin: 20127
   end: 20206
[ We also plan to apply the insights developed in this paper to improve pre-annotations for annotators engaged in real-world annotation projects.]
Sentence
   sofa: _InitialView
   begin: 20206
   end: 20350
[ Finally, in order to apply these techniques seamlessly in annotation projects, it would be beneficial to discover a way of learning to automatically identify the boundaries between versions so that annotators need not manually identify annotation scheme changes.]
Sentence
   sofa: _InitialView
   begin: 20350
   end: 20613
[6. References]
Paragraph
   sofa: _InitialView
   begin: 20613
   end: 20626
[6. References]
Sentence
   sofa: _InitialView
   begin: 20613
   end: 20626
[50000]
Paragraph
   sofa: _InitialView
   begin: 20626
   end: 20631
[50000]
Sentence
   sofa: _InitialView
   begin: 20626
   end: 20631
[50000]
Paragraph
   sofa: _InitialView
   begin: 20631
   end: 20636
[50000]
Sentence
   sofa: _InitialView
   begin: 20631
   end: 20636
[50000]
Paragraph
   sofa: _InitialView
   begin: 20636
   end: 20641
[50000]
Sentence
   sofa: _InitialView
   begin: 20636
   end: 20641
[50000]
Paragraph
   sofa: _InitialView
   begin: 20641
   end: 20646
[50000]
Sentence
   sofa: _InitialView
   begin: 20641
   end: 20646
-------- View _InitialView end ----------------------------------

======== CAS 0 end ==================================


