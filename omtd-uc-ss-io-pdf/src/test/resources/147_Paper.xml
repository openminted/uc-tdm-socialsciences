<?xml version='1.0' encoding='UTF-8'?>
<pdfx xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://pdfx.cs.man.ac.uk/static/article-schema.xsd">
  <meta>
    <job>edc3184b2de7049efd9e457f8c1ed6dc08c19f3c03422f206ebff721c01a9fe8</job>
    <base_name>3z</base_name>
    <doi confidence="possible" alt_doi="http://dx.doi.org/10.17562/pb-53-5">http://dx.doi.org/10.1007/978-3-540-78646-7_19</doi>
    <warning>Name identification was not possible. </warning>
  </meta>
  <article>
    <front class="DoCO:FrontMatter">
      <title-group>
        <article-title class="DoCO:Title" id="2"><s id="1" sid="1">Using Transfer Learning to Assist Exploratory Corpus Annotation</s></article-title>
      </title-group>
      <abstract class="DoCO:Abstract" id="15"><s id="9" sid="2">We describe an under-studied problem in language resource management: that of providing automatic assistance to annotators working in exploratory settings.</s><s id="10" sid="3"> When no satisfactory tagset already exists, such as in under-resourced or undocumented languages, it must be developed iteratively while annotating data.</s><s id="11" sid="4"> This process naturally gives rise to a sequence of datasets, each annotated differently.</s><s id="12" sid="5"> We argue that this problem is best regarded as a transfer learning problem with multiple source tasks.</s><s id="13" sid="6"> Using part-of-speech tagging data with simulated exploratory tagsets, we demonstrate that even simple transfer learning techniques can significantly improve the quality of pre-annotations in an exploratory annotation.</s><s id="14" sid="7"> Keywords: corpus annotation, transfer learning, machine learning</s></abstract>
    </front>
    <body class="DoCO:BodyMatter">
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="3" page="1" column="1">Paul Felt, Eric K. Ringger, Kevin D. Seppi, Kristian Heal</h1>
        <region class="DoCO:TextChunk" id="8" confidence="possible" page="1" column="1"><s id="4" sid="8">Brigham Young University Department of Computer Science, † Neal A. Maxwell Institute Provo, UT 84602 USA paul <email id="5">felt@byu.edu</email>, {ringger, <email id="6">kseppi}@cs.byu.edu</email>, kristian <email id="7">heal@byu.edu</email></s></region>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="16" page="1" column="1">1. Exploratory Corpus Annotation (ECA)</h1>
      </section>
      <region class="DoCO:TextChunk" id="39" page="1" column="1"><s id="17" sid="9">Because corpora are useful for investigating the struc- ture of language, studying the way that languages change over time, testing linguistic hypotheses, charting the move- ment of ideas and historical trends, and even improving the effectiveness of language teaching and acquisition, they are an essential linguistic resource (<xref ref-type="bibr" rid="R17" id="18" class="deo:Reference">Kroch, 1989</xref>; <xref ref-type="bibr" rid="R27" id="19" class="deo:Reference">Sinclair, 2004</xref>; <xref ref-type="bibr" rid="R21" id="20" class="deo:Reference">Nesselhauf, 2004</xref>).</s><s id="21" sid="10"> One of the most urgent needs for annotated corpora is in the realm of under-resourced and endangered language documentation (Grenoble and Whaley, 1998; <xref ref-type="bibr" rid="R6" id="22" class="deo:Reference">Crystal, 2002</xref>; <xref ref-type="bibr" rid="R1" id="23" class="deo:Reference">Bird and Simons, 2003</xref>; <xref ref-type="bibr" rid="R14" id="24" class="deo:Reference">Gippert et al., 2006</xref>).</s><s id="25" sid="11"> In domains such as under-resourced language documentation, annotation is unavoidably exploratory and iterative in nature (<xref ref-type="bibr" rid="R16" id="26" class="deo:Reference">Hovy and Lavid, 2010</xref>).</s><s id="27" sid="12"> The annotator proposes an annotation scheme, annotates data, and then revises that annotation scheme in light of insights gener- ated by applying the annotation scheme to real world data (<xref ref-type="fig" rid="F1" id="28" class="deo:Reference">Figure 1</xref>.),</s><s id="29" sid="13"> a process which for brevity we refer to as ECA (exploratory corpus annotation).</s><s id="30" sid="14"> ECA results in a sequence of possibly disjoint annotation sets, or “versions”, V 1 ⊕ .</s><s id="31" sid="15"> .</s><s id="32" sid="16"> .</s><s id="33" sid="17"> ⊕ V K = V , where each V v consists of data and annotations, {(x i , y i )} N i=1 v , produced according to V v ’s annotation scheme.</s><s id="34" sid="18"> Each time the annotation scheme changes, some cost is incurred as existing annotations are invalidated and must be updated before the corpus is complete.</s><s id="35" sid="19"> The cost associated with evolving annotation schemes is largely a hidden cost, since few annotation projects record or report internal changes.</s><s id="36" sid="20"> For example, the Natural Language Processing (NLP) Lab at BYU is collaborating with scholars of ancient languages at the Neal A. Maxwell Institute for Religious Scholarship to create a large corpus of annotated Classical Syriac.</s><s id="37" sid="21"> 1 Although significant time was spent at the outset defining the annotation scheme that would be used, as pre- liminary data has been annotated at least a dozen updates have already been made to the annotation scheme.</s><s id="38" sid="22"> Since</s></region>
      <region class="unknown" id="41" page="1" column="1">1 <ext-link ext-link-type="uri" href="http://cpart.maxwellinstitute.byu.edu/" id="40">http://cpart.maxwellinstitute.byu.edu/</ext-link> home/sec/</region>
      <region class="unknown" id="42" page="1" column="2">†</region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="43" page="1" column="2">Heal</h1>
      </section>
      <region class="DoCO:TextChunk" id="58" page="1" column="2"><s id="44" sid="23">we are starting with a sizable body of already annotated text, some of these changes have required considerable time and effort to implement (via re-annotation).</s><s id="45" sid="24"> Annotation scheme revisions are especially likely in exploratory annotation scenarios dealing with languages or linguistic theories that have not previously been codified into annotation schemes.</s><s id="46" sid="25"> However, revisions can occur even in well established annotation tasks such as English part of speech tagging and parsing.</s><s id="47" sid="26"> When creating the Penn Treebank corpus, <xref ref-type="bibr" rid="R19" id="48" class="deo:Reference">Marcus et al. (1993)</xref> re-annotated the Brown corpus data with revised part-of-speech tags.</s><s id="49" sid="27"> Addi- tionally, Marcus et al.</s><s id="50" sid="28"> report that after publishing the Penn Treebank, they identified a variety of limitations and incon- sistencies in their annotation scheme for English syntactic parsing and subsequently spent a good deal of effort repair- ing the parsing scheme and re-annotating data for future releases (<xref ref-type="bibr" rid="R20" id="51" class="deo:Reference">Marcus et al., 1995</xref>).</s><s id="52" sid="29"> A more extreme case comes from the SUSANNE corpus, another derivative of the Brown Corpus, annotated with very detailed parsing information.</s><s id="53" sid="30"> The 512-page book describing the SUSANNE annotation scheme required twelve years of work to finish, and the author describes the accompanying 130,000-word corpus as a “by- product of the work of creating the SUSANNE annotation scheme” (<xref ref-type="bibr" rid="R24" id="54" class="deo:Reference">Sampson, 2008</xref>).</s><s id="55" sid="31"> These examples underscore the effort involved in developing a satisfactory annotation scheme, even for mainstream languages and linguistic annotation tasks.</s><s id="56" sid="32"> The costs involved in iteratively improving an annotation scheme mean that budget-constrained corpus developers often must choose between developing a linguistically optimal annotation scheme and generating useful amounts of annotated data.</s><s id="57" sid="33"> To make matters worse, statistical pre- annotation—the traditional method of reducing annotation overhead—is hampered by the lack of a self-consistent training set.</s></region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="59" page="1" column="2">2. Previous Work</h1>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="95" page="2" column="2">2.1. Transfer Learning</h2>
          <region class="DoCO:TextChunk" id="100" page="2" column="2"><s id="96" sid="34">Using knowledge from one or more source tasks to improve performance on a target task, as the Penn Treebank developers did, is known as transfer learning, and is an area of active research within machine learning.</s><s id="97" sid="35"> Providing pre- annotations for ECA fits naturally into the transfer learning framework.</s><s id="98" sid="36"> The following definition of transfer learning borrows notation and ideas from <xref ref-type="bibr" rid="R22" id="99" class="deo:Reference">Pan and Yang (2010)</xref>, but with minor changes to highlight connections between transfer learning and the motivation presented in Section 1..</s></region>
          <region class="DoCO:TextChunk" id="105" confidence="possible" page="2" column="2"><s id="101" sid="37">Definition 1 (Transfer Learning) Let D denote a domain comprising the feature space X and a distribution p(x) over data x ∈ X .</s><s id="102" sid="38"> Let T denote a task, or annotation scheme, comprising a feature space X , a label space Y, and a labeling function f : x → y where x ∈ X and y ∈ Y.</s><s id="103" sid="39"> 2 Finally, let version V t be the set of annotations produced according to the annotation scheme of task T t .</s><s id="104" sid="40"> Then the goal of transfer learning is to use data from all source versions V 1..t−1 to improve our ability to model the target annotation scheme T t .</s></region>
          <region class="DoCO:TextChunk" id="115" page="2" column="2"><s id="106" sid="41">Definition 1 encompasses a large number of scenarios.</s><s id="107" sid="42"> There may be one or many source versions.</s><s id="108" sid="43"> Differ- ent quantities of data and annotations may be available in any given version.</s><s id="109" sid="44"> Furthermore, each version is associated with an annotator, domain, and task, and therefore may differ from other versions in terms of X , p(x), Y, or f .</s><s id="110" sid="45"> Each of these differences can be understood via simple examples.</s><s id="111" sid="46"> Text and images come from domains D 1 , D 2 where X 1 = X 2 .</s><s id="112" sid="47"> Poetry and newswire text come from domains where p 1 (x) = p 2 (x).</s><s id="113" sid="48"> When two part-of-speech tagging tasks use different tagsets, then Y 1 = Y 2 ; when they assign different meanings to the same tag, f 1 = f 2 .</s><s id="114" sid="49"> Each setting</s></region>
          <region class="DoCO:TextChunk" id="118" confidence="possible" page="2" column="2"><s id="116" sid="50">2 Although f may be approximately described in annotation manuals, the true f is generally unseen.</s><s id="117" sid="51"> In probabilistic ap- proaches, f is often modeled as p(y|x)</s></region>
          <outsider class="DoCO:TextBox" type="page_nr" id="119" page="2" column="2">141</outsider>
          <region class="DoCO:TextChunk" id="130" page="3" column="1"><s id="120" sid="52">of these variables in a versioned dataset V 1...t corresponds to a different transfer learning scenario.</s><s id="121" sid="53"> Much work on transfer learning for NLP is currently in domain adaptation, the transfer setting in which T s = T t , D s = D t , and the domains differ only in the marginal distribution of the data, p s (X) and p t (X).</s><s id="122" sid="54"> An example of domain adaptation would be using Wikipedia text in which named entities (people, places, events, etc.)</s><s id="123" sid="55"> have been labeled in order to improve named entity recognition in movie reviews.</s><s id="124" sid="56"> Other notable related work includes multi-task learning, a transfer setting in which D s = D t and there are multiple tasks that all differ from one another.</s><s id="125" sid="57"> Multi-task learning is unusual in that no source/destination distinctions made among tasks (<xref ref-type="bibr" rid="R2" id="126" class="deo:Reference">Caruana, 1997</xref>).</s><s id="127" sid="58"> For example, <xref ref-type="bibr" rid="R5" id="128" class="deo:Reference">Collobert et al. (2011)</xref> construct a system that simultaneously learns part- of-speech tagging, named entity recognition, chunking, and semantic role labeling.</s><s id="129" sid="59"> Each task helps to inform the others, leading to higher performance on all tasks learned jointly than was possible for any individual task when learned in- dividually.</s></region>
        </section>
      </section>
      <region class="DoCO:TextChunk" id="61" page="1" column="2"><s id="60" sid="60">Numerous annotation projects have shown that assist- ing annotators with good pre-annotations is essential to an-</s></region>
      <outsider class="DoCO:TextBox" type="page_nr" id="62" page="1" column="2">140</outsider>
      <region class="DoCO:FigureBox" id="Fx63">
        <image class="DoCO:Figure" src="3z.page_002.image_03.png" thmb="3z.page_002.image_03-thumb.png"/>
        <image class="DoCO:Figure" src="3z.page_002.image_02.png" thmb="3z.page_002.image_02-thumb.png"/>
        <image class="DoCO:Figure" src="3z.page_002.image_01.png" thmb="3z.page_002.image_01-thumb.png"/>
      </region>
      <region class="DoCO:TextChunk" id="65" confidence="possible" page="2" column="1"><s id="64" sid="61">(a) Corpus annotation with a pre-defined, un- changing annotation scheme</s></region>
      <region class="DoCO:FigureBox" id="F1">
        <caption class="deo:Caption" id="67" page="2" column="1"><s id="66" sid="62">Figure 1: Two Kinds of Corpus Annotation</s></caption>
      </region>
      <region class="DoCO:TextChunk" id="92" page="2" column="1"><s id="68" sid="63">notator efficiency and accuracy.</s><s id="69" sid="64"> Studies in English part- of-speech tagging, Chinese parsing, information extraction, named entity recognition, and Semitic morphological analysis all demonstrate that high accuracy pre-annotations cor- relate strongly with low annotation cost (<xref ref-type="bibr" rid="R19" id="70" class="deo:Reference">Marcus et al., 1993</xref>; <xref ref-type="bibr" rid="R3" id="71" class="deo:Reference">Chiou et al., 2001</xref>; <xref ref-type="bibr" rid="R7" id="72" class="deo:Reference">Culotta and McCallum, 2005</xref>; <xref ref-type="bibr" rid="R12" id="73" class="deo:Reference">Ganchev et al., 2007</xref>; <xref ref-type="bibr" rid="R10" id="74" class="deo:Reference">Felt et al., 2012</xref>).</s><s id="75" sid="65"> This point is critical to our future decision (see Section 4.)</s><s id="76" sid="66"> to focus on increasing model accuracy as a stand-in for reduced cost.</s><s id="77" sid="67"> Because accurate pre-annotation models are so effective in reducing annotation effort, much work has been done to train high quality models with as little data as possible.</s><s id="78" sid="68"> The active learning literature aims to reduce the cost required to train a model by selecting instances for annotation that are likely to be most informative (<xref ref-type="bibr" rid="R26" id="79" class="deo:Reference">Settles, 2010</xref>).</s><s id="80" sid="69"> Weakly supervised techniques attempt to speed model training by learning from unlabeled instances, or by allowing annotators to communicate their knowledge to the model by specifying labels or constraints that are applicable to large classes of data instances (<xref ref-type="bibr" rid="R23" id="81" class="deo:Reference">Roth and Yih, 2004</xref>; <xref ref-type="bibr" rid="R9" id="82" class="deo:Reference">Druck et al., 2009</xref>; <xref ref-type="bibr" rid="R18" id="83" class="deo:Reference">Liang et al., 2009</xref>; <xref ref-type="bibr" rid="R13" id="84" class="deo:Reference">Ganchev et al., 2010</xref>).</s><s id="85" sid="70"> We know of no previous work that explicitly addresses the problem of providing automatic assistance for ECA; however, corpus developers have naturally gravitated to- wards the solution of adapting knowledge from the data in out-of-date versions.</s><s id="86" sid="71"> For example, the creators of the Penn Treebank corpus used an altered version of the Brown Corpus’s annotation scheme, which can be seen as an example of a single large step in the iterative process of ECA (<xref ref-type="bibr" rid="R19" id="87" class="deo:Reference">Marcus et al., 1993</xref>).</s><s id="88" sid="72"> Although the existing Brown Corpus annotations were unsuitable for direct use, the creators of the Penn Treebank used an automatic tagging model trained on heuristically modified Brown Corpus data to automatically pre-annotate Penn Treebank data.</s><s id="89" sid="73"> Although im- perfect, these pre-annotations effectively doubled annotation speed, greatly reducing annotation cost (<xref ref-type="bibr" rid="R11" id="90" class="deo:Reference">Francis and Kucera, 1979</xref>; <xref ref-type="bibr" rid="R4" id="91" class="deo:Reference">Church, 1988</xref>).</s></region>
      <region class="DoCO:TextChunk" id="94" confidence="possible" page="2" column="2"><s id="93" sid="74">(b) Exploratory corpus annotation (ECA) Ann = annotation generation Rev = revising the annotation scheme</s></region>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="131" page="3" column="1">3. ECA as Transfer Learning</h1>
        <region class="DoCO:TextChunk" id="134" page="3" column="1"><s id="132" sid="75">We formally define the problem of providing machine assistance in the setting of exploratory corpus annotation as a transfer learning problem and introduce some simple solutions adapted from previous work.</s><s id="133" sid="76"> The solutions described below are appealing since they are conceptually simple and easy to implement using existing models as building blocks.</s></region>
        <region class="DoCO:TextChunk" id="140" confidence="possible" page="3" column="1"><s id="135" sid="77">Definition 2 (Exploratory Corpus Annotation) The transfer learning setting in which the following are true.</s><s id="136" sid="78"> There are multiple source tasks T 1..t−1 .</s><s id="137" sid="79"> For each pair i, j of source tasks, D i = D j and T i = T j .</s><s id="138" sid="80"> Finally, each source version has at least some labeled data.</s><s id="139" sid="81"> Little or no labeled data is available for the target version V t .</s></region>
        <region class="DoCO:TextChunk" id="143" page="3" column="1"><s id="141" sid="82">Only a few of the possible transfer learning settings are commonly studied, and none of those match Definition 2.</s><s id="142" sid="83"> ECA is unusual and interesting from a transfer learning point of view because it has multiple source tasks and there is often a sequential relationship among the tasks.</s></region>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="144" page="3" column="1">3.1. Baselines: TGTTRAIN and ALLTRAIN</h2>
          <region class="DoCO:TextChunk" id="152" page="3" column="1"><s id="145" sid="84">Let TGTTRAIN be the approach that ignores all data from source tasks and trains a traditional supervised classification model only on the current target data V t .</s><s id="146" sid="85"> We can expect TGTTRAIN to do well when V t is large and badly when V t is small, such as at the beginning of a project or just after a change is made to the annotation scheme.</s><s id="147" sid="86"> TGTTRAIN corresponds to annotation projects that discard out- dated annotations when a new version is introduced.</s><s id="148" sid="87"> In practice, this tends to happen during the initial stages of a project, when the perceived value of the information being lost is low.</s><s id="149" sid="88"> Let ALLTRAIN be the algorithm that trains a traditional supervised classification model on all available data V 1...t , ignoring version boundaries.</s><s id="150" sid="89"> We would expect ALLTRAIN to do well when there are few differences between<marker type="column" number="2"/><marker type="block"/> the source and target datasets, and badly when there are large differences.</s></region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="153" page="3" column="2">3.2. STACK</h2>
          <region class="DoCO:TextChunk" id="157" page="3" column="2"><s id="154" sid="90">STACK refers to an adaptation of stacked generalization, in which traditional supervised models are trained on each of the datasets, and a higher level model is trained to accomplish the target task using the predictions of the lower level models as features (<xref ref-type="bibr" rid="R29" id="155" class="deo:Reference">Wolpert, 1992</xref>).</s><s id="156" sid="91"> The higher level model can potentially discover patterns in the errors of the underlying models in order to know which are trustworthy in which contexts, and whether their guesses are wrong in ways that can be predictably mapped to the correct answer.</s></region>
        </section>
        <section class="DoCO:Section">
          <h2 class="DoCO:SectionTitle" id="158" page="3" column="2">3.3. AUGMENT</h2>
          <region class="DoCO:TextChunk" id="166" page="3" column="2"><s id="159" sid="92">AUGMENT is a simple and effective domain adaptation technique proposed by <xref ref-type="bibr" rid="R8" id="160" class="deo:Reference">Daumé (2007)</xref>.</s><s id="161" sid="93"> AUGMENT moves the data into a feature space that allows traditional supervised learning techniques to find commonalities and differences among data from different domains.</s><s id="162" sid="94"> It is assumed that there are two datasets: the source X s and the target X t .</s><s id="163" sid="95"> Each source feature vector is mapped into the new feature space by the kernel function Φ s (x) = x, x, 0 , and each target feature vector is mapped by the function Φ t (x) = x, 0, x .</s><s id="164" sid="96"> Thus each feature has a source-specific version, a target-specific version, and a general version in the new feature space.</s><s id="165" sid="97"> This can be generalized to the context of multiple source domains by defining Φ s1 (x) = x, 0, 0, ..., x , Φ s2 (x) = 0, x, 0, ..., x ,...,Φ t (x) = 0, 0, ..., x, x .</s></region>
        </section>
      </section>
      <section class="DoCO:Section">
        <h1 class="DoCO:SectionTitle" id="167" page="3" column="2">4. Experiments</h1>
        <region class="DoCO:TextChunk" id="173" page="3" column="2"><s id="168" sid="98">We would like to test the hypothesis that transfer learning can improve pre-annotations for ECA.</s><s id="169" sid="99"> However, evaluating a model in this setting requires access to corpora that recorded every version of the data V since the beginning of the project.</s><s id="170" sid="100"> We are aware of no such datasets.</s><s id="171" sid="101"> However, we can simulate such corpora by starting with an existing annotated corpus and probabilistically generating sequences of intermediate versions that explain how that corpus’s annotation scheme might have come to be.</s><s id="172" sid="102"> For example, to start in familiar territory, we use the following process to create versioned datasets explaining the derivation of the Penn Treebank’s part-of-speech tagged data.</s></region>
        <region class="DoCO:TextChunk" id="175" confidence="possible" page="3" column="2"><s id="174" sid="103">Algorithm 1 Simulate Versioned POS Datasets Given: GoldData is the reference dataset Given: GoldT ags is the reference tagset 1: T ags ← CompositeT ag(GoldT ags) 2: dataset ← {} 3: while T ags = GoldTags do 4: op ← sample({SPLIT,MOVE,MERGE}) 5: T ags ← apply(op, T ags) 6: κ ← sampleV ersionSize() 7: dataset ← annotate(κ, T ags, GoldData) 8: return dataset</s></region>
        <region class="DoCO:TextChunk" id="222" page="3" column="2"><s id="176" sid="104">Algorithm 1 starts by grouping all the reference tags into a single composite tagset, then iterates between alter- ing the tagset and annotating data until the original tagset is reached.</s><s id="177" sid="105"> A SPLIT represents an annotator deciding that<marker type="page" number="4"/><marker type="column" number="1"/><marker type="block"/> the largest composite tag in the tagset is too broad and di- viding it.</s><s id="180" sid="106"> A MERGE represents an annotator deciding that the distinctions between two tags are too fine and lumping them together.</s><s id="181" sid="107"> A MOVE represents an annotator moving one of the reference tags out of one composite tag and into another; in other words, deciding that a set of words that was previously labeled as something would be better labeled something else.</s><s id="182" sid="108"> In order to be linguistically reasonable, splits are de- termined by finding a min-cut in the graph of reference tags, where reference tags are connected with strong ad- hoc weights if they are in the same family of tags (e.g., nouns, verbs, punctuation, etc), and weak weights other- wise.</s><s id="183" sid="109"> Merges and moves are chosen by sampling from a distribution over reference tag pairs where pairs that are identified by the Penn Treebank tagging guidelines as confusable (25 of these) or very confusable (9 of these) are more probable (<xref ref-type="bibr" rid="R25" id="184" class="deo:Reference">Santorini, 1990</xref>).</s><s id="185" sid="110"> Finally, sampleV ersionSize() is implemented by hypothesizing that annotation projects alternate between small annotation batches for development and large batches for production approaching the size of the desired corpus, N , as the tagset converges on the reference set.</s><s id="186" sid="111"> The final mix favors development mode, since production mode in- volves heavy costs in the later stages (<xref ref-type="fig" rid="F2" id="187" class="deo:Reference">Figure 2</xref>).</s><marker type="block"/> <s id="192" sid="112">This simulated data is clearly not ideal, and we and are actively working on developing real-world ECA data based on annotation projects we are involved in (Felt et al., In Press).</s><s id="193" sid="113"> However, in the meantime, simulated data allows us to make cautious observations about the characteristics of the problem and projections about the potential of transfer learning models to improve pre-annotation for ECA.</s><s id="194" sid="114"> We used Algorithm 1 to generate 30 diverse datasets, choosing values for the simulation parameters at random.</s><marker type="column" number="2"/><marker type="block"/> <s id="196" sid="115">We used maximum-entropy Markov models (“maxent tag- gers”) with standard features (<xref ref-type="bibr" rid="R28" id="197" class="deo:Reference">Toutanova et al., 2003</xref>) to implement the transfer algorithms described in Section 3..</s><s id="198" sid="116"> <xref ref-type="fig" rid="F3" id="199" class="deo:Reference">Figure 3</xref> shows the learning curve of each algorithm on a single dataset.</s><s id="200" sid="117"> TGTTRAIN’s learning curve shows deep valleys at each version transition, because it is equivalent to beginning an entirely new learning curve at the beginning of each version.</s><s id="201" sid="118"> ALLTRAIN, on the other hand, shows a much smoother pattern.</s><s id="202" sid="119"> Using old data allows it to avoid the valleys seen in TGTTRAIN, but hurts its ability to reach high peaks quickly.</s><s id="203" sid="120"> STACK and AUGMENT both have peaks similar to TGTTRAIN, but manage to recover more quickly from version changes and avoid the low valleys.</s><marker type="block"/> <s id="213" sid="121">Because we are interested in a model that performs well at all stages of the ECA process, we need to compare entire learning curves rather than just final accuracy.</s><s id="214" sid="122"> A natural summary statistic for the quality of a learning curve is average area under the curve (AAUC).</s><s id="215" sid="123"> An accurate estimate of AAUC requires good resolution on the learning curve, so we evaluate between 500 and 1000 points on each learning curve, sampling more densely around version transitions.</s><s id="216" sid="124"> In <xref ref-type="table" rid="T1" id="217" class="deo:Reference">Table 1</xref> we report the average AAUC of each algorithm over all the simulated datasets, along with model timing statistics.</s><s id="218" sid="125"> STACK and AUGMENT significantly outperform both TGTTRAIN and ALLTRAIN.</s><s id="219" sid="126"> Notice that AUGMENT is particularly slow to train, since its feature space has been expanded linearly in K, the number of versions.</s><s id="220" sid="127"> STACK is unusually slow at inference time, since it must solicit predictions from K subordinate models as features.</s><s id="221" sid="128"> The fact that TGTTRAIN was relatively easy to beat is encour- aging, suggesting that there is room for more sophisticated transfer learning to make significant improvements over the baseline approach.</s></region>
        <outsider class="DoCO:TextBox" type="page_nr" id="179" page="3" column="2">142</outsider>
        <region class="unknown" id="189" page="4" column="1">(a) Development: Noisy Gaussian Size N Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqqqqqqq qq qq qqqqq 5 10 15 20 25 30 35 40 45 (b) Production: Number Noisy of Tags Sigmoid Size N qq qq qq qq q q Dataset qqq qq q q q qqqqqqqqqqqqqqqqqqqqqqqqqqqq 0 5 10 15 20 25 30 35 40 45 Number (c) Mixed of Tags Size N q q q Dataset qqqqq 0 qqq qqqqqqqqqqqqqqqq q qqqqqqq q qqqq q qq qq q 5 10 15 20 25 30 35 40 45 Number of Simulated Composite Tags</region>
        <region class="DoCO:FigureBox" id="F2">
          <caption class="deo:Caption" id="191" page="4" column="1"><s id="190" sid="129">Figure 2: Example draws from sampleV ersionSize()</s></caption>
        </region>
        <region class="DoCO:TableBox" id="Tx205">
          <content>
            <table class="DoCO:Table" number="1" page="4">
              <thead class="table">
                <tr class="table">
                  <th class="table"></th>
                  <th class="table"> Heldout</th>
                  <th class="table"> Train</th>
                  <th class="table"> Eval</th>
                </tr>
              </thead>
              <tbody>
                <tr class="table.strange">
                  <td class="table.strange"></td>
                  <td class="table.strange"> AAUC</td>
                  <td class="table.strange"> Secs</td>
                  <td class="table.strange"> Secs</td>
                </tr>
                <tr class="table">
                  <td class="table"> TGTTRAIN</td>
                  <td class="table"> 225.4</td>
                  <td class="table"> 0.2</td>
                  <td class="table"> 0.003</td>
                </tr>
                <tr class="table">
                  <td class="table"> ALLTRAIN</td>
                  <td class="table"> 218.0</td>
                  <td class="table"> 7.7</td>
                  <td class="table"> 0.004</td>
                </tr>
                <tr class="table">
                  <td class="table"> STACK</td>
                  <td class="table"> 240.3</td>
                  <td class="table"> 6.5</td>
                  <td class="table"> 0.046</td>
                </tr>
                <tr class="table">
                  <td class="table"> AUGMENT</td>
                  <td class="table"> 246.4</td>
                  <td class="table"> 60.9</td>
                  <td class="table"> 0.006</td>
                </tr>
              </tbody>
            </table>
          </content>
          <region class="TableInfo" id="206" confidence="possible" page="4" column="2">Heldout Train Eval AAUC Secs Secs TGTTRAIN 225.4 0.2 0.003 ALLTRAIN 218.0 7.7 0.004 STACK 240.3 6.5 0.046 AUGMENT 246.4 60.9 0.006</region>
          <caption class="deo:Caption" id="212" page="4" column="2"><s id="207" sid="130">Table 1: Bolded accuracies are significantly (p-val&lt;0.01) better than non-bolded competitors.</s><s id="208" sid="131"> Underlined times are significantly (p-val¡0.01) worse than non-underlined competitors.</s><s id="209" sid="132"> AAUC is averaged over 30 datasets.</s><s id="210" sid="133"> “Train Secs” means model training time averaged over all datasets.</s><s id="211" sid="134"> “Eval Secs” means average seconds to infer the labeling of a single sentence.</s></caption>
        </region>
      </section>
      <section class="deo:Conclusion">
        <h1 class="DoCO:SectionTitle" id="223" page="4" column="2">5. Conclusions and Future Work</h1>
        <region class="DoCO:TextChunk" id="227" page="4" column="2"><s id="224" sid="135">We have described the problem of providing automatic assistance to annotators working in exploratory settings.</s><s id="225" sid="136"> We have argued that this problem should be regarded as a transfer learning problem, and shown that existing transfer learning techniques can be adapted to significantly improve the quality of pre-annotations in simulated exploratory part- of-speech tagging.</s><s id="226" sid="137"> Corpus annotators working in novel an-</s></region>
        <outsider class="DoCO:TextBox" type="page_nr" id="228" page="4" column="2">143</outsider>
        <region class="DoCO:TextChunk" id="231" confidence="possible" page="5" column="1"><s id="229" sid="138">(a) TGTTRAIN 1 Accuracy 0 50 200 500 2000 10000 (b) Words ALLTRAIN (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (c) STACK (logscale) 1 Accuracy 0 50 200 500 2000 10000 (d) Words AUGMENT (logscale) 1 Accuracy 0 50 200 500 2000 10000 Words (logscale) <xref ref-type="fig" rid="F3" id="230" class="deo:Reference">Figure 3</xref>: An example learning curve for each algorithm on the same dataset.</s></region>
        <region class="DoCO:TextChunk" id="236" page="5" column="1"><s id="232" sid="139">notation domains should be encouraged by these results and by the existence of a rich body of transfer learning work to draw on.</s><s id="233" sid="140"> We plan to develop models that leverage the sequential nature of the versions.</s><s id="234" sid="141"> We also plan to apply the insights developed in this paper to improve pre-annotations for annotators engaged in real-world annotation projects.</s><s id="235" sid="142"> Finally, in order to apply these techniques seamlessly in annotation projects, it would be beneficial to discover a way of learning to automatically identify the boundaries between versions so that annotators need not manually identify annotation scheme changes.</s></region>
      </section>
      <section class="DoCO:Bibliography">
        <h1 class="DoCO:SectionTitle" id="237" page="5" column="1">6. References</h1>
        <ref-list class="DoCO:BiblioGraphicReferenceList">
          <ref rid="R1" class="deo:BibliographicReference" id="238" page="5" column="1">Bird, S. and Simons, G. (2003). Seven Dimensions of Portability for Language Documentation and Descrip- tion. Language, pages 557–582.</ref>
          <ref rid="R2" class="deo:BibliographicReference" id="239" confidence="possible" page="5" column="1">Caruana, R. (1997). Multitask learning. Machine learning, 28(1):41–75.</ref>
          <ref rid="R3" class="deo:BibliographicReference" id="244" page="5" column="2">Chiou, F.-D., Chiang, D., and Palmer, M. (2001). Facil- itating Treebank Annotation Using a Statistical Parser. In Proceedings of the First International Conference on Human Language Technology Research, pages 1–4.</ref>
          <ref rid="R4" class="deo:BibliographicReference" id="245" confidence="possible" page="5" column="2">Church, K. W. (1988). A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136–143. Association for Computational Linguistics.</ref>
          <ref rid="R5" class="deo:BibliographicReference" id="246" confidence="possible" page="5" column="2">Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. (2011). Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</ref>
          <ref rid="R6" class="deo:BibliographicReference" id="247" confidence="possible" page="5" column="2">Crystal, D. (2002). Language Death. Cambridge University Press.</ref>
          <ref rid="R7" class="deo:BibliographicReference" id="248" confidence="possible" page="5" column="2">Culotta, A. and McCallum, A. (2005). Reducing Labeling Effort for Structured Prediction Tasks. In Proceedings of the 20th Conference on Artificial Intelligence, pages 746–751. AAAI.</ref>
          <ref rid="R8" class="deo:BibliographicReference" id="250" page="6" column="1">Daumé III, H. (2007). Frustratingly easy domain adaptation. In Annual Meeting of the Association for Computational Linguistics, volume 45, page 256.</ref>
          <ref rid="R9" class="deo:BibliographicReference" id="251" confidence="possible" page="6" column="1">Druck, G., Settles, B., and McCallum, A. (2009). Active learning by labeling features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 81–90. Association for Computational Linguistics.</ref>
          <ref rid="R10" class="deo:BibliographicReference" id="252" confidence="possible" page="6" column="1">Felt, P., Ringger, E., Seppi, K., Heal, K., Haertel, R., and Lonsdale, D. (2012). First results in a study evaluating pre-annotation and correction propagation for machine- assisted syriac morphological analysis. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), pages 878–885.</ref>
          <ref rid="R11" class="deo:BibliographicReference" id="253" confidence="possible" page="6" column="1">Felt, P., Ringger, E. K., Seppi, K., Heal, K. S., Haertel, R. A., and Lonsdale, D. (In Press.). Evaluating machine-assisted annotation in under-resourced settings. Language Resources and Evaluation. Francis, N. and Kucera, H. (1979). Brown Corpus Man- ual. Technical report, Department of Linguistics, Brown University, Providence, Rhode Island, US.</ref>
          <ref rid="R12" class="deo:BibliographicReference" id="254" confidence="possible" page="6" column="1">Ganchev, K., Pereira, F., Mandel, M., Carroll, S., and White, P. (2007). Semi-automated named entity annotation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics: Linguis- tic Annotation Workshop, pages 53–56.</ref>
          <ref rid="R13" class="deo:BibliographicReference" id="255" confidence="possible" page="6" column="1">Ganchev, K., Gra  ̧a, J. a., Gillenwater, J., and Taskar, B. (2010). Posterior regularization for structured latent variable models. The Journal of Machine Learning Research, 11:2001–2049.</ref>
          <ref rid="R14" class="deo:BibliographicReference" id="256" confidence="possible" page="6" column="1">Gippert, J., Himmelmann, N. P., and Mosel, U. (2006). Es- sentials of Language Documentation, volume 178. Wal- ter de Gruyter.</ref>
          <ref rid="R15" class="deo:BibliographicReference" id="257" confidence="possible" page="6" column="1">Grenoble, L. A. and Whaley, L. J. (1998). Endangered Languages: Language Loss and Community Response. Cambridge University Press.</ref>
          <ref rid="R16" class="deo:BibliographicReference" id="258" confidence="possible" page="6" column="1">Hovy, E. and Lavid, J. (2010). Towards a ‘science’ of corpus annotation: a new methodological challenge for corpus linguistics. International Journal of Translation, 22(1):13–36.</ref>
          <ref rid="R17" class="deo:BibliographicReference" id="259" confidence="possible" page="6" column="1">Kroch, A. S. (1989). Reflexes of grammar in patterns of language change. Language Variation and Change, 1(03):199–244.</ref>
          <ref rid="R18" class="deo:BibliographicReference" id="260" confidence="possible" page="6" column="1">Liang, P., Jordan, M. I., and Klein, D. (2009). Learning from measurements in exponential families. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 641–648. ACM.</ref>
          <ref rid="R19" class="deo:BibliographicReference" id="261" confidence="possible" page="6" column="1">Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B. (1993). Building a large annotated corpus of en- glish: The penn treebank. Computational Linguistics, 19(2):313–330.</ref>
          <ref rid="R20" class="deo:BibliographicReference" id="262" confidence="possible" page="6" column="1">Marcus, M., Santorini, B., and Marcinkiewicz, M. (1995). Treebank-2: Ldc95t7.</ref>
          <ref rid="R21" class="deo:BibliographicReference" id="263" confidence="possible" page="6" column="1">Nesselhauf, N. (2004). Learner corpora and their potential for language teaching. How to Use Corpora in Language Teaching, 12.</ref>
          <ref rid="R22" class="deo:BibliographicReference" id="264" confidence="possible" page="6" column="1">Pan, S. J. and Yang, Q. (2010). A survey on transfer learning. Knowledge and Data Engineering, IEEE Transac- tions on, 22(10):1345–1359.</ref>
          <ref rid="R23" class="deo:BibliographicReference" id="265" page="6" column="2">Roth, D. and Yih, W.-t. (2004). A Linear Programming Formulation for Global Inference in Natural Language Tasks. Defense Technical Information Center.</ref>
          <ref rid="R24" class="deo:BibliographicReference" id="267" confidence="possible" page="6" column="2">Sampson, G. (2008). The SUSANNE Analytic Scheme. <ext-link ext-link-type="uri" href="http://www.grsampson.net/RSue.html." id="266">http://www.grsampson.net/RSue.html.</ext-link> Ac- cessed: 10/3/2012.</ref>
          <ref rid="R25" class="deo:BibliographicReference" id="268" confidence="possible" page="6" column="2">Santorini, B. (1990). Part-of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision). Technical report, Department of Computer &amp; Information Science, University of Pennsylvania, Philadelphia, US.</ref>
          <ref rid="R26" class="deo:BibliographicReference" id="269" confidence="possible" page="6" column="2">Settles, B. (2010). Active learning literature survey. University of Wisconsin, Madison.</ref>
          <ref rid="R27" class="deo:BibliographicReference" id="270" confidence="possible" page="6" column="2">Sinclair, J. M. (2004). How to use Corpora in Language Teaching, volume 12. John Benjamins Publishing Com- pany.</ref>
          <ref rid="R28" class="deo:BibliographicReference" id="271" confidence="possible" page="6" column="2">Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003). Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 173–180. Association for Computational Linguistics.</ref>
          <ref rid="R29" class="deo:BibliographicReference" id="272" confidence="possible" page="6" column="2">Wolpert, D. H. (1992). Stacked generalization. Neural networks, 5(2):241–259.</ref>
        </ref-list>
        <region class="unknown" id="240" page="5" column="2">50000</region>
        <region class="unknown" id="241" page="5" column="2">50000</region>
        <region class="unknown" id="242" page="5" column="2">50000</region>
        <region class="unknown" id="243" page="5" column="2">50000</region>
        <outsider class="DoCO:TextBox" type="page_nr" id="249" page="5" column="2">144</outsider>
        <outsider class="DoCO:TextBox" type="page_nr" id="273" page="6" column="2">145</outsider>
      </section>
    </body>
  </article>
</pdfx>
