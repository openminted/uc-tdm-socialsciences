======== CAS 0 begin ==================================

-------- View _InitialView begin ----------------------------------

DocumentMetaData
   sofa: _InitialView
   begin: 0
   end: 40634
   language: "en"
   documentTitle: "The CIRDO Corpus: Comprehensive Audio/Video Database of Domestic Falls of Elderly People"
   documentId: "The CIRDO Corpus: Comprehensive Audio/Video Database of Domestic Falls of Elderly People"
   isLastSegment: false

CAS-Text:
The CIRDO Corpus: Comprehensive Audio/Video Database of Domestic Falls of Elderly PeopleM. Vacher, S. Bouakaz, M.-E. Bobillier Chaumon, F. Aman, R. A. Khan, S. Bekkadja, F. Portet, E. Guillou, S. Rossato, B. LecouteuxCNRS, LIG, F-38000 Grenoble, France Univ. Grenoble Alpes, LIG, F-38000, Grenoble, France LIRIS, UMR 5205 CNRS/Université Claude Bernard Lyon 1, F-69622 Villeurbanne, France GRePS, Université Lyon 2, F-69676 Bron, France Michel.Vacher@imag.fr, saida.bouakaz@univ-lyon1.fr, marc-eric.bobillier-chaumon@univ-lyon2.frIRDOAbstract Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes. In particular, regarding elderly living alone at home, the detection of distress situation after a fall is very important to reassure this kind of population. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The C corpus is a dataset recorded in realistic conditions in D , a fully equipped Smart Home with microphones and home automation sensors, in which participants performed scenarios including real falls on a carpet and calls for help. These scenarios were elaborated thanks to a field study involving elderly persons. Experiments related in a first part to distress detection in real-time using audio and speech analysis and in a second part to fall detection using video analysis are presented. Results show the difficulty of the task. The database can be used as standardized database by researchers to evaluate and compare their systems for elderly person’s assistance. Keywords: audio and video data set, multimodal corpus, natural language and multimodal interaction, Ambient Assisted Living (AAL), distress situation.1. IntroductionTwenty first century is witnessing a rapid growth in population over 65 years old worldwide (Who, 2013). This change in demography put pressure on governments which are incurring unprecedented expenditure to support ageing population since ageing is correlated with an increase in health and daily living support. For instance, in France, 12 million out of 66 million people are receiving the per- sonalized allocation of autonomy (PAA) 1 , a government financial aid for elderly person to support them in their daily life. This demographic change is very challenging to the society, governments and to technologists to come up with reliable and sustainable solutions to help ageing population to live as independently and safely as possible while reliev- ing their family’s emotional and financial burden. However this trend also provides the opportunity to come up with technologies that can help ageing population to carry on with their lives without compromising their privacy and at the same time provide assistance to caregivers i.e. nurse, partner, children etc. One of the main sources of stress in the ageing population for which technology can be of help is the fall incident. Indeed fall is a major source of injury for elderly people, between 20 and 40 percent of adults over 65 who live at home fall (Saim, 2014). Fall could lead elderly persons to faint or become unconscious. Another emergency situation is the inability to call for a help in distress situation of some aged persons due to their reduced mobility. In the absences of another person in the home (such as a carer), these situations could have disastrous effect. Several studies had worked on an ICT solutions to bring autonomy and security back to the user (Popescu et al., 2008;1 This allowance from the state is given to people over 60 years old in loss of autonomyOMUSHamill et al., 2009; Young and Mihailidis, 2010; Bloch et al., 2011). These studies have followed the paradigm shift in Human Computer Interaction design, from computer- centred designs to human-centred designs (Pantic et al., 2006). Thus reliable and robust analysis of ageing population’s gesture and voice has become inevitable in many application areas, such as social robots, emergency situation detection, behaviour monitoring, communication support, etc. Since many of these studies are based on statisti- cal analysis (either for modelling or for machine learning), there is a dire need of databases of audio and video recordings to build and evaluate performance of systems that can analyse ageing population’s gesture and voice. Indeed evaluation and benchmarking of systems / algorithms must be performed on standardized and accessible databases for meaningful comparison. In the absence of comparative tests on such standardized databases it is difficult to exhibits the relative strengths and weaknesses of these systems. To the best of our knowledge there exists no such database that has video with audio recordings of aged persons during fall and distress situations while it is recognised as one of the major problem in the evaluation of fall detectors (Igual et al., 2013). Young and Mihailidis (Young and Mihailidis, 2013) have collected a corpus of spontaneous speech, read sentences, and emergency scenarios from adult actors aged 23–91 years but it is in English and do not contain videos recording. This could be due to the fact that it is very difficult to setup a camera and microphone to record such events. Secondly, there is an ethical issue associated with recording aged person in distress situation and later making it available to pub- lic. Very few studies were able to record real falls of elderly persons (Kangas et al., 2012; Klenk et al., 2011) but those were only concerned with accelerometer sensors. To overcome this bottleneck in the research and development of emergency situation detectors, we introduce the C IRDO database which is part of the C IRDO project (Bouakaz et al., 2014) whose aim was to develop an audio/video emergency detection system for elderly persons. The C IRDO database contains video with audio (French language) recordings of aged persons during falls and distress situations. C IRDO database can be used as standardized database by researchers to evaluate and compare their systems for elderly person’s assistance. In this article, we provide all the details related to various steps that have been carried out to record data in such a challenging situation.2. Data AcquisitionRecording sessions were based on written scenarios. These scenarios were well elaborated thanks to a field study involving 15 elderly persons (Bobillier-Chaumon et al., 2012). Four scenarios were related to fall (F1 to F4), one to blocked hip (B) and two were true negative (TN1 and TN2). Recording sessions were conducted in the D OMUS smart room of the LIG laboratory configured to look like a standard room (chairs, carpet, coffee table, TV. . . ).2.1. ParticipantsIdeally scenarios should be played by elderly people. However, it is cumbersome to find elder person who is capa- ble and willing to play such scenarios (refer Section 2.2.). To record realistic data (but not necessarily played by elder person), people under 60 were recruited. Then, these vol- unteers were instructed to wear an equipment i.e. old age simulator (see Figure 1). Old age simulator hampered mobility, reduced vision and hearing. Overall 17 participants were recruited (9 men and 8 women) with mean age of 40 years (SD 19.5). Among them 13 people were under 60 and worn the simulator.2.2. Experimental ProtocolEach participant was introduced to the context of the research and was invited to sign a consent form. The participants played five types of fall chosen from 28 risky situations identified as: slip, stumble, falls in a stationary position and a position of hip blocked on the sofa. Figure 2 shows two elderly participants when they fall on the carpet. These situations were selected because they were represen- tative falls in domestic environment and could be played safely. Two other scenarios, called “true-false”, were added for the evaluation and verification of automatic fall detection. The first “true-false” situation consists of a rapid ac- tion of picking up of magazines from the floor (close to a situation of fall), while the second was to try to hold a remote control from the coffee table when the person is sit- ting on sofa (close to a situation in which the person has a blocked hip). Scenarios included call for help sentences. These sentences were chosen based on sentences identified during the field study and on sentences extracted from the AD80 corpus (Aman et al., 2013) which was built with the aim of elderly voice study. Table 2 gives some examples.Distress Sentence AD80 Sentence Aïe aïe aïe Aidez-moi Oh là Au secours Merde e-lio, appelle du secours Je suis tombé e-lio appelle ma fille Je peux pas me relever Appelle quelqu’un e-lio Qu’est-ce qu’il m’arrive e-lio, appelle quelqu’un Aïe ! J’ai mal e-lio appelle ma fille Oh là ! Je saigne ! Je me suis blessé e-lio appelle le SAMUBefore each recording session, the experimenter explained the scenario to the participant. Scenario has following sub- components: physical location, time of the day, activity (what the person intends to do and what eventually hap- pens), gesture (way of falling), and the spoken sentences. The participant rehearsed the scenario several times before final acquisition of data. On average, the duration of acquisition was 2 hours and 30 minutes per person.2.3. Equipment and Tools The studio and the equipment used are shown in Figure 1. Moreover, the DOMUS smart home was equipped with the social inclusion product e-lio 2 . Regarding audio analysis, we conducted a multi-source capture with two wireless Sennheiser microphones. A eW-300-G2 type ME2 microphone was placed on the sus- pended ceiling of the studio, and a SKM-300-G2 microphone was placed on a furniture close to the participant. The recording was performed in the technical room by a computer connected to the microphones high-frequency re- ceivers by National Instruments PCI-6220 8-channel card. A loudspeaker in the control room enabled experimenters to hear the progression of the scene in the studio. The National Instruments PCI-6220 card was controlled by the StreamHIS software (Vacher et al., 2011) developed by the GETALP team that enables data acquisition. Regarding video analysis, we conducted the video capture with two webcam cameras (Sony PSEye: 640x480 60Hz connected by a USB 2.0). Cameras were fixed to the wall, one in front of the Sofa and other at an angle of 45 degrees. Moreover, we acquired depth image with Microsoft Kinect. Synchronization and recording of video streams from all three cameras were performed in the technical room by computer (Intel i3 3.2GHz, 4GB RAM, NVidia GeForce GTS 450 512MB). The tools that enable video acquisition was developed by the team SAARA (based on the OpenCV library). Video processing (background learning and subtraction, body parts segmentation) was performed on a PC cluster, taking advantage of algorithms parallelization on CPU and GPU. Motion tracking and identification of haz- ardous situations was done on a PC (Intel i7 3.5GHz, 4GB RAM, NVIDIA GTX 660 4GB GeFore).3. The Acquired CorpusThe C IRDO corpus is divided into three parts, the audio corpus for sound and speech analysis, the video corpus and a third part with the annotations. The corpus is detailed in Sections 3.1. and 3.2.. The audio and video corpus was annotated for the video part, with the Advene software 3 , developed at the LIRIS laboratory, and for the sound part, with Transcriber. The C IRDO corpus is a web-based data library, hosted at LIRIS and accessible for the academic and research pur- pose. The license to use the C IRDO corpus is granted on a case-by-case basis. We can also grant permission to researchers to extend the corpus given they fulfil the required conditions and sign agreement.3.1. Audio CorpusWhen they played the scenarios, some participants produced sighs, grunts, coughs, cries, groans, panting or throat clearings. As our current studies are in the domain of automatic speech recognition, these sounds were not considered during the annotation process, but this can be done in the framework of future studies. In the same way, speeches mixed with sound produced by the fall were ignored. At the end, each speaker uttered between 10 and 65 short sentences or interjections (“ah”, “oh”, “aïe”, etc.) as shown Table 3.2 http://www.technosens.fr 3 http://liris.cnrs.fr/advene/All Call for helpNb. of interjections Spk. or short sentences Size (s) Y01 22 14 37.59 Y02 16 15 27.51 Y03 24 21 35.59 Y04 25 15 43.97 Y05 32 21 38.16 Y06 19 15 48.25 Y07 12 12 18.75 Y08 15 12 23.58 Y09 23 21 39.86 Y10 20 19 29.83 Y11 29 27 43.96 Y12 24 21 33.54 Y13 17 14 25.32 S01 65 53 92.07 S02 23 19 31.21 S03 23 21 26.02 S04 24 21 50.33 ALL 413 341 645.54 No Occurrence number of each scenario TimeSentences were often close to those identified during the field studies (“je peux pas me relever - I can’t get up”, “e- lio appelle du secours - e-lio call for help”, etc.), some were different (“oh bein on est bien là tiens - oh I am in a sticky situation”). In practice, participants cut some sentences (i.e., inserted a delay between “e-lio” and “appelle ma fille - call my daughter”), uttered some spontaneous sentences, interjections or non-verbal sounds (i.e., groan) The content of the video corpus is displayed in Table 4 As mentioned above, the video corpus includes various types of fall according to the scenarios defined in the Protocol (refer Section 2.2.). Each scenario includes a background sequence (300-600 frames) which usually is required for human body extraction. Description of the corpus is given in Table 4 wherein F1 to F4 are the fall scenarios, B blocked hip and TN1, TN2 true negatives. The total duration of the recordings is equal to 1 hour and 55 minutes, with a total of 162 video segments.3.2. Video Corpus4. Conclusion Drawing from Feedback from the ParticipantsFeedback from the participants were analyzed and allowed some conclusions (Body-Bekkadja et al., 2015). A high- lighted domestic accident, outlined (for oneself and for others) by C IRDO , leads the elderly individual to relate to the falls in other ways. They suddenly attempt to further regu- late and control it, sometimes by risking a profound change in behavior that may not be detected by C IRDO . More specifically, in our various studies (simulated C IRDO use), it was observed that: The elderly developed two oppos- ing conducts in the management of their fall monitored via C IRDO . Some of them chose to boost their falling movement or overplay screams to make sure the device will recognize the danger. They were also the same elderly that, as we mentioned in Study 2, used tangible remote monitoring systems (inset). They doubted the ability of the new am- bient system to detect their accident, and thus they risk a more serious injury. Others, however, in order to hide these incidents from the entourage, try to control the fall: They do not scream, and try to recover at all costs, even though it could exacerbate the deleterious effects of the fall. In both cases, we see that the falling movement choreography (falling is unintentional by nature) will be intentionally modified by the subjects, whether to be seen to fall, and, hence, be better recognized, or otherwise to be hidden from the technology and its supervision. Thus, the presence of technological artifacts changes the dynamic of the fall, because this behaviour will be addressed, directed toward a target (technology object), and also to other involved individuals. As mentioned by Clot (Clot, 1999), the movements of the falls are not only directed by the conduct of the sub- ject; they are also directed through the use of the technical object, and toward others (the representation they have of the system and the possible recipients of the alert). The fall choreography is influenced by what the individual wants to show to, or hide from, the device. Therefore, the risk of non-detecting these movements is possible, because they sometimes tend to veer too far from programmed scripts. Another more symbolical consequence involves the status and the legitimacy that C IRDO will give the fall. While, in the past, the word of the victim, or of a third party, used to be enough to prove the occurrence of a fall, these days, surveillance technologies are used to validate/approve the occurrence of an incident. We could even add that, they also assess whether this fall is acceptable, and conform to pre-defined scripts. In other words, the non-detection (non- recognition) of a fall by C IRDO may mean that: (1) The individual has poorly conducted the choreography of the falling; or (2) the movement cannot be categorized as a risk movement. This can therefore lead to a denial of risk and recognition of the elderly individual’s status of "victim", thereby throwing suspicion on their statements. As a result, C IRDO may operate a transfer of the risk recognition: It is the technology that gives official status to a fall in a domestic incident. It gives credibility and legitimacy to it. Thus, technological reliability wins against the word of the elderly; and then the latter can be discredited, for ex- ample, if the user talks about incidents not recognized by technology: “If the system is not triggered, then the fall did not occur." In both cases, setting C IRDO service in the domestic social system can either redefine the falls choreography, or redefine their status. All these reasons may be caused by malfunction or rejection. Several recommendations can be addressed to the design- ers. In addition to reassuring, involving, and properly training to use the new system (Hwang and Thorn, 1999)(Ku- jala, 2003), by including demonstrations and updates in real situations adapted to their habits and lifestyle practices (to demonstrate the system works efficiently in detecting falls, and avoiding any worsening of movements), design- ers should create programs and falls-detection algorithms more flexible, to cover a larger spectrum of falls choreography, and not rely solely on fixed scripts in risk behavior. In the same way, effective articulation between Audio and Video detections should be ensured to allow better falls- recognition and validation. As part of a participatory design through use (He and King, 2008), it would be interesting to adjust from feed- backs in the field, the location of the sensors and the level of falls scripts detection, by taking into account conduct- adjustments and daily risk evolutions from the moment the technological artifact is introduced. This requires a situated analysis of the actual device’s uses, in order to re-design it from its usage (Norman and Draper, 1986). This reinforced monitoring system will ask the elderly to make an oral (con- firmation) emergency call to an outside third party. This alert is then triggered only in case of a positive response, or a lack of response from the individual, enabling them to properly control the system (at to avoid false alarms). Finally, this new device for monitoring the activity will be meaningful only if it serves and supports its users’ quality of life. Therefore, one must be careful to involve and assist the various participants during all phases of C IRDO design, in an inclusive approach (as in the Living lab approach of (Pino et al., 2015), especially to anticipate reconfigurations at work in the social framework, as we shall now see.5. Call for Help Recognition from Audio AnalysisThe audio corpus was used for studies related to call for help recognition in Distant Speech conditions thanks to online speech analysis using SGMM acoustic modelling (Vacher et al., 2015a) (Vacher et al., 2015b). Non-speech analysis might bring information of high interest but it was not considered in our study because of the lack of training data. Currently non-speech audio analysis is a relatively unexplored field due not only to the lack of data but also to the unexpected sound classes that can be recorded at home in unconstrained conditions (Vacher et al., 2011). Moreover, non-speech sounds are made of non-verbal sounds (i.e. groans, panting, throat clearings, etc.) and of daily living sounds (i.e. falling objects, door slap, etc.) which represent different semantic information and are difficult to differentiate. For these reasons, only speech analysis was considered.5.1. Acoustic modellingThe Kaldi speech recognition tool-kit (Povey et al., 2011b) was chosen as ASR system. Kaldi is an open-source state- of-the-art Automatic Speech Recognition (ASR) system with a high number of tools and a strong support from the community. In the experiments, the acoustic models were context-dependent classical three-state left-right HMMs. Acoustic features were based on Mel-frequency cepstral coefficients, 13 MFCC-features coefficients were first extracted and then expanded with delta and double delta features and energy (40 features). Acoustic models were composed of 11,000 context-dependent states and 150,000 Gaussians. The state tying is performed using a decision tree based on a tree-clustering of the phones. In addition, off-line fMLLR linear transformation acoustic adap- tation was performed. The acoustic models were trained on 500 hours of transcribed French speech composed of the ESTER 1&2 (broadcast news and conversational speech recorded on the radio) and REPERE (TV news and talk-shows) challenges as well as from 7 hours of transcribed French speech of the S WEET -H OME corpus (Vacher et al., 2014) which consists of records of 60 speakers interacting within a smart home and from 28 minutes of the Voix-détresse corpus (Aman, 2014) which is made of records of speakers eliciting a distress emotion.5.1.1. Subspace GMM Acoustic ModellingThe GMM and Subspace GMM (SGMM) both model emis- sion probability of each HMM state with a Gaussian mixture model, but in the SGMM approach, the Gaussian means and the mixture component weights are generated from the phonetic and speaker subspaces along with a set of weight projections. The SGMM model (Povey et al., 2011a) is described in the following equations:where x denotes the feature vector, j ∈ {1..J} is the HMM state, i is the Gaussian index, m is the substate and c jm is the substate weight. Each state j is associated to a vector v jm ∈ R S (S is the phonetic subspace dimension) which derives the means, μ jmi and mixture weights, w jmi and it has a shared number of Gaussians, I. The phonetic subspace M i , weight projections w i T and covariance matrices Σ i i.e; the globally shared parameters Φ i = {M i , w i T , Σ i } are common across all states. These parameters can be shared and estimated over multiple record conditions. A generic mixture of I Gaussians, denoted as Universal Background Model (UBM), models all the speech training data for the initialization of the SGMM. Our experiments aims at obtaining SGMM shared parameters using both S WEET -H OME data (7h), Voix-détresse (28mn) and clean data (ESTER+REPERE 500h). Regarding the GMM part, the three training data set are just merged in a single one. (Povey et al., 2011a) showed that the model is also effective with large amount of training data. Therefore, three UBMs were trained respectively on S WEET -H OME data, Voix-détresse and clean data. These tree UBMs contained 1K gaussians and were merged into a single one mixed down to 1K gaussian (closest Gaussians pairs were merged (Zouari and Chollet, 2006)). The aim is to bias specifically the acoustic model towards distant speech home and expressive speech conditions.5.2. Recognition of distress callsThe recognition of distress calls consists in computing the phonetic distance of an hypothesis to a list of predefined distress calls. Each ASR hypothesis H i is phonetized, every predefined voice command T j is aligned to H i using Levenshtein distance. The deletion, insertion and substitu- tion costs were computed empirically while the cumulative distance γ(i, j) between H j and T i is given by Equation 2.The decision to select or not a detected sentence is then taken according a detection threshold on the aligned sym- bol score (phonems) of each identified call. This approach takes into account some recognition errors like word end- ings or light variations. Moreover, in a lot of cases, a miss- decoded word is phonetically close to the good one (due to the close pronunciation). From this the CER (Call Error Rate i.e., distress call error rate) is defined as: Number of missed calls CER = (3) Number of calls This measure was chosen because of the content of the corpus Cirdo-set used in this study. Indeed, this corpus is made of sentences and interjections. All sentences are calls for help, without any other kind of sentences (e.g., colloquial sentences), and therefore it is not possible to determine a false alarm rate in this framework.5.3. Off line experimentsThe methods presented in previous sections were run on the Cirdo-set audio corpus presented in Section 3.1. The SGMM model presented in Section 5.1. was used as acoutic model. The generic language model (LM) was estimated from French newswire collected in the Gigaword corpus. It was 1- gram with 13,304 words. Moreover, to reduce the linguistic variability, a 3-gram domain language model, the specialized language model was learnt from the sentences used during the corpus collection described in Section 2.2., with 99 1-gram, 225 2-gram and 273 3-gram models. Finally, the language model was a 3-gram-type which resulted from the combination of the generic LM (with a 10% weight) and the specialized LM (with 90% weight). This combination has been shown as leading to the best WER for domain specific application (Lecouteux et al., 2011). The interest of such combination is to bias the recognition towards the domain LM but when the speaker deviates from the domain, the general LM makes it possible to avoid the recognition of sentences leading to “false- positive” detection. Results on manually annotated data are given Table 5. The most important performance measures are the Word Error Rate (WER) of the overall decoded speech and those of the specific distress calls as well as the Call Error Rate (CER: c.f. equation 3). Considering distress calls only, the average WER is 34.0% whereas it a 39.3% when all interjections and sentences are taken into account. On average, CER is equal to 26.8% with an important dis- parity between the speakers. Unfortunately and as mentioned above, the used corpus does not allow to determine a False Alarm Rate. Previous studies based on the AD80 corpus showed recall, precision and F-measure equal to 88.4%, 86.9% and 87.2% (Aman et al., 2013). Nevertheless, this corpus was recorded in very different conditions, text reading in a studio, in contrary of those of Cirdo-set. WER (%) WER (%)6. Fall Detection from Video AnalysisThe video analysis framework for fall events labeling is based on the silhouette extraction. The silhouette is extracted by removal of background pixels within the video scene using. The background subtraction is performed by Mixture of Gaussian based approach, and eigenbackground (PCA) approach (Deeb et al., 2012), (Priyank Shah, 2014). The extracted shape is then used to obtain discriminative features to detect anomaly in person’s movement in the scene. The overview of proposed framework is shown in Figure 3.6.1. Foreground Extraction and Modeling of Body PartsAfter silhouette extraction the human body is segmented into colored connected components. Colored connected components regarded as regions are extracted by applying region growing technique. Each component with similar color are fitted into blob (Hsieh et al., 2010). Blob is repre- sented by spatial color Gaussian mixture model, assuming that spatial color components are de-correlated. The probability of an observation X t of that pixel at time t to belong to the background is given by equation 4.where L is the number of gaussian, η is Gaussian probability density function, Σ k,t is an estimates of weight and ω k,t is covariance matrix of the k-th Gaussian in the mixture at time t. In the last, an energy based function is formulated over the unknown labels of every pixel in the form of a first order Markov random field (MRF) energy function:Here, N e is neighboring pixels and the data energy Σ p∈P D p (f p ) evaluates the likelihood of each pixel to take a label. Finally, energy function is minimized with a graph cut algorithm via a swap approach (Miguel Angel Bautista, 2015). Background model is updated selectively with an online EM algorithm (Moon, 1996).6.2. Tracking of Body PartsOur method begins with an iterative process and considers that person is in standing position. Body is decomposed in three parts i.e. head, torso and lower part. Then, dynamic processing technique is used to obtain different level of body details (i.e. number of blobs), if required. At first, a blob (k-th blob) is built for each connected component with a feature vector containing: color, center’s coordinates C k , a predicted change of color P k and velocity V k . Set of current colored connected components are extracted from the smoothed foreground by region growing. By considering tracking of blobs as a matching process, we introduce a novel cost function to obtain distance between two blobs. The cost function is given by: Cost(j, k) = α 1 ( P k −P j t /V k )+α 2 ( C j −C k M / C k ) (6) where C j t and P j t respectively, the color and the center’s coordinates of the j-th connected component extracted at time t.6.3. Pose Recognition and Labeling of EventsThe labeling of events is based on the scenario define above. To identify poses, the extracted silhouette from the body parts was used. To recognize and label different poses from the video data, an histogram based approach was used as they runs in real time and improves its accu- racy with the passage of run time The recognition wss based on comparison of two histograms i.e. key pose frame histogram and histogram of frame in hand (Barnachon et al., 2012). The histograms were then compared with the Bhattacharyya distance and warped by a dynamic time warping process to achieve their optimal alignment (Barnachon et al., 2014). Bhattacharyya distance is calculated using:Where H 1 and H 2 are integral histograms. In this study we focused on the analysis of posture to detect event of distress. Our framework for fall detection could further be enhanced by considering other factors as well, i.e. facial expression analysis. Analysis of facial expressions to detect pain can be very advantageous in case of prolonged immobility of elderly person (Khan et al., 2013).7. ConclusionThis paper investigated smart home technology and recorded comprehensive audio/video corpus of domestic falls. For elderly, the fall is one of the most feared and recurring problems. Surveillance technologies tries to provide solution to this issue by alerting contact person in case of fall. There is lot of work which focuses on this problem but to the best of our knowledge there is no common database for comparing results. To address this issue we have made available a database composed of audio/video records of falls and prolonged immobility. In this paper, studies using this database for the automatic identification of these events were also presented. The C IRDO corpus was recorded in a smart environment reproducing a typical living room containing an e-lio communication device equipped with microphone and camera. The experiment was guided by an ethnographic study which detailed the various events related to distress situations faced by elderly people. Various scenarios were estab- lished from this study that describes pattern of postures in case of fall so that acted falls were as close to real situation as possible. The database can thus be useful to researchers of the community studying video or audio emergency situations as well as to model the relationship between audio/video events and learn fall model using machine learning.AcknowledgementsThis work was supported by the French funding agen- cies ANR and CNSA through C IRDO project (ANR-2010- TECS-012). The authors would like to thanks the persons who agreed to participate in the survey or in the recordings.Bibliographical ReferencesAman, F., Vacher, M., Rossato, S., and Portet, F. (2013). Speech Recognition of Aged Voices in the AAL Context: Detection of Distress Sentences. In The 7th Int. Conf. on Speech Technology and Human-Computer Dialogue, SpeD 2013, pages 177–184. Aman, F. (2014). Reconnaissance automatique de la pa- role de personnes âgées pour les services d’assistance à domicile. Ph.D. thesis, Université de Grenoble, Ecole doctorale MSTII. Barnachon, M., Bouakaz, S., Boufama, B., and Guillou, E. (2012). Human actions recognition from streamed mo- tion capture. In Pattern Recognition (ICPR), Int. Conf. on, pages 3807–3810. IEEE. Barnachon, M., Bouakaz, S., Boufama, B., and Guillou, E. (2014). Ongoing Human Action Recognition with Motion Capture. Pattern Recognition, 47(1):238–247. Bloch, F., Gautier, V., Noury, N., Lundy, J., Poujaud, J., Claessens, Y., and Rigaud, A. (2011). Evaluation under real-life conditions of a stand-alone fall detector for the elderly subjects. Annals of Physical and Rehabilitation Medicine, 54:391–398. Bobillier-Chaumon, M.-E., Cuvillier, B., Bouakaz, S., and Vacher, M. (2012). Démarche de développement de technologies ambiantes pour le maintien à domicile des personnes dépendantes : vers une triangulation des méth- odes et des approches. In Actes du 1er Congrès Eu- ropéen de Stimulation Cognitive, pages 121–122, Dijon, France. Body-Bekkadja, S., Bobillier-Chaumon, M.-E., Cuvillier, B., and Cros, F. (2015). Understanding the Socio- Domestic Activity: A Challenge for the Ambient Technologies Acceptance in the Case of Homecare Assis- tance. In HCI International, volume Part II of LNCS 9194, pages 399–411. Bouakaz, S., Vacher, M., Bobillier-Chaumon, M.-E., FrédéricAman, Bekkadja, Portet, Guillou, E., Rossato, S., Desserée, E., Traineau, P., Vimon, J.-P., and Cheva- lier, T. (2014). CIRDO: Smart companion for helping elderly to live at home for longer. Innovation and Re- search in BioMedical engineering, 35(2):101–108. Clot, Y. (1999). La fonction psychologique du travail. PUF, Paris, France. Deeb, R., Desseree, E., and Bouakaz, S. (2012). Real-time two-level foreground detection and person-silhouette extraction enhanced by body-parts tracking. In Proc. SPIE, volume 83010R, pages 1–8. Hamill, M., Young, V., Boger, J., and Mihailidis, A. (2009). Development of an automated speech recognition interface for personal emergency response systems. Journal of NeuroEngineering and Rehabilitation, 6(1):26. He, J. and King, W. (2008). The role of user participa- tion in information systems development: Implications from a meta-analysis. Journal of Management Information Systems, 25(1):301–331. Hsieh, C., Chuang, S., Chen, S., Chen, C., and Fan, K. (2010). Segmentation of human body parts using de- formable triangulation. IEEE Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans, 40(3):596–610. Hwang, M. and Thorn, R. (1999). The effect of user en- gagement on system success: A meta-analytical integra- tion of research findings. Information and Management, 35(4):229–236. Igual, R., Medrano, C., and Plaza, I. (2013). Challenges, issues and trends in fall detection systems. BioMedical Engineering OnLine, 12(1):1–24. Kangas, M., Vikman, I., Nyberg, L., Korpelainen, R., Lind- blom, J., and Jämsä, T. (2012). Comparison of real-life accidental falls in older people with experimental falls in middle-aged test subjects. Gait & Posture, 35(3):500 – 505. Khan, R. A., Meyer, A., Konik, H., and Bouakaz, S. (2013). Pain detection through shape and appearance features. In Multimedia and Expo (ICME), 2013 IEEE International Conference on, pages 1–6, July. Klenk, J., Becker, C., Lieken, F., Nicolai, S., Maetzler, W., Alt, W., Zijlstra, W., Hausdorff, J., van Lummel, R., Chiari, L., and Lindemann, U. (2011). Comparison of acceleration signals of simulated and real-world back- ward falls. Medical Engineering & Physics, 33(3):368 – 373. Kujala, S. (2003). User involvement: A review of the ben- efits and challenges. Behaviour and Information Technology, 22(1):1–16. Lecouteux, B., Vacher, M., and Portet, F. (2011). Distant Speech Recognition in a Smart Home: Comparison of Several Multisource ASRs in Realistic Conditions. In In- terspeech 2011, pages 1–4, Florence, Italy. Miguel Angel Bautista, Sergio Escalera, D. S. (2015). Hupba8k: Dataset and ecoc-graph-cut based segmentation of human limbs. Neurocomputing, 150(A):173–188. Moon, T. K. (1996). The expectation-maximization algorithm. IEEE Signal Processing Magazine, 13(6):47–60. Norman, D. A. and Draper, S. W. (1986). User Centered System Design; New Perspectives on Human-Computer Interaction. L. Erlbaum Associates Inc. Pantic, M., Pentland, A., Nijholt, A., and Huang, T. (2006). Human computing and machine understanding of human behavior: survey. In ACM Int. Conf. on Multimodal In- terfaces. Pino, M., Moget, C., Benveniste, S., Picard, R., and Rigaud, A.-S. (2015). Innovative technology-based healthcare and support services for older adults: How and why in- dustrial initiatives convert to the living lab approach. In HCI International, volume Part II of LNCS 9194, pages 158–169. Springer International Publishing. Popescu, M., Li, Y., Skubic, M., and Rantz, M. (2008). An acoustic fall detector system that uses sound height information to reduce the false alarm rate. In Proc. 30th Annual Int. Conference of the IEEE-EMBS 2008, pages 4628–4631, 20–25 Aug. Povey, D., Burget, L., Agarwal, M., Akyazi, P., Kai, F., Ghoshal, A., Glembek, O., Goel, N., Karafiát, M., Rastrow, A., Rose, R. C., Schwarz, P., and Thomas, S. (2011a). The subspace gaussian mixture model—a structured model for speech recognition. Computer Speech & Language, 25(2):404 – 439. Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., and Vesely, K. (2011b). The Kaldi Speech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Soci- ety, December. IEEE Catalog No.: CFP11SRW-USB. Priyank Shah, H. M. (2014). Comperehensive study and comparative analysis of different types of background subtraction algorithms. International Journal of Im- age,Graphics and Signal Processing, 6(8):47–52. Saim, R. (2014). Accidental falls amongst the elderly: Health impact and effective intervention strategies. Mas- ter’s thesis. Vacher, M., Portet, F., Fleury, A., and Noury, N. (2011). Development of Audio Sensing Technology for Ambient Assisted Living: Applications and Challenges. International journal of E-Health and medical communications, 2(1):35–54, March. Vacher, M., Lecouteux, B., Chahuara, P., Portet, F., Meil- lon, B., and Bonnefond, N. (2014). The Sweet-Home speech and multimodal corpus for home automation interaction. In The 9th edition of the Language Resources and Evaluation Conference (LREC), pages 4499–4506, Reykjavik, Iceland. Vacher, M., Aman, F., Rossato, S., and Portet, F. (2015a). Development of Automatic Speech Recognition Tech- niques for Elderly Home Support: Applications and Challenges. In J. Zou et al., editors, HCI International, volume Part II of LNCS 9194, pages 341–353, Los An- geles, CA, United States, August. Springer International Publishing Switzerland. Vacher, M., Lecouteux, B., Aman, F., Rossato, S., and Portet, F. (2015b). Recognition of Distress Calls in Distant Speech Setting: a Preliminary Experiment in a Smart Home. In 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 1–7, Dresden, Germany, September. SIG-SLPAT. Who. (2013). World population ageing: 1950-2050. Tech- nical report, Executive report, United Nations, Depart- ment of Economic and Social Affairs, Population Divi- sion. Young, V. and Mihailidis, A. (2010). An automated, speech-based emergency response system for the older adult. Gerontechnology, 9(2):261. Young, V. and Mihailidis, A. (2013). The CARES corpus: a database of older adult actor simulated emergency dia- logue for developing a personal emergency response system. I. J. Speech Technology, 16(1):55–73. Zouari, L. and Chollet, G. (2006). Efficient gaussian mixture for speech recognition. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 4, pages 294–297.
[The CIRDO Corpus: Comprehensive Audio/Video Database of Domestic Falls of Elderly People]
Paragraph
   sofa: _InitialView
   begin: 0
   end: 88
[The CIRDO Corpus: Comprehensive Audio/Video Database of Domestic Falls of Elderly People]
Sentence
   sofa: _InitialView
   begin: 0
   end: 88
[M. Vacher, S. Bouakaz, M.-E. Bobillier Chaumon, F. Aman, R. A. Khan, S. Bekkadja, F. Portet, E. Guillou, S. Rossato, B. Lecouteux]
Paragraph
   sofa: _InitialView
   begin: 88
   end: 217
[M. Vacher, S. Bouakaz, M.-E. Bobillier Chaumon, F. Aman, R. A. Khan, S. Bekkadja, F. Portet, E. Guillou, S. Rossato, B. Lecouteux]
Sentence
   sofa: _InitialView
   begin: 88
   end: 217
[CNRS, LIG, F-38000 Grenoble, France Univ. Grenoble Alpes, LIG, F-38000, Grenoble, France LIRIS, UMR 5205 CNRS/Université Claude Bernard Lyon 1, F-69622 Villeurbanne, France GRePS, Université Lyon 2, F-69676 Bron, France Michel.Vacher@imag.fr, saida.bouakaz@univ-lyon1.fr, marc-eric.bobillier-chaumon@univ-lyon2.fr]
Paragraph
   sofa: _InitialView
   begin: 217
   end: 530
[CNRS, LIG, F-38000 Grenoble, France Univ.]
Sentence
   sofa: _InitialView
   begin: 217
   end: 258
[ Grenoble Alpes, LIG, F-38000, Grenoble, France LIRIS, UMR 5205 CNRS/Université Claude Bernard Lyon 1, F-69622 Villeurbanne, France GRePS, Université Lyon 2, F-69676 Bron, France Michel.Vacher@imag.fr, saida.bouakaz@univ-lyon1.fr, marc-eric.bobillier-chaumon@univ-lyon2.fr]
Sentence
   sofa: _InitialView
   begin: 258
   end: 530
[IRDO]
Paragraph
   sofa: _InitialView
   begin: 530
   end: 534
[IRDO]
Sentence
   sofa: _InitialView
   begin: 530
   end: 534
[Abstract Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes. In particular, regarding elderly living alone at home, the detection of distress situation after a fall is very important to reassure this kind of population. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The C corpus is a dataset recorded in realistic conditions in D , a fully equipped Smart Home with microphones and home automation sensors, in which participants performed scenarios including real falls on a carpet and calls for help. These scenarios were elaborated thanks to a field study involving elderly persons. Experiments related in a first part to distress detection in real-time using audio and speech analysis and in a second part to fall detection using video analysis are presented. Results show the difficulty of the task. The database can be used as standardized database by researchers to evaluate and compare their systems for elderly person’s assistance. Keywords: audio and video data set, multimodal corpus, natural language and multimodal interaction, Ambient Assisted Living (AAL), distress situation.]
Paragraph
   sofa: _InitialView
   begin: 534
   end: 1825
[Abstract Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes. In particular, regarding elderly living alone at home, the detection of distress situation after a fall is very important to reassure this kind of population. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The C corpus is a dataset recorded in realistic conditions in D , a fully equipped Smart Home with microphones and home automation sensors, in which participants performed scenarios including real falls on a carpet and calls for help. These scenarios were elaborated thanks to a field study involving elderly persons. Experiments related in a first part to distress detection in real-time using audio and speech analysis and in a second part to fall detection using video analysis are presented. Results show the difficulty of the task. The database can be used as standardized database by researchers to evaluate and compare their systems for elderly person’s assistance. Keywords: audio and video data set, multimodal corpus, natural language and multimodal interaction, Ambient Assisted Living (AAL), distress situation.]
Sentence
   sofa: _InitialView
   begin: 534
   end: 1825
[1. Introduction]
Paragraph
   sofa: _InitialView
   begin: 1825
   end: 1840
[1. Introduction]
Sentence
   sofa: _InitialView
   begin: 1825
   end: 1840
[Twenty first century is witnessing a rapid growth in population over 65 years old worldwide (Who, 2013). This change in demography put pressure on governments which are incurring unprecedented expenditure to support ageing population since ageing is correlated with an increase in health and daily living support. For instance, in France, 12 million out of 66 million people are receiving the per- sonalized allocation of autonomy (PAA) 1 , a government financial aid for elderly person to support them in their daily life. This demographic change is very challenging to the society, governments and to technologists to come up with reliable and sustainable solutions to help ageing population to live as independently and safely as possible while reliev- ing their family’s emotional and financial burden. However this trend also provides the opportunity to come up with technologies that can help ageing population to carry on with their lives without compromising their privacy and at the same time provide assistance to caregivers i.e. nurse, partner, children etc. One of the main sources of stress in the ageing population for which technology can be of help is the fall incident. Indeed fall is a major source of injury for elderly people, between 20 and 40 percent of adults over 65 who live at home fall (Saim, 2014). Fall could lead elderly persons to faint or become unconscious. Another emergency situation is the inability to call for a help in distress situation of some aged persons due to their reduced mobility. In the absences of another person in the home (such as a carer), these situations could have disastrous effect. Several studies had worked on an ICT solutions to bring autonomy and security back to the user (Popescu et al., 2008;]
Paragraph
   sofa: _InitialView
   begin: 1840
   end: 3598
[Twenty first century is witnessing a rapid growth in population over 65 years old worldwide (Who, 2013).]
Sentence
   sofa: _InitialView
   begin: 1840
   end: 1944
[ This change in demography put pressure on governments which are incurring unprecedented expenditure to support ageing population since ageing is correlated with an increase in health and daily living support.]
Sentence
   sofa: _InitialView
   begin: 1944
   end: 2153
[ For instance, in France, 12 million out of 66 million people are receiving the per- sonalized allocation of autonomy (PAA) 1 , a government financial aid for elderly person to support them in their daily life.]
Sentence
   sofa: _InitialView
   begin: 2153
   end: 2363
[ This demographic change is very challenging to the society, governments and to technologists to come up with reliable and sustainable solutions to help ageing population to live as independently and safely as possible while reliev- ing their family’s emotional and financial burden.]
Sentence
   sofa: _InitialView
   begin: 2363
   end: 2646
[ However this trend also provides the opportunity to come up with technologies that can help ageing population to carry on with their lives without compromising their privacy and at the same time provide assistance to caregivers i.e.]
Sentence
   sofa: _InitialView
   begin: 2646
   end: 2879
[ nurse, partner, children etc.]
Sentence
   sofa: _InitialView
   begin: 2879
   end: 2909
[ One of the main sources of stress in the ageing population for which technology can be of help is the fall incident.]
Sentence
   sofa: _InitialView
   begin: 2909
   end: 3026
[ Indeed fall is a major source of injury for elderly people, between 20 and 40 percent of adults over 65 who live at home fall (Saim, 2014).]
Sentence
   sofa: _InitialView
   begin: 3026
   end: 3166
[ Fall could lead elderly persons to faint or become unconscious.]
Sentence
   sofa: _InitialView
   begin: 3166
   end: 3230
[ Another emergency situation is the inability to call for a help in distress situation of some aged persons due to their reduced mobility.]
Sentence
   sofa: _InitialView
   begin: 3230
   end: 3368
[ In the absences of another person in the home (such as a carer), these situations could have disastrous effect.]
Sentence
   sofa: _InitialView
   begin: 3368
   end: 3480
[ Several studies had worked on an ICT solutions to bring autonomy and security back to the user (Popescu et al., 2008;]
Sentence
   sofa: _InitialView
   begin: 3480
   end: 3598
[1 This allowance from the state is given to people over 60 years old in loss of autonomy]
Paragraph
   sofa: _InitialView
   begin: 3598
   end: 3686
[1 This allowance from the state is given to people over 60 years old in loss of autonomy]
Sentence
   sofa: _InitialView
   begin: 3598
   end: 3686
[OMUS]
Paragraph
   sofa: _InitialView
   begin: 3686
   end: 3690
[OMUS]
Sentence
   sofa: _InitialView
   begin: 3686
   end: 3690
[Hamill et al., 2009; Young and Mihailidis, 2010; Bloch et al., 2011). These studies have followed the paradigm shift in Human Computer Interaction design, from computer- centred designs to human-centred designs (Pantic et al., 2006). Thus reliable and robust analysis of ageing population’s gesture and voice has become inevitable in many application areas, such as social robots, emergency situation detection, behaviour monitoring, communication support, etc. Since many of these studies are based on statisti- cal analysis (either for modelling or for machine learning), there is a dire need of databases of audio and video recordings to build and evaluate performance of systems that can analyse ageing population’s gesture and voice. Indeed evaluation and benchmarking of systems / algorithms must be performed on standardized and accessible databases for meaningful comparison. In the absence of comparative tests on such standardized databases it is difficult to exhibits the relative strengths and weaknesses of these systems. To the best of our knowledge there exists no such database that has video with audio recordings of aged persons during fall and distress situations while it is recognised as one of the major problem in the evaluation of fall detectors (Igual et al., 2013). Young and Mihailidis (Young and Mihailidis, 2013) have collected a corpus of spontaneous speech, read sentences, and emergency scenarios from adult actors aged 23–91 years but it is in English and do not contain videos recording. This could be due to the fact that it is very difficult to setup a camera and microphone to record such events. Secondly, there is an ethical issue associated with recording aged person in distress situation and later making it available to pub- lic. Very few studies were able to record real falls of elderly persons (Kangas et al., 2012; Klenk et al., 2011) but those were only concerned with accelerometer sensors. To overcome this bottleneck in the research and development of emergency situation detectors, we introduce the C IRDO database which is part of the C IRDO project (Bouakaz et al., 2014) whose aim was to develop an audio/video emergency detection system for elderly persons. The C IRDO database contains video with audio (French language) recordings of aged persons during falls and distress situations. C IRDO database can be used as standardized database by researchers to evaluate and compare their systems for elderly person’s assistance. In this article, we provide all the details related to various steps that have been carried out to record data in such a challenging situation.]
Paragraph
   sofa: _InitialView
   begin: 3690
   end: 6315
[Hamill et al., 2009; Young and Mihailidis, 2010; Bloch et al., 2011).]
Sentence
   sofa: _InitialView
   begin: 3690
   end: 3759
[ These studies have followed the paradigm shift in Human Computer Interaction design, from computer- centred designs to human-centred designs (Pantic et al., 2006).]
Sentence
   sofa: _InitialView
   begin: 3759
   end: 3923
[ Thus reliable and robust analysis of ageing population’s gesture and voice has become inevitable in many application areas, such as social robots, emergency situation detection, behaviour monitoring, communication support, etc.]
Sentence
   sofa: _InitialView
   begin: 3923
   end: 4151
[ Since many of these studies are based on statisti- cal analysis (either for modelling or for machine learning), there is a dire need of databases of audio and video recordings to build and evaluate performance of systems that can analyse ageing population’s gesture and voice.]
Sentence
   sofa: _InitialView
   begin: 4151
   end: 4428
[ Indeed evaluation and benchmarking of systems / algorithms must be performed on standardized and accessible databases for meaningful comparison.]
Sentence
   sofa: _InitialView
   begin: 4428
   end: 4573
[ In the absence of comparative tests on such standardized databases it is difficult to exhibits the relative strengths and weaknesses of these systems.]
Sentence
   sofa: _InitialView
   begin: 4573
   end: 4724
[ To the best of our knowledge there exists no such database that has video with audio recordings of aged persons during fall and distress situations while it is recognised as one of the major problem in the evaluation of fall detectors (Igual et al., 2013).]
Sentence
   sofa: _InitialView
   begin: 4724
   end: 4981
[ Young and Mihailidis (Young and Mihailidis, 2013) have collected a corpus of spontaneous speech, read sentences, and emergency scenarios from adult actors aged 23–91 years but it is in English and do not contain videos recording.]
Sentence
   sofa: _InitialView
   begin: 4981
   end: 5211
[ This could be due to the fact that it is very difficult to setup a camera and microphone to record such events.]
Sentence
   sofa: _InitialView
   begin: 5211
   end: 5323
[ Secondly, there is an ethical issue associated with recording aged person in distress situation and later making it available to pub- lic.]
Sentence
   sofa: _InitialView
   begin: 5323
   end: 5462
[ Very few studies were able to record real falls of elderly persons (Kangas et al., 2012; Klenk et al., 2011) but those were only concerned with accelerometer sensors.]
Sentence
   sofa: _InitialView
   begin: 5462
   end: 5629
[ To overcome this bottleneck in the research and development of emergency situation detectors, we introduce the C IRDO database which is part of the C IRDO project (Bouakaz et al., 2014) whose aim was to develop an audio/video emergency detection system for elderly persons.]
Sentence
   sofa: _InitialView
   begin: 5629
   end: 5903
[ The C IRDO database contains video with audio (French language) recordings of aged persons during falls and distress situations.]
Sentence
   sofa: _InitialView
   begin: 5903
   end: 6032
[ C IRDO database can be used as standardized database by researchers to evaluate and compare their systems for elderly person’s assistance.]
Sentence
   sofa: _InitialView
   begin: 6032
   end: 6171
[ In this article, we provide all the details related to various steps that have been carried out to record data in such a challenging situation.]
Sentence
   sofa: _InitialView
   begin: 6171
   end: 6315
[2. Data Acquisition]
Paragraph
   sofa: _InitialView
   begin: 6315
   end: 6334
[2. Data Acquisition]
Sentence
   sofa: _InitialView
   begin: 6315
   end: 6334
[Recording sessions were based on written scenarios. These scenarios were well elaborated thanks to a field study involving 15 elderly persons (Bobillier-Chaumon et al., 2012). Four scenarios were related to fall (F1 to F4), one to blocked hip (B) and two were true negative (TN1 and TN2). Recording sessions were conducted in the D OMUS smart room of the LIG laboratory configured to look like a standard room (chairs, carpet, coffee table, TV. . . ).]
Paragraph
   sofa: _InitialView
   begin: 6334
   end: 6785
[Recording sessions were based on written scenarios.]
Sentence
   sofa: _InitialView
   begin: 6334
   end: 6385
[ These scenarios were well elaborated thanks to a field study involving 15 elderly persons (Bobillier-Chaumon et al., 2012).]
Sentence
   sofa: _InitialView
   begin: 6385
   end: 6509
[ Four scenarios were related to fall (F1 to F4), one to blocked hip (B) and two were true negative (TN1 and TN2).]
Sentence
   sofa: _InitialView
   begin: 6509
   end: 6622
[ Recording sessions were conducted in the D OMUS smart room of the LIG laboratory configured to look like a standard room (chairs, carpet, coffee table, TV.]
Sentence
   sofa: _InitialView
   begin: 6622
   end: 6778
[ .]
Sentence
   sofa: _InitialView
   begin: 6778
   end: 6780
[ .]
Sentence
   sofa: _InitialView
   begin: 6780
   end: 6782
[ ).]
Sentence
   sofa: _InitialView
   begin: 6782
   end: 6785
[2.1. Participants]
Paragraph
   sofa: _InitialView
   begin: 6785
   end: 6802
[2.1. Participants]
Sentence
   sofa: _InitialView
   begin: 6785
   end: 6802
[Ideally scenarios should be played by elderly people. However, it is cumbersome to find elder person who is capa- ble and willing to play such scenarios (refer Section 2.2.). To record realistic data (but not necessarily played by elder person), people under 60 were recruited. Then, these vol- unteers were instructed to wear an equipment i.e. old age simulator (see Figure 1). Old age simulator hampered mobility, reduced vision and hearing. Overall 17 participants were recruited (9 men and 8 women) with mean age of 40 years (SD 19.5). Among them 13 people were under 60 and worn the simulator.]
Paragraph
   sofa: _InitialView
   begin: 6802
   end: 7400
[Ideally scenarios should be played by elderly people.]
Sentence
   sofa: _InitialView
   begin: 6802
   end: 6855
[ However, it is cumbersome to find elder person who is capa- ble and willing to play such scenarios (refer Section 2.2.).]
Sentence
   sofa: _InitialView
   begin: 6855
   end: 6976
[ To record realistic data (but not necessarily played by elder person), people under 60 were recruited.]
Sentence
   sofa: _InitialView
   begin: 6976
   end: 7079
[ Then, these vol- unteers were instructed to wear an equipment i.e.]
Sentence
   sofa: _InitialView
   begin: 7079
   end: 7146
[ old age simulator (see Figure 1).]
Sentence
   sofa: _InitialView
   begin: 7146
   end: 7180
[Figure 1]
Reference
   sofa: _InitialView
   begin: 7170
   end: 7178
   refId: "F1"
   refType: "fig"
[ Old age simulator hampered mobility, reduced vision and hearing.]
Sentence
   sofa: _InitialView
   begin: 7180
   end: 7245
[ Overall 17 participants were recruited (9 men and 8 women) with mean age of 40 years (SD 19.5).]
Sentence
   sofa: _InitialView
   begin: 7245
   end: 7341
[ Among them 13 people were under 60 and worn the simulator.]
Sentence
   sofa: _InitialView
   begin: 7341
   end: 7400
[2.2. Experimental Protocol]
Paragraph
   sofa: _InitialView
   begin: 7400
   end: 7426
[2.2. Experimental Protocol]
Sentence
   sofa: _InitialView
   begin: 7400
   end: 7426
[Each participant was introduced to the context of the research and was invited to sign a consent form. The participants played five types of fall chosen from 28 risky situations identified as: slip, stumble, falls in a stationary position and a position of hip blocked on the sofa. Figure 2 shows two elderly participants when they fall on the carpet. These situations were selected because they were represen- tative falls in domestic environment and could be played safely. Two other scenarios, called “true-false”, were added for the evaluation and verification of automatic fall detection. The first “true-false” situation consists of a rapid ac- tion of picking up of magazines from the floor (close to a situation of fall), while the second was to try to hold a remote control from the coffee table when the person is sit- ting on sofa (close to a situation in which the person has a blocked hip). Scenarios included call for help sentences. These sentences were chosen based on sentences identified during the field study and on sentences extracted from the AD80 corpus (Aman et al., 2013) which was built with the aim of elderly voice study. Table 2 gives some examples.]
Paragraph
   sofa: _InitialView
   begin: 7426
   end: 8604
[Each participant was introduced to the context of the research and was invited to sign a consent form.]
Sentence
   sofa: _InitialView
   begin: 7426
   end: 7528
[ The participants played five types of fall chosen from 28 risky situations identified as: slip, stumble, falls in a stationary position and a position of hip blocked on the sofa.]
Sentence
   sofa: _InitialView
   begin: 7528
   end: 7707
[ Figure 2 shows two elderly participants when they fall on the carpet.]
Sentence
   sofa: _InitialView
   begin: 7707
   end: 7777
[Figure 2]
Reference
   sofa: _InitialView
   begin: 7708
   end: 7716
   refId: "F2"
   refType: "fig"
[ These situations were selected because they were represen- tative falls in domestic environment and could be played safely.]
Sentence
   sofa: _InitialView
   begin: 7777
   end: 7901
[ Two other scenarios, called “true-false”, were added for the evaluation and verification of automatic fall detection.]
Sentence
   sofa: _InitialView
   begin: 7901
   end: 8019
[ The first “true-false” situation consists of a rapid ac- tion of picking up of magazines from the floor (close to a situation of fall), while the second was to try to hold a remote control from the coffee table when the person is sit- ting on sofa (close to a situation in which the person has a blocked hip).]
Sentence
   sofa: _InitialView
   begin: 8019
   end: 8329
[ Scenarios included call for help sentences.]
Sentence
   sofa: _InitialView
   begin: 8329
   end: 8373
[ These sentences were chosen based on sentences identified during the field study and on sentences extracted from the AD80 corpus (Aman et al., 2013) which was built with the aim of elderly voice study.]
Sentence
   sofa: _InitialView
   begin: 8373
   end: 8575
[ Table 2 gives some examples.]
Sentence
   sofa: _InitialView
   begin: 8575
   end: 8604
[Table 2]
Reference
   sofa: _InitialView
   begin: 8576
   end: 8583
   refId: "T2"
   refType: "table"
[Distress Sentence AD80 Sentence Aïe aïe aïe Aidez-moi Oh là Au secours Merde e-lio, appelle du secours Je suis tombé e-lio appelle ma fille Je peux pas me relever Appelle quelqu’un e-lio Qu’est-ce qu’il m’arrive e-lio, appelle quelqu’un Aïe ! J’ai mal e-lio appelle ma fille Oh là ! Je saigne ! Je me suis blessé e-lio appelle le SAMU]
Paragraph
   sofa: _InitialView
   begin: 8604
   end: 8938
[Distress Sentence AD80 Sentence Aïe aïe aïe Aidez-moi Oh là Au secours Merde e-lio, appelle du secours Je suis tombé e-lio appelle ma fille Je peux pas me relever Appelle quelqu’un e-lio Qu’est-ce qu’il m’arrive e-lio, appelle quelqu’un Aïe ! J’ai mal e-lio appelle ma fille Oh là ! Je saigne ! Je me suis blessé e-lio appelle le SAMU]
Sentence
   sofa: _InitialView
   begin: 8604
   end: 8938
[Before each recording session, the experimenter explained the scenario to the participant. Scenario has following sub- components: physical location, time of the day, activity (what the person intends to do and what eventually hap- pens), gesture (way of falling), and the spoken sentences. The participant rehearsed the scenario several times before final acquisition of data. On average, the duration of acquisition was 2 hours and 30 minutes per person.]
Paragraph
   sofa: _InitialView
   begin: 8938
   end: 9394
[Before each recording session, the experimenter explained the scenario to the participant.]
Sentence
   sofa: _InitialView
   begin: 8938
   end: 9028
[ Scenario has following sub- components: physical location, time of the day, activity (what the person intends to do and what eventually hap- pens), gesture (way of falling), and the spoken sentences.]
Sentence
   sofa: _InitialView
   begin: 9028
   end: 9228
[ The participant rehearsed the scenario several times before final acquisition of data.]
Sentence
   sofa: _InitialView
   begin: 9228
   end: 9315
[ On average, the duration of acquisition was 2 hours and 30 minutes per person.]
Sentence
   sofa: _InitialView
   begin: 9315
   end: 9394
[2.3. Equipment and Tools The studio and the equipment used are shown in Figure 1. Moreover, the DOMUS smart home was equipped with the social inclusion product e-lio 2 . Regarding audio analysis, we conducted a multi-source capture with two wireless Sennheiser microphones. A eW-300-G2 type ME2 microphone was placed on the sus- pended ceiling of the studio, and a SKM-300-G2 microphone was placed on a furniture close to the participant. The recording was performed in the technical room by a computer connected to the microphones high-frequency re- ceivers by National Instruments PCI-6220 8-channel card. A loudspeaker in the control room enabled experimenters to hear the progression of the scene in the studio. The National Instruments PCI-6220 card was controlled by the StreamHIS software (Vacher et al., 2011) developed by the GETALP team that enables data acquisition. Regarding video analysis, we conducted the video capture with two webcam cameras (Sony PSEye: 640x480 60Hz connected by a USB 2.0). Cameras were fixed to the wall, one in front of the Sofa and other at an angle of 45 degrees. Moreover, we acquired depth image with Microsoft Kinect. Synchronization and recording of video streams from all three cameras were performed in the technical room by computer (Intel i3 3.2GHz, 4GB RAM, NVidia GeForce GTS 450 512MB). The tools that enable video acquisition was developed by the team SAARA (based on the OpenCV library). Video processing (background learning and subtraction, body parts segmentation) was performed on a PC cluster, taking advantage of algorithms parallelization on CPU and GPU. Motion tracking and identification of haz- ardous situations was done on a PC (Intel i7 3.5GHz, 4GB RAM, NVIDIA GTX 660 4GB GeFore).]
Paragraph
   sofa: _InitialView
   begin: 9394
   end: 11141
[2.3.]
Sentence
   sofa: _InitialView
   begin: 9394
   end: 9398
[ Equipment and Tools The studio and the equipment used are shown in Figure 1.]
Sentence
   sofa: _InitialView
   begin: 9398
   end: 9475
[Figure 1]
Reference
   sofa: _InitialView
   begin: 9466
   end: 9474
   refId: "F1"
   refType: "fig"
[ Moreover, the DOMUS smart home was equipped with the social inclusion product e-lio 2 .]
Sentence
   sofa: _InitialView
   begin: 9475
   end: 9563
[ Regarding audio analysis, we conducted a multi-source capture with two wireless Sennheiser microphones.]
Sentence
   sofa: _InitialView
   begin: 9563
   end: 9667
[ A eW-300-G2 type ME2 microphone was placed on the sus- pended ceiling of the studio, and a SKM-300-G2 microphone was placed on a furniture close to the participant.]
Sentence
   sofa: _InitialView
   begin: 9667
   end: 9832
[ The recording was performed in the technical room by a computer connected to the microphones high-frequency re- ceivers by National Instruments PCI-6220 8-channel card.]
Sentence
   sofa: _InitialView
   begin: 9832
   end: 10001
[ A loudspeaker in the control room enabled experimenters to hear the progression of the scene in the studio.]
Sentence
   sofa: _InitialView
   begin: 10001
   end: 10109
[ The National Instruments PCI-6220 card was controlled by the StreamHIS software (Vacher et al., 2011) developed by the GETALP team that enables data acquisition.]
Sentence
   sofa: _InitialView
   begin: 10109
   end: 10271
[ Regarding video analysis, we conducted the video capture with two webcam cameras (Sony PSEye: 640x480 60Hz connected by a USB 2.0).]
Sentence
   sofa: _InitialView
   begin: 10271
   end: 10403
[ Cameras were fixed to the wall, one in front of the Sofa and other at an angle of 45 degrees.]
Sentence
   sofa: _InitialView
   begin: 10403
   end: 10497
[ Moreover, we acquired depth image with Microsoft Kinect.]
Sentence
   sofa: _InitialView
   begin: 10497
   end: 10554
[ Synchronization and recording of video streams from all three cameras were performed in the technical room by computer (Intel i3 3.2GHz, 4GB RAM, NVidia GeForce GTS 450 512MB).]
Sentence
   sofa: _InitialView
   begin: 10554
   end: 10731
[ The tools that enable video acquisition was developed by the team SAARA (based on the OpenCV library).]
Sentence
   sofa: _InitialView
   begin: 10731
   end: 10834
[ Video processing (background learning and subtraction, body parts segmentation) was performed on a PC cluster, taking advantage of algorithms parallelization on CPU and GPU.]
Sentence
   sofa: _InitialView
   begin: 10834
   end: 11008
[ Motion tracking and identification of haz- ardous situations was done on a PC (Intel i7 3.5GHz, 4GB RAM, NVIDIA GTX 660 4GB GeFore).]
Sentence
   sofa: _InitialView
   begin: 11008
   end: 11141
[3. The Acquired Corpus]
Paragraph
   sofa: _InitialView
   begin: 11141
   end: 11163
[3. The Acquired Corpus]
Sentence
   sofa: _InitialView
   begin: 11141
   end: 11163
[The C IRDO corpus is divided into three parts, the audio corpus for sound and speech analysis, the video corpus and a third part with the annotations. The corpus is detailed in Sections 3.1. and 3.2.. The audio and video corpus was annotated for the video part, with the Advene software 3 , developed at the LIRIS laboratory, and for the sound part, with Transcriber. The C IRDO corpus is a web-based data library, hosted at LIRIS and accessible for the academic and research pur- pose. The license to use the C IRDO corpus is granted on a case-by-case basis. We can also grant permission to researchers to extend the corpus given they fulfil the required conditions and sign agreement.]
Paragraph
   sofa: _InitialView
   begin: 11163
   end: 11849
[The C IRDO corpus is divided into three parts, the audio corpus for sound and speech analysis, the video corpus and a third part with the annotations.]
Sentence
   sofa: _InitialView
   begin: 11163
   end: 11313
[ The corpus is detailed in Sections 3.1. and 3.2..]
Sentence
   sofa: _InitialView
   begin: 11313
   end: 11363
[ The audio and video corpus was annotated for the video part, with the Advene software 3 , developed at the LIRIS laboratory, and for the sound part, with Transcriber.]
Sentence
   sofa: _InitialView
   begin: 11363
   end: 11530
[ The C IRDO corpus is a web-based data library, hosted at LIRIS and accessible for the academic and research pur- pose.]
Sentence
   sofa: _InitialView
   begin: 11530
   end: 11649
[ The license to use the C IRDO corpus is granted on a case-by-case basis.]
Sentence
   sofa: _InitialView
   begin: 11649
   end: 11722
[ We can also grant permission to researchers to extend the corpus given they fulfil the required conditions and sign agreement.]
Sentence
   sofa: _InitialView
   begin: 11722
   end: 11849
[3.1. Audio Corpus]
Paragraph
   sofa: _InitialView
   begin: 11849
   end: 11866
[3.1. Audio Corpus]
Sentence
   sofa: _InitialView
   begin: 11849
   end: 11866
[When they played the scenarios, some participants produced sighs, grunts, coughs, cries, groans, panting or throat clearings. As our current studies are in the domain of automatic speech recognition, these sounds were not considered during the annotation process, but this can be done in the framework of future studies. In the same way, speeches mixed with sound produced by the fall were ignored. At the end, each speaker uttered between 10 and 65 short sentences or interjections (“ah”, “oh”, “aïe”, etc.) as shown Table 3.]
Paragraph
   sofa: _InitialView
   begin: 11866
   end: 12392
[When they played the scenarios, some participants produced sighs, grunts, coughs, cries, groans, panting or throat clearings.]
Sentence
   sofa: _InitialView
   begin: 11866
   end: 11991
[ As our current studies are in the domain of automatic speech recognition, these sounds were not considered during the annotation process, but this can be done in the framework of future studies.]
Sentence
   sofa: _InitialView
   begin: 11991
   end: 12186
[ In the same way, speeches mixed with sound produced by the fall were ignored.]
Sentence
   sofa: _InitialView
   begin: 12186
   end: 12264
[ At the end, each speaker uttered between 10 and 65 short sentences or interjections (“ah”, “oh”, “aïe”, etc.)]
Sentence
   sofa: _InitialView
   begin: 12264
   end: 12374
[ as shown Table 3.]
Sentence
   sofa: _InitialView
   begin: 12374
   end: 12392
[Table 3]
Reference
   sofa: _InitialView
   begin: 12384
   end: 12391
   refId: "T3"
   refType: "table"
[2 http://www.technosens.fr 3 http://liris.cnrs.fr/advene/]
Paragraph
   sofa: _InitialView
   begin: 12392
   end: 12449
[2 http://www.technosens.fr 3 http://liris.cnrs.fr/advene/]
Sentence
   sofa: _InitialView
   begin: 12392
   end: 12449
[All Call for help]
Paragraph
   sofa: _InitialView
   begin: 12449
   end: 12466
[All Call for help]
Sentence
   sofa: _InitialView
   begin: 12449
   end: 12466
[Nb. of interjections Spk. or short sentences Size (s) Y01 22 14 37.59 Y02 16 15 27.51 Y03 24 21 35.59 Y04 25 15 43.97 Y05 32 21 38.16 Y06 19 15 48.25 Y07 12 12 18.75 Y08 15 12 23.58 Y09 23 21 39.86 Y10 20 19 29.83 Y11 29 27 43.96 Y12 24 21 33.54 Y13 17 14 25.32 S01 65 53 92.07 S02 23 19 31.21 S03 23 21 26.02 S04 24 21 50.33 ALL 413 341 645.54]
Paragraph
   sofa: _InitialView
   begin: 12466
   end: 12810
[Nb. of interjections Spk. or short sentences Size (s) Y01 22 14 37.59 Y02 16 15 27.51 Y03 24 21 35.59 Y04 25 15 43.97 Y05 32 21 38.16 Y06 19 15 48.25 Y07 12 12 18.75 Y08 15 12 23.58 Y09 23 21 39.86 Y10 20 19 29.83 Y11 29 27 43.96 Y12 24 21 33.54 Y13 17 14 25.32 S01 65 53 92.07 S02 23 19 31.21 S03 23 21 26.02 S04 24 21 50.33 ALL 413 341 645.54]
Sentence
   sofa: _InitialView
   begin: 12466
   end: 12810
[ No Occurrence number of each scenario Time]
Paragraph
   sofa: _InitialView
   begin: 12810
   end: 12853
[ No Occurrence number of each scenario Time]
Sentence
   sofa: _InitialView
   begin: 12810
   end: 12853
[Sentences were often close to those identified during the field studies (“je peux pas me relever - I can’t get up”, “e- lio appelle du secours - e-lio call for help”, etc.), some were different (“oh bein on est bien là tiens - oh I am in a sticky situation”). In practice, participants cut some sentences (i.e., inserted a delay between “e-lio” and “appelle ma fille - call my daughter”), uttered some spontaneous sentences, interjections or non-verbal sounds (i.e., groan) The content of the video corpus is displayed in Table 4 As mentioned above, the video corpus includes various types of fall according to the scenarios defined in the Protocol (refer Section 2.2.). Each scenario includes a background sequence (300-600 frames) which usually is required for human body extraction. Description of the corpus is given in Table 4 wherein F1 to F4 are the fall scenarios, B blocked hip and TN1, TN2 true negatives. The total duration of the recordings is equal to 1 hour and 55 minutes, with a total of 162 video segments.]
Paragraph
   sofa: _InitialView
   begin: 12853
   end: 13876
[Sentences were often close to those identified during the field studies (“je peux pas me relever - I can’t get up”, “e- lio appelle du secours - e-lio call for help”, etc.),]
Sentence
   sofa: _InitialView
   begin: 12853
   end: 13026
[ some were different (“oh bein on est bien là tiens - oh I am in a sticky situation”).]
Sentence
   sofa: _InitialView
   begin: 13026
   end: 13112
[ In practice, participants cut some sentences (i.e., inserted a delay between “e-lio” and “appelle ma fille - call my daughter”), uttered some spontaneous sentences, interjections or non-verbal sounds (i.e., groan) The content of the video corpus is displayed in Table 4 As mentioned above, the video corpus includes various types of fall according to the scenarios defined in the Protocol (refer Section 2.2.).]
Sentence
   sofa: _InitialView
   begin: 13112
   end: 13523
[Table 4]
Reference
   sofa: _InitialView
   begin: 13375
   end: 13382
   refId: "T4"
   refType: "table"
[ Each scenario includes a background sequence (300-600 frames) which usually is required for human body extraction.]
Sentence
   sofa: _InitialView
   begin: 13523
   end: 13638
[ Description of the corpus is given in Table 4 wherein F1 to F4 are the fall scenarios, B blocked hip and TN1, TN2 true negatives.]
Sentence
   sofa: _InitialView
   begin: 13638
   end: 13768
[Table 4]
Reference
   sofa: _InitialView
   begin: 13677
   end: 13684
   refId: "T4"
   refType: "table"
[ The total duration of the recordings is equal to 1 hour and 55 minutes, with a total of 162 video segments.]
Sentence
   sofa: _InitialView
   begin: 13768
   end: 13876
[3.2. Video Corpus]
Paragraph
   sofa: _InitialView
   begin: 13876
   end: 13893
[3.2. Video Corpus]
Sentence
   sofa: _InitialView
   begin: 13876
   end: 13893
[4. Conclusion Drawing from Feedback from the Participants]
Paragraph
   sofa: _InitialView
   begin: 13893
   end: 13950
[4. Conclusion Drawing from Feedback from the Participants]
Sentence
   sofa: _InitialView
   begin: 13893
   end: 13950
[Feedback from the participants were analyzed and allowed some conclusions (Body-Bekkadja et al., 2015). A high- lighted domestic accident, outlined (for oneself and for others) by C IRDO , leads the elderly individual to relate to the falls in other ways. They suddenly attempt to further regu- late and control it, sometimes by risking a profound change in behavior that may not be detected by C IRDO . More specifically, in our various studies (simulated C IRDO use), it was observed that: The elderly developed two oppos- ing conducts in the management of their fall monitored via C IRDO . Some of them chose to boost their falling movement or overplay screams to make sure the device will recognize the danger. They were also the same elderly that, as we mentioned in Study 2, used tangible remote monitoring systems (inset). They doubted the ability of the new am- bient system to detect their accident, and thus they risk a more serious injury. Others, however, in order to hide these incidents from the entourage, try to control the fall: They do not scream, and try to recover at all costs, even though it could exacerbate the deleterious effects of the fall. In both cases, we see that the falling movement choreography (falling is unintentional by nature) will be intentionally modified by the subjects, whether to be seen to fall, and, hence, be better recognized, or otherwise to be hidden from the technology and its supervision. Thus, the presence of technological artifacts changes the dynamic of the fall, because this behaviour will be addressed, directed toward a target (technology object), and also to other involved individuals. As mentioned by Clot (Clot, 1999), the movements of the falls are not only directed by the conduct of the sub- ject; they are also directed through the use of the technical object, and toward others (the representation they have of the system and the possible recipients of the alert). The fall choreography is influenced by what the individual wants to show to, or hide from, the device. Therefore, the risk of non-detecting these movements is possible, because they sometimes tend to veer too far from programmed scripts. Another more symbolical consequence involves the status and the legitimacy that C IRDO will give the fall. While, in the past, the word of the victim, or of a third party, used to be enough to prove the occurrence of a fall, these days, surveillance technologies are used to validate/approve the occurrence of an incident. We could even add that, they also assess whether this fall is acceptable, and conform to pre-defined scripts. In other words, the non-detection (non- recognition) of a fall by C IRDO may mean that: (1) The individual has poorly conducted the choreography of the falling; or (2) the movement cannot be categorized as a risk movement. This can therefore lead to a denial of risk and recognition of the elderly individual’s status of "victim", thereby throwing suspicion on their statements. As a result, C IRDO may operate a transfer of the risk recognition: It is the technology that gives official status to a fall in a domestic incident. It gives credibility and legitimacy to it. Thus, technological reliability wins against the word of the elderly; and then the latter can be discredited, for ex- ample, if the user talks about incidents not recognized by technology: “If the system is not triggered, then the fall did not occur." In both cases, setting C IRDO service in the domestic social system can either redefine the falls choreography, or redefine their status. All these reasons may be caused by malfunction or rejection. Several recommendations can be addressed to the design- ers. In addition to reassuring, involving, and properly training to use the new system (Hwang and Thorn, 1999)(Ku- jala, 2003), by including demonstrations and updates in real situations adapted to their habits and lifestyle practices (to demonstrate the system works efficiently in detecting falls, and avoiding any worsening of movements), design- ers should create programs and falls-detection algorithms more flexible, to cover a larger spectrum of falls choreography, and not rely solely on fixed scripts in risk behavior. In the same way, effective articulation between Audio and Video detections should be ensured to allow better falls- recognition and validation. As part of a participatory design through use (He and King, 2008), it would be interesting to adjust from feed- backs in the field, the location of the sensors and the level of falls scripts detection, by taking into account conduct- adjustments and daily risk evolutions from the moment the technological artifact is introduced. This requires a situated analysis of the actual device’s uses, in order to re-design it from its usage (Norman and Draper, 1986). This reinforced monitoring system will ask the elderly to make an oral (con- firmation) emergency call to an outside third party. This alert is then triggered only in case of a positive response, or a lack of response from the individual, enabling them to properly control the system (at to avoid false alarms). Finally, this new device for monitoring the activity will be meaningful only if it serves and supports its users’ quality of life. Therefore, one must be careful to involve and assist the various participants during all phases of C IRDO design, in an inclusive approach (as in the Living lab approach of (Pino et al., 2015), especially to anticipate reconfigurations at work in the social framework, as we shall now see.]
Paragraph
   sofa: _InitialView
   begin: 13950
   end: 19500
[Feedback from the participants were analyzed and allowed some conclusions (Body-Bekkadja et al., 2015).]
Sentence
   sofa: _InitialView
   begin: 13950
   end: 14053
[ A high- lighted domestic accident, outlined (for oneself and for others) by C IRDO , leads the elderly individual to relate to the falls in other ways.]
Sentence
   sofa: _InitialView
   begin: 14053
   end: 14205
[ They suddenly attempt to further regu- late and control it, sometimes by risking a profound change in behavior that may not be detected by C IRDO .]
Sentence
   sofa: _InitialView
   begin: 14205
   end: 14353
[ More specifically, in our various studies (simulated C IRDO use), it was observed that: The elderly developed two oppos- ing conducts in the management of their fall monitored via C IRDO .]
Sentence
   sofa: _InitialView
   begin: 14353
   end: 14542
[ Some of them chose to boost their falling movement or overplay screams to make sure the device will recognize the danger.]
Sentence
   sofa: _InitialView
   begin: 14542
   end: 14664
[ They were also the same elderly that, as we mentioned in Study 2, used tangible remote monitoring systems (inset).]
Sentence
   sofa: _InitialView
   begin: 14664
   end: 14779
[ They doubted the ability of the new am- bient system to detect their accident, and thus they risk a more serious injury.]
Sentence
   sofa: _InitialView
   begin: 14779
   end: 14900
[ Others, however, in order to hide these incidents from the entourage, try to control the fall: They do not scream, and try to recover at all costs, even though it could exacerbate the deleterious effects of the fall.]
Sentence
   sofa: _InitialView
   begin: 14900
   end: 15117
[ In both cases, we see that the falling movement choreography (falling is unintentional by nature) will be intentionally modified by the subjects, whether to be seen to fall, and, hence, be better recognized, or otherwise to be hidden from the technology and its supervision.]
Sentence
   sofa: _InitialView
   begin: 15117
   end: 15392
[ Thus, the presence of technological artifacts changes the dynamic of the fall, because this behaviour will be addressed, directed toward a target (technology object), and also to other involved individuals.]
Sentence
   sofa: _InitialView
   begin: 15392
   end: 15599
[ As mentioned by Clot (Clot, 1999), the movements of the falls are not only directed by the conduct of the sub- ject; they are also directed through the use of the technical object, and toward others (the representation they have of the system and the possible recipients of the alert).]
Sentence
   sofa: _InitialView
   begin: 15599
   end: 15885
[ The fall choreography is influenced by what the individual wants to show to, or hide from, the device.]
Sentence
   sofa: _InitialView
   begin: 15885
   end: 15988
[ Therefore, the risk of non-detecting these movements is possible, because they sometimes tend to veer too far from programmed scripts.]
Sentence
   sofa: _InitialView
   begin: 15988
   end: 16123
[ Another more symbolical consequence involves the status and the legitimacy that C IRDO will give the fall.]
Sentence
   sofa: _InitialView
   begin: 16123
   end: 16230
[ While, in the past, the word of the victim, or of a third party, used to be enough to prove the occurrence of a fall, these days, surveillance technologies are used to validate/approve the occurrence of an incident.]
Sentence
   sofa: _InitialView
   begin: 16230
   end: 16446
[ We could even add that, they also assess whether this fall is acceptable, and conform to pre-defined scripts.]
Sentence
   sofa: _InitialView
   begin: 16446
   end: 16556
[ In other words, the non-detection (non- recognition) of a fall by C IRDO may mean that: (1) The individual has poorly conducted the choreography of the falling; or (2) the movement cannot be categorized as a risk movement.]
Sentence
   sofa: _InitialView
   begin: 16556
   end: 16779
[ This can therefore lead to a denial of risk and recognition of the elderly individual’s status of "victim", thereby throwing suspicion on their statements.]
Sentence
   sofa: _InitialView
   begin: 16779
   end: 16935
[ As a result, C IRDO may operate a transfer of the risk recognition: It is the technology that gives official status to a fall in a domestic incident.]
Sentence
   sofa: _InitialView
   begin: 16935
   end: 17085
[ It gives credibility and legitimacy to it.]
Sentence
   sofa: _InitialView
   begin: 17085
   end: 17128
[ Thus, technological reliability wins against the word of the elderly; and then the latter can be discredited, for ex- ample, if the user talks about incidents not recognized by technology: “If the system is not triggered, then the fall did not occur."]
Sentence
   sofa: _InitialView
   begin: 17128
   end: 17380
[ In both cases, setting C IRDO service in the domestic social system can either redefine the falls choreography, or redefine their status.]
Sentence
   sofa: _InitialView
   begin: 17380
   end: 17518
[ All these reasons may be caused by malfunction or rejection.]
Sentence
   sofa: _InitialView
   begin: 17518
   end: 17579
[ Several recommendations can be addressed to the design- ers.]
Sentence
   sofa: _InitialView
   begin: 17579
   end: 17640
[ In addition to reassuring, involving, and properly training to use the new system (Hwang and Thorn, 1999)(Ku- jala, 2003), by including demonstrations and updates in real situations adapted to their habits and lifestyle practices (to demonstrate the system works efficiently in detecting falls, and avoiding any worsening of movements), design- ers should create programs and falls-detection algorithms more flexible, to cover a larger spectrum of falls choreography, and not rely solely on fixed scripts in risk behavior.]
Sentence
   sofa: _InitialView
   begin: 17640
   end: 18163
[ In the same way, effective articulation between Audio and Video detections should be ensured to allow better falls- recognition and validation.]
Sentence
   sofa: _InitialView
   begin: 18163
   end: 18307
[ As part of a participatory design through use (He and King, 2008), it would be interesting to adjust from feed- backs in the field, the location of the sensors and the level of falls scripts detection, by taking into account conduct- adjustments and daily risk evolutions from the moment the technological artifact is introduced.]
Sentence
   sofa: _InitialView
   begin: 18307
   end: 18637
[ This requires a situated analysis of the actual device’s uses, in order to re-design it from its usage (Norman and Draper, 1986).]
Sentence
   sofa: _InitialView
   begin: 18637
   end: 18767
[ This reinforced monitoring system will ask the elderly to make an oral (con- firmation) emergency call to an outside third party.]
Sentence
   sofa: _InitialView
   begin: 18767
   end: 18897
[ This alert is then triggered only in case of a positive response, or a lack of response from the individual, enabling them to properly control the system (at to avoid false alarms).]
Sentence
   sofa: _InitialView
   begin: 18897
   end: 19079
[ Finally, this new device for monitoring the activity will be meaningful only if it serves and supports its users’ quality of life.]
Sentence
   sofa: _InitialView
   begin: 19079
   end: 19210
[ Therefore, one must be careful to involve and assist the various participants during all phases of C IRDO design, in an inclusive approach (as in the Living lab approach of (Pino et al., 2015), especially to anticipate reconfigurations at work in the social framework, as we shall now see.]
Sentence
   sofa: _InitialView
   begin: 19210
   end: 19500
[5. Call for Help Recognition from Audio Analysis]
Paragraph
   sofa: _InitialView
   begin: 19500
   end: 19548
[5. Call for Help Recognition from Audio Analysis]
Sentence
   sofa: _InitialView
   begin: 19500
   end: 19548
[The audio corpus was used for studies related to call for help recognition in Distant Speech conditions thanks to online speech analysis using SGMM acoustic modelling (Vacher et al., 2015a) (Vacher et al., 2015b). Non-speech analysis might bring information of high interest but it was not considered in our study because of the lack of training data. Currently non-speech audio analysis is a relatively unexplored field due not only to the lack of data but also to the unexpected sound classes that can be recorded at home in unconstrained conditions (Vacher et al., 2011). Moreover, non-speech sounds are made of non-verbal sounds (i.e. groans, panting, throat clearings, etc.) and of daily living sounds (i.e. falling objects, door slap, etc.) which represent different semantic information and are difficult to differentiate. For these reasons, only speech analysis was considered.]
Paragraph
   sofa: _InitialView
   begin: 19548
   end: 20433
[The audio corpus was used for studies related to call for help recognition in Distant Speech conditions thanks to online speech analysis using SGMM acoustic modelling (Vacher et al., 2015a) (Vacher et al., 2015b).]
Sentence
   sofa: _InitialView
   begin: 19548
   end: 19761
[ Non-speech analysis might bring information of high interest but it was not considered in our study because of the lack of training data.]
Sentence
   sofa: _InitialView
   begin: 19761
   end: 19899
[ Currently non-speech audio analysis is a relatively unexplored field due not only to the lack of data but also to the unexpected sound classes that can be recorded at home in unconstrained conditions (Vacher et al., 2011).]
Sentence
   sofa: _InitialView
   begin: 19899
   end: 20122
[ Moreover, non-speech sounds are made of non-verbal sounds (i.e.]
Sentence
   sofa: _InitialView
   begin: 20122
   end: 20186
[ groans, panting, throat clearings, etc.)]
Sentence
   sofa: _InitialView
   begin: 20186
   end: 20227
[ and of daily living sounds (i.e.]
Sentence
   sofa: _InitialView
   begin: 20227
   end: 20260
[ falling objects, door slap, etc.)]
Sentence
   sofa: _InitialView
   begin: 20260
   end: 20294
[ which represent different semantic information and are difficult to differentiate.]
Sentence
   sofa: _InitialView
   begin: 20294
   end: 20377
[ For these reasons, only speech analysis was considered.]
Sentence
   sofa: _InitialView
   begin: 20377
   end: 20433
[5.1. Acoustic modelling]
Paragraph
   sofa: _InitialView
   begin: 20433
   end: 20456
[5.1. Acoustic modelling]
Sentence
   sofa: _InitialView
   begin: 20433
   end: 20456
[The Kaldi speech recognition tool-kit (Povey et al., 2011b) was chosen as ASR system. Kaldi is an open-source state- of-the-art Automatic Speech Recognition (ASR) system with a high number of tools and a strong support from the community. In the experiments, the acoustic models were context-dependent classical three-state left-right HMMs. Acoustic features were based on Mel-frequency cepstral coefficients, 13 MFCC-features coefficients were first extracted and then expanded with delta and double delta features and energy (40 features). Acoustic models were composed of 11,000 context-dependent states and 150,000 Gaussians. The state tying is performed using a decision tree based on a tree-clustering of the phones. In addition, off-line fMLLR linear transformation acoustic adap- tation was performed. The acoustic models were trained on 500 hours of transcribed French speech composed of the ESTER 1&2 (broadcast news and conversational speech recorded on the radio) and REPERE (TV news and talk-shows) challenges as well as from 7 hours of transcribed French speech of the S WEET -H OME corpus (Vacher et al., 2014) which consists of records of 60 speakers interacting within a smart home and from 28 minutes of the Voix-détresse corpus (Aman, 2014) which is made of records of speakers eliciting a distress emotion.]
Paragraph
   sofa: _InitialView
   begin: 20456
   end: 21782
[The Kaldi speech recognition tool-kit (Povey et al., 2011b) was chosen as ASR system.]
Sentence
   sofa: _InitialView
   begin: 20456
   end: 20541
[ Kaldi is an open-source state- of-the-art Automatic Speech Recognition (ASR) system with a high number of tools and a strong support from the community.]
Sentence
   sofa: _InitialView
   begin: 20541
   end: 20694
[ In the experiments, the acoustic models were context-dependent classical three-state left-right HMMs.]
Sentence
   sofa: _InitialView
   begin: 20694
   end: 20796
[ Acoustic features were based on Mel-frequency cepstral coefficients, 13 MFCC-features coefficients were first extracted and then expanded with delta and double delta features and energy (40 features).]
Sentence
   sofa: _InitialView
   begin: 20796
   end: 20997
[ Acoustic models were composed of 11,000 context-dependent states and 150,000 Gaussians.]
Sentence
   sofa: _InitialView
   begin: 20997
   end: 21085
[ The state tying is performed using a decision tree based on a tree-clustering of the phones.]
Sentence
   sofa: _InitialView
   begin: 21085
   end: 21178
[ In addition, off-line fMLLR linear transformation acoustic adap- tation was performed.]
Sentence
   sofa: _InitialView
   begin: 21178
   end: 21265
[ The acoustic models were trained on 500 hours of transcribed French speech composed of the ESTER 1&2 (broadcast news and conversational speech recorded on the radio) and REPERE (TV news and talk-shows) challenges as well as from 7 hours of transcribed French speech of the S WEET -H OME corpus (Vacher et al., 2014) which consists of records of 60 speakers interacting within a smart home and from 28 minutes of the Voix-détresse corpus (Aman, 2014) which is made of records of speakers eliciting a distress emotion.]
Sentence
   sofa: _InitialView
   begin: 21265
   end: 21782
[5.1.1. Subspace GMM Acoustic Modelling]
Paragraph
   sofa: _InitialView
   begin: 21782
   end: 21820
[5.1.1. Subspace GMM Acoustic Modelling]
Sentence
   sofa: _InitialView
   begin: 21782
   end: 21820
[The GMM and Subspace GMM (SGMM) both model emis- sion probability of each HMM state with a Gaussian mixture model, but in the SGMM approach, the Gaussian means and the mixture component weights are generated from the phonetic and speaker subspaces along with a set of weight projections. The SGMM model (Povey et al., 2011a) is described in the following equations:]
Paragraph
   sofa: _InitialView
   begin: 21820
   end: 22185
[The GMM and Subspace GMM (SGMM) both model emis- sion probability of each HMM state with a Gaussian mixture model, but in the SGMM approach, the Gaussian means and the mixture component weights are generated from the phonetic and speaker subspaces along with a set of weight projections.]
Sentence
   sofa: _InitialView
   begin: 21820
   end: 22107
[ The SGMM model (Povey et al., 2011a) is described in the following equations:]
Sentence
   sofa: _InitialView
   begin: 22107
   end: 22185
[where x denotes the feature vector, j ∈ {1..J} is the HMM state, i is the Gaussian index, m is the substate and c jm is the substate weight. Each state j is associated to a vector v jm ∈ R S (S is the phonetic subspace dimension) which derives the means, μ jmi and mixture weights, w jmi and it has a shared number of Gaussians, I. The phonetic subspace M i , weight projections w i T and covariance matrices Σ i i.e; the globally shared parameters Φ i = {M i , w i T , Σ i } are common across all states. These parameters can be shared and estimated over multiple record conditions. A generic mixture of I Gaussians, denoted as Universal Background Model (UBM), models all the speech training data for the initialization of the SGMM. Our experiments aims at obtaining SGMM shared parameters using both S WEET -H OME data (7h), Voix-détresse (28mn) and clean data (ESTER+REPERE 500h). Regarding the GMM part, the three training data set are just merged in a single one. (Povey et al., 2011a) showed that the model is also effective with large amount of training data. Therefore, three UBMs were trained respectively on S WEET -H OME data, Voix-détresse and clean data. These tree UBMs contained 1K gaussians and were merged into a single one mixed down to 1K gaussian (closest Gaussians pairs were merged (Zouari and Chollet, 2006)). The aim is to bias specifically the acoustic model towards distant speech home and expressive speech conditions.]
Paragraph
   sofa: _InitialView
   begin: 22185
   end: 23631
[where x denotes the feature vector, j ∈ {1..J} is the HMM state, i is the Gaussian index, m is the substate and c jm is the substate weight.]
Sentence
   sofa: _InitialView
   begin: 22185
   end: 22325
[ Each state j is associated to a vector v jm ∈ R S (S is the phonetic subspace dimension) which derives the means, μ jmi and mixture weights, w jmi and it has a shared number of Gaussians, I.]
Sentence
   sofa: _InitialView
   begin: 22325
   end: 22516
[ The phonetic subspace M i , weight projections w i T and covariance matrices Σ i i.e; the globally shared parameters Φ i = {M i , w i T , Σ i } are common across all states.]
Sentence
   sofa: _InitialView
   begin: 22516
   end: 22690
[ These parameters can be shared and estimated over multiple record conditions.]
Sentence
   sofa: _InitialView
   begin: 22690
   end: 22768
[ A generic mixture of I Gaussians, denoted as Universal Background Model (UBM), models all the speech training data for the initialization of the SGMM.]
Sentence
   sofa: _InitialView
   begin: 22768
   end: 22919
[ Our experiments aims at obtaining SGMM shared parameters using both S WEET -H OME data (7h), Voix-détresse (28mn) and clean data (ESTER+REPERE 500h).]
Sentence
   sofa: _InitialView
   begin: 22919
   end: 23069
[ Regarding the GMM part, the three training data set are just merged in a single one.]
Sentence
   sofa: _InitialView
   begin: 23069
   end: 23154
[ (Povey et al., 2011a) showed that the model is also effective with large amount of training data.]
Sentence
   sofa: _InitialView
   begin: 23154
   end: 23252
[ Therefore, three UBMs were trained respectively on S WEET -H OME data, Voix-détresse and clean data.]
Sentence
   sofa: _InitialView
   begin: 23252
   end: 23353
[ These tree UBMs contained 1K gaussians and were merged into a single one mixed down to 1K gaussian (closest Gaussians pairs were merged (Zouari and Chollet, 2006)).]
Sentence
   sofa: _InitialView
   begin: 23353
   end: 23518
[ The aim is to bias specifically the acoustic model towards distant speech home and expressive speech conditions.]
Sentence
   sofa: _InitialView
   begin: 23518
   end: 23631
[5.2. Recognition of distress calls]
Paragraph
   sofa: _InitialView
   begin: 23631
   end: 23665
[5.2. Recognition of distress calls]
Sentence
   sofa: _InitialView
   begin: 23631
   end: 23665
[The recognition of distress calls consists in computing the phonetic distance of an hypothesis to a list of predefined distress calls. Each ASR hypothesis H i is phonetized, every predefined voice command T j is aligned to H i using Levenshtein distance. The deletion, insertion and substitu- tion costs were computed empirically while the cumulative distance γ(i, j) between H j and T i is given by Equation 2.]
Paragraph
   sofa: _InitialView
   begin: 23665
   end: 24076
[The recognition of distress calls consists in computing the phonetic distance of an hypothesis to a list of predefined distress calls.]
Sentence
   sofa: _InitialView
   begin: 23665
   end: 23799
[ Each ASR hypothesis H i is phonetized, every predefined voice command T j is aligned to H i using Levenshtein distance.]
Sentence
   sofa: _InitialView
   begin: 23799
   end: 23919
[ The deletion, insertion and substitu- tion costs were computed empirically while the cumulative distance γ(i, j) between H j and T i is given by Equation 2.]
Sentence
   sofa: _InitialView
   begin: 23919
   end: 24076
[The decision to select or not a detected sentence is then taken according a detection threshold on the aligned sym- bol score (phonems) of each identified call. This approach takes into account some recognition errors like word end- ings or light variations. Moreover, in a lot of cases, a miss- decoded word is phonetically close to the good one (due to the close pronunciation). From this the CER (Call Error Rate i.e., distress call error rate) is defined as: Number of missed calls CER = (3) Number of calls This measure was chosen because of the content of the corpus Cirdo-set used in this study. Indeed, this corpus is made of sentences and interjections. All sentences are calls for help, without any other kind of sentences (e.g., colloquial sentences), and therefore it is not possible to determine a false alarm rate in this framework.]
Paragraph
   sofa: _InitialView
   begin: 24076
   end: 24922
[The decision to select or not a detected sentence is then taken according a detection threshold on the aligned sym- bol score (phonems) of each identified call.]
Sentence
   sofa: _InitialView
   begin: 24076
   end: 24236
[ This approach takes into account some recognition errors like word end- ings or light variations.]
Sentence
   sofa: _InitialView
   begin: 24236
   end: 24334
[ Moreover, in a lot of cases, a miss- decoded word is phonetically close to the good one (due to the close pronunciation).]
Sentence
   sofa: _InitialView
   begin: 24334
   end: 24456
[ From this the CER (Call Error Rate i.e., distress call error rate) is defined as: Number of missed calls CER = (3) Number of calls This measure was chosen because of the content of the corpus Cirdo-set used in this study.]
Sentence
   sofa: _InitialView
   begin: 24456
   end: 24678
[ Indeed, this corpus is made of sentences and interjections.]
Sentence
   sofa: _InitialView
   begin: 24678
   end: 24738
[ All sentences are calls for help, without any other kind of sentences (e.g., colloquial sentences), and therefore it is not possible to determine a false alarm rate in this framework.]
Sentence
   sofa: _InitialView
   begin: 24738
   end: 24922
[5.3. Off line experiments]
Paragraph
   sofa: _InitialView
   begin: 24922
   end: 24947
[5.3. Off line experiments]
Sentence
   sofa: _InitialView
   begin: 24922
   end: 24947
[The methods presented in previous sections were run on the Cirdo-set audio corpus presented in Section 3.1. The SGMM model presented in Section 5.1. was used as acoutic model. The generic language model (LM) was estimated from French newswire collected in the Gigaword corpus. It was 1- gram with 13,304 words. Moreover, to reduce the linguistic variability, a 3-gram domain language model, the specialized language model was learnt from the sentences used during the corpus collection described in Section 2.2., with 99 1-gram, 225 2-gram and 273 3-gram models. Finally, the language model was a 3-gram-type which resulted from the combination of the generic LM (with a 10% weight) and the specialized LM (with 90% weight). This combination has been shown as leading to the best WER for domain specific application (Lecouteux et al., 2011). The interest of such combination is to bias the recognition towards the domain LM but when the speaker deviates from the domain, the general LM makes it possible to avoid the recognition of sentences leading to “false- positive” detection. Results on manually annotated data are given Table 5. The most important performance measures are the Word Error Rate (WER) of the overall decoded speech and those of the specific distress calls as well as the Call Error Rate (CER: c.f. equation 3). Considering distress calls only, the average WER is 34.0% whereas it a 39.3% when all interjections and sentences are taken into account. On average, CER is equal to 26.8% with an important dis- parity between the speakers. Unfortunately and as mentioned above, the used corpus does not allow to determine a False Alarm Rate. Previous studies based on the AD80 corpus showed recall, precision and F-measure equal to 88.4%, 86.9% and 87.2% (Aman et al., 2013). Nevertheless, this corpus was recorded in very different conditions, text reading in a studio, in contrary of those of Cirdo-set.]
Paragraph
   sofa: _InitialView
   begin: 24947
   end: 26868
[The methods presented in previous sections were run on the Cirdo-set audio corpus presented in Section 3.1.]
Sentence
   sofa: _InitialView
   begin: 24947
   end: 25054
[ The SGMM model presented in Section 5.1. was used as acoutic model.]
Sentence
   sofa: _InitialView
   begin: 25054
   end: 25122
[ The generic language model (LM) was estimated from French newswire collected in the Gigaword corpus.]
Sentence
   sofa: _InitialView
   begin: 25122
   end: 25223
[ It was 1- gram with 13,304 words.]
Sentence
   sofa: _InitialView
   begin: 25223
   end: 25257
[ Moreover, to reduce the linguistic variability, a 3-gram domain language model, the specialized language model was learnt from the sentences used during the corpus collection described in Section 2.2., with 99 1-gram, 225 2-gram and 273 3-gram models.]
Sentence
   sofa: _InitialView
   begin: 25257
   end: 25509
[ Finally, the language model was a 3-gram-type which resulted from the combination of the generic LM (with a 10% weight) and the specialized LM (with 90% weight).]
Sentence
   sofa: _InitialView
   begin: 25509
   end: 25671
[ This combination has been shown as leading to the best WER for domain specific application (Lecouteux et al., 2011).]
Sentence
   sofa: _InitialView
   begin: 25671
   end: 25788
[ The interest of such combination is to bias the recognition towards the domain LM but when the speaker deviates from the domain, the general LM makes it possible to avoid the recognition of sentences leading to “false- positive” detection.]
Sentence
   sofa: _InitialView
   begin: 25788
   end: 26028
[Results on manually annotated data are given Table 5.]
Sentence
   sofa: _InitialView
   begin: 26029
   end: 26082
[Table 5]
Reference
   sofa: _InitialView
   begin: 26074
   end: 26081
   refId: "T5"
   refType: "table"
[ The most important performance measures are the Word Error Rate (WER) of the overall decoded speech and those of the specific distress calls as well as the Call Error Rate (CER: c.f.]
Sentence
   sofa: _InitialView
   begin: 26082
   end: 26265
[ equation 3).]
Sentence
   sofa: _InitialView
   begin: 26265
   end: 26278
[ Considering distress calls only, the average WER is 34.0% whereas it a 39.3% when all interjections and sentences are taken into account.]
Sentence
   sofa: _InitialView
   begin: 26278
   end: 26416
[ On average, CER is equal to 26.8% with an important dis- parity between the speakers.]
Sentence
   sofa: _InitialView
   begin: 26416
   end: 26502
[ Unfortunately and as mentioned above, the used corpus does not allow to determine a False Alarm Rate.]
Sentence
   sofa: _InitialView
   begin: 26502
   end: 26604
[ Previous studies based on the AD80 corpus showed recall, precision and F-measure equal to 88.4%, 86.9% and 87.2% (Aman et al., 2013).]
Sentence
   sofa: _InitialView
   begin: 26604
   end: 26738
[ Nevertheless, this corpus was recorded in very different conditions, text reading in a studio, in contrary of those of Cirdo-set.]
Sentence
   sofa: _InitialView
   begin: 26738
   end: 26868
[ WER (%) WER (%)]
Paragraph
   sofa: _InitialView
   begin: 26868
   end: 26884
[ WER (%) WER (%)]
Sentence
   sofa: _InitialView
   begin: 26868
   end: 26884
[6. Fall Detection from Video Analysis]
Paragraph
   sofa: _InitialView
   begin: 26884
   end: 26921
[6. Fall Detection from Video Analysis]
Sentence
   sofa: _InitialView
   begin: 26884
   end: 26921
[The video analysis framework for fall events labeling is based on the silhouette extraction. The silhouette is extracted by removal of background pixels within the video scene using. The background subtraction is performed by Mixture of Gaussian based approach, and eigenbackground (PCA) approach (Deeb et al., 2012), (Priyank Shah, 2014). The extracted shape is then used to obtain discriminative features to detect anomaly in person’s movement in the scene. The overview of proposed framework is shown in Figure 3.]
Paragraph
   sofa: _InitialView
   begin: 26921
   end: 27437
[The video analysis framework for fall events labeling is based on the silhouette extraction.]
Sentence
   sofa: _InitialView
   begin: 26921
   end: 27013
[ The silhouette is extracted by removal of background pixels within the video scene using.]
Sentence
   sofa: _InitialView
   begin: 27013
   end: 27103
[ The background subtraction is performed by Mixture of Gaussian based approach, and eigenbackground (PCA) approach (Deeb et al., 2012), (Priyank Shah, 2014).]
Sentence
   sofa: _InitialView
   begin: 27103
   end: 27260
[ The extracted shape is then used to obtain discriminative features to detect anomaly in person’s movement in the scene.]
Sentence
   sofa: _InitialView
   begin: 27260
   end: 27380
[ The overview of proposed framework is shown in Figure 3.]
Sentence
   sofa: _InitialView
   begin: 27380
   end: 27437
[Figure 3]
Reference
   sofa: _InitialView
   begin: 27428
   end: 27436
   refId: "F3"
   refType: "fig"
[6.1. Foreground Extraction and Modeling of Body Parts]
Paragraph
   sofa: _InitialView
   begin: 27437
   end: 27490
[6.1. Foreground Extraction and Modeling of Body Parts]
Sentence
   sofa: _InitialView
   begin: 27437
   end: 27490
[After silhouette extraction the human body is segmented into colored connected components. Colored connected components regarded as regions are extracted by applying region growing technique. Each component with similar color are fitted into blob (Hsieh et al., 2010). Blob is repre- sented by spatial color Gaussian mixture model, assuming that spatial color components are de-correlated. The probability of an observation X t of that pixel at time t to belong to the background is given by equation 4.]
Paragraph
   sofa: _InitialView
   begin: 27490
   end: 27993
[After silhouette extraction the human body is segmented into colored connected components.]
Sentence
   sofa: _InitialView
   begin: 27490
   end: 27580
[ Colored connected components regarded as regions are extracted by applying region growing technique.]
Sentence
   sofa: _InitialView
   begin: 27580
   end: 27681
[ Each component with similar color are fitted into blob (Hsieh et al., 2010).]
Sentence
   sofa: _InitialView
   begin: 27681
   end: 27758
[ Blob is repre- sented by spatial color Gaussian mixture model, assuming that spatial color components are de-correlated.]
Sentence
   sofa: _InitialView
   begin: 27758
   end: 27879
[ The probability of an observation X t of that pixel at time t to belong to the background is given by equation 4.]
Sentence
   sofa: _InitialView
   begin: 27879
   end: 27993
[where L is the number of gaussian, η is Gaussian probability density function, Σ k,t is an estimates of weight and ω k,t is covariance matrix of the k-th Gaussian in the mixture at time t. In the last, an energy based function is formulated over the unknown labels of every pixel in the form of a first order Markov random field (MRF) energy function:]
Paragraph
   sofa: _InitialView
   begin: 27993
   end: 28344
[where L is the number of gaussian, η is Gaussian probability density function, Σ k,t is an estimates of weight and ω k,t is covariance matrix of the k-th Gaussian in the mixture at time t. In the last, an energy based function is formulated over the unknown labels of every pixel in the form of a first order Markov random field (MRF) energy function:]
Sentence
   sofa: _InitialView
   begin: 27993
   end: 28344
[Here, N e is neighboring pixels and the data energy Σ p∈P D p (f p ) evaluates the likelihood of each pixel to take a label. Finally, energy function is minimized with a graph cut algorithm via a swap approach (Miguel Angel Bautista, 2015). Background model is updated selectively with an online EM algorithm (Moon, 1996).]
Paragraph
   sofa: _InitialView
   begin: 28344
   end: 28666
[Here, N e is neighboring pixels and the data energy Σ p∈P D p (f p ) evaluates the likelihood of each pixel to take a label.]
Sentence
   sofa: _InitialView
   begin: 28344
   end: 28468
[ Finally, energy function is minimized with a graph cut algorithm via a swap approach (Miguel Angel Bautista, 2015).]
Sentence
   sofa: _InitialView
   begin: 28468
   end: 28584
[ Background model is updated selectively with an online EM algorithm (Moon, 1996).]
Sentence
   sofa: _InitialView
   begin: 28584
   end: 28666
[6.2. Tracking of Body Parts]
Paragraph
   sofa: _InitialView
   begin: 28666
   end: 28693
[6.2. Tracking of Body Parts]
Sentence
   sofa: _InitialView
   begin: 28666
   end: 28693
[Our method begins with an iterative process and considers that person is in standing position. Body is decomposed in three parts i.e. head, torso and lower part. Then, dynamic processing technique is used to obtain different level of body details (i.e. number of blobs), if required. At first, a blob (k-th blob) is built for each connected component with a feature vector containing: color, center’s coordinates C k , a predicted change of color P k and velocity V k . Set of current colored connected components are extracted from the smoothed foreground by region growing. By considering tracking of blobs as a matching process, we introduce a novel cost function to obtain distance between two blobs. The cost function is given by: Cost(j, k) = α 1 ( P k −P j t /V k )+α 2 ( C j −C k M / C k ) (6) where C j t and P j t respectively, the color and the center’s coordinates of the j-th connected component extracted at time t.]
Paragraph
   sofa: _InitialView
   begin: 28693
   end: 29622
[Our method begins with an iterative process and considers that person is in standing position.]
Sentence
   sofa: _InitialView
   begin: 28693
   end: 28787
[ Body is decomposed in three parts i.e.]
Sentence
   sofa: _InitialView
   begin: 28787
   end: 28826
[ head, torso and lower part.]
Sentence
   sofa: _InitialView
   begin: 28826
   end: 28854
[ Then, dynamic processing technique is used to obtain different level of body details (i.e.]
Sentence
   sofa: _InitialView
   begin: 28854
   end: 28945
[ number of blobs), if required.]
Sentence
   sofa: _InitialView
   begin: 28945
   end: 28976
[ At first, a blob (k-th blob) is built for each connected component with a feature vector containing: color, center’s coordinates C k , a predicted change of color P k and velocity V k .]
Sentence
   sofa: _InitialView
   begin: 28976
   end: 29162
[ Set of current colored connected components are extracted from the smoothed foreground by region growing.]
Sentence
   sofa: _InitialView
   begin: 29162
   end: 29268
[ By considering tracking of blobs as a matching process, we introduce a novel cost function to obtain distance between two blobs.]
Sentence
   sofa: _InitialView
   begin: 29268
   end: 29397
[ The cost function is given by: Cost(j, k) = α 1 ( P k −P j t /V k )+α 2 ( C j −C k M / C k ) (6) where C j t and P j t respectively, the color and the center’s coordinates of the j-th connected component extracted at time t.]
Sentence
   sofa: _InitialView
   begin: 29397
   end: 29622
[6.3. Pose Recognition and Labeling of Events]
Paragraph
   sofa: _InitialView
   begin: 29622
   end: 29666
[6.3. Pose Recognition and Labeling of Events]
Sentence
   sofa: _InitialView
   begin: 29622
   end: 29666
[The labeling of events is based on the scenario define above. To identify poses, the extracted silhouette from the body parts was used. To recognize and label different poses from the video data, an histogram based approach was used as they runs in real time and improves its accu- racy with the passage of run time The recognition wss based on comparison of two histograms i.e. key pose frame histogram and histogram of frame in hand (Barnachon et al., 2012). The histograms were then compared with the Bhattacharyya distance and warped by a dynamic time warping process to achieve their optimal alignment (Barnachon et al., 2014). Bhattacharyya distance is calculated using:]
Paragraph
   sofa: _InitialView
   begin: 29666
   end: 30342
[The labeling of events is based on the scenario define above.]
Sentence
   sofa: _InitialView
   begin: 29666
   end: 29727
[ To identify poses, the extracted silhouette from the body parts was used.]
Sentence
   sofa: _InitialView
   begin: 29727
   end: 29801
[ To recognize and label different poses from the video data, an histogram based approach was used as they runs in real time and improves its accu- racy with the passage of run time The recognition wss based on comparison of two histograms i.e.]
Sentence
   sofa: _InitialView
   begin: 29801
   end: 30044
[ key pose frame histogram and histogram of frame in hand (Barnachon et al., 2012).]
Sentence
   sofa: _InitialView
   begin: 30044
   end: 30126
[ The histograms were then compared with the Bhattacharyya distance and warped by a dynamic time warping process to achieve their optimal alignment (Barnachon et al., 2014).]
Sentence
   sofa: _InitialView
   begin: 30126
   end: 30298
[ Bhattacharyya distance is calculated using:]
Sentence
   sofa: _InitialView
   begin: 30298
   end: 30342
[Where H 1 and H 2 are integral histograms. In this study we focused on the analysis of posture to detect event of distress. Our framework for fall detection could further be enhanced by considering other factors as well, i.e. facial expression analysis. Analysis of facial expressions to detect pain can be very advantageous in case of prolonged immobility of elderly person (Khan et al., 2013).]
Paragraph
   sofa: _InitialView
   begin: 30342
   end: 30737
[Where H 1 and H 2 are integral histograms.]
Sentence
   sofa: _InitialView
   begin: 30342
   end: 30384
[ In this study we focused on the analysis of posture to detect event of distress.]
Sentence
   sofa: _InitialView
   begin: 30384
   end: 30465
[ Our framework for fall detection could further be enhanced by considering other factors as well, i.e.]
Sentence
   sofa: _InitialView
   begin: 30465
   end: 30567
[ facial expression analysis.]
Sentence
   sofa: _InitialView
   begin: 30567
   end: 30595
[ Analysis of facial expressions to detect pain can be very advantageous in case of prolonged immobility of elderly person (Khan et al., 2013).]
Sentence
   sofa: _InitialView
   begin: 30595
   end: 30737
[7. Conclusion]
Paragraph
   sofa: _InitialView
   begin: 30737
   end: 30750
[7. Conclusion]
Sentence
   sofa: _InitialView
   begin: 30737
   end: 30750
[This paper investigated smart home technology and recorded comprehensive audio/video corpus of domestic falls. For elderly, the fall is one of the most feared and recurring problems. Surveillance technologies tries to provide solution to this issue by alerting contact person in case of fall. There is lot of work which focuses on this problem but to the best of our knowledge there is no common database for comparing results. To address this issue we have made available a database composed of audio/video records of falls and prolonged immobility. In this paper, studies using this database for the automatic identification of these events were also presented. The C IRDO corpus was recorded in a smart environment reproducing a typical living room containing an e-lio communication device equipped with microphone and camera. The experiment was guided by an ethnographic study which detailed the various events related to distress situations faced by elderly people. Various scenarios were estab- lished from this study that describes pattern of postures in case of fall so that acted falls were as close to real situation as possible. The database can thus be useful to researchers of the community studying video or audio emergency situations as well as to model the relationship between audio/video events and learn fall model using machine learning.]
Paragraph
   sofa: _InitialView
   begin: 30750
   end: 32107
[This paper investigated smart home technology and recorded comprehensive audio/video corpus of domestic falls.]
Sentence
   sofa: _InitialView
   begin: 30750
   end: 30860
[ For elderly, the fall is one of the most feared and recurring problems.]
Sentence
   sofa: _InitialView
   begin: 30860
   end: 30932
[ Surveillance technologies tries to provide solution to this issue by alerting contact person in case of fall.]
Sentence
   sofa: _InitialView
   begin: 30932
   end: 31042
[ There is lot of work which focuses on this problem but to the best of our knowledge there is no common database for comparing results.]
Sentence
   sofa: _InitialView
   begin: 31042
   end: 31177
[ To address this issue we have made available a database composed of audio/video records of falls and prolonged immobility.]
Sentence
   sofa: _InitialView
   begin: 31177
   end: 31300
[ In this paper, studies using this database for the automatic identification of these events were also presented.]
Sentence
   sofa: _InitialView
   begin: 31300
   end: 31413
[ The C IRDO corpus was recorded in a smart environment reproducing a typical living room containing an e-lio communication device equipped with microphone and camera.]
Sentence
   sofa: _InitialView
   begin: 31413
   end: 31579
[ The experiment was guided by an ethnographic study which detailed the various events related to distress situations faced by elderly people.]
Sentence
   sofa: _InitialView
   begin: 31579
   end: 31720
[ Various scenarios were estab- lished from this study that describes pattern of postures in case of fall so that acted falls were as close to real situation as possible.]
Sentence
   sofa: _InitialView
   begin: 31720
   end: 31889
[ The database can thus be useful to researchers of the community studying video or audio emergency situations as well as to model the relationship between audio/video events and learn fall model using machine learning.]
Sentence
   sofa: _InitialView
   begin: 31889
   end: 32107
[Acknowledgements]
Paragraph
   sofa: _InitialView
   begin: 32107
   end: 32123
[Acknowledgements]
Sentence
   sofa: _InitialView
   begin: 32107
   end: 32123
[This work was supported by the French funding agen- cies ANR and CNSA through C IRDO project (ANR-2010- TECS-012). The authors would like to thanks the persons who agreed to participate in the survey or in the recordings.]
Paragraph
   sofa: _InitialView
   begin: 32123
   end: 32344
[This work was supported by the French funding agen- cies ANR and CNSA through C IRDO project (ANR-2010- TECS-012).]
Sentence
   sofa: _InitialView
   begin: 32123
   end: 32237
[ The authors would like to thanks the persons who agreed to participate in the survey or in the recordings.]
Sentence
   sofa: _InitialView
   begin: 32237
   end: 32344
[Bibliographical References]
Paragraph
   sofa: _InitialView
   begin: 32344
   end: 32370
[Bibliographical References]
Sentence
   sofa: _InitialView
   begin: 32344
   end: 32370
[Aman, F., Vacher, M., Rossato, S., and Portet, F. (2013). Speech Recognition of Aged Voices in the AAL Context: Detection of Distress Sentences. In The 7th Int. Conf. on Speech Technology and Human-Computer Dialogue, SpeD 2013, pages 177–184. Aman, F. (2014). Reconnaissance automatique de la pa- role de personnes âgées pour les services d’assistance à domicile. Ph.D. thesis, Université de Grenoble, Ecole doctorale MSTII. Barnachon, M., Bouakaz, S., Boufama, B., and Guillou, E. (2012). Human actions recognition from streamed mo- tion capture. In Pattern Recognition (ICPR), Int. Conf. on, pages 3807–3810. IEEE. Barnachon, M., Bouakaz, S., Boufama, B., and Guillou, E. (2014). Ongoing Human Action Recognition with Motion Capture. Pattern Recognition, 47(1):238–247. Bloch, F., Gautier, V., Noury, N., Lundy, J., Poujaud, J., Claessens, Y., and Rigaud, A. (2011). Evaluation under real-life conditions of a stand-alone fall detector for the elderly subjects. Annals of Physical and Rehabilitation Medicine, 54:391–398. Bobillier-Chaumon, M.-E., Cuvillier, B., Bouakaz, S., and Vacher, M. (2012). Démarche de développement de technologies ambiantes pour le maintien à domicile des personnes dépendantes : vers une triangulation des méth- odes et des approches. In Actes du 1er Congrès Eu- ropéen de Stimulation Cognitive, pages 121–122, Dijon, France. Body-Bekkadja, S., Bobillier-Chaumon, M.-E., Cuvillier, B., and Cros, F. (2015). Understanding the Socio- Domestic Activity: A Challenge for the Ambient Technologies Acceptance in the Case of Homecare Assis- tance. In HCI International, volume Part II of LNCS 9194, pages 399–411. Bouakaz, S., Vacher, M., Bobillier-Chaumon, M.-E., FrédéricAman, Bekkadja, Portet, Guillou, E., Rossato, S., Desserée, E., Traineau, P., Vimon, J.-P., and Cheva- lier, T. (2014). CIRDO: Smart companion for helping elderly to live at home for longer. Innovation and Re- search in BioMedical engineering, 35(2):101–108. Clot, Y. (1999). La fonction psychologique du travail. PUF, Paris, France. Deeb, R., Desseree, E., and Bouakaz, S. (2012). Real-time two-level foreground detection and person-silhouette extraction enhanced by body-parts tracking. In Proc. SPIE, volume 83010R, pages 1–8. Hamill, M., Young, V., Boger, J., and Mihailidis, A. (2009). Development of an automated speech recognition interface for personal emergency response systems. Journal of NeuroEngineering and Rehabilitation, 6(1):26. He, J. and King, W. (2008). The role of user participa- tion in information systems development: Implications from a meta-analysis. Journal of Management Information Systems, 25(1):301–331. Hsieh, C., Chuang, S., Chen, S., Chen, C., and Fan, K. (2010). Segmentation of human body parts using de- formable triangulation. IEEE Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans, 40(3):596–610. Hwang, M. and Thorn, R. (1999). The effect of user en- gagement on system success: A meta-analytical integra- tion of research findings. Information and Management, 35(4):229–236. Igual, R., Medrano, C., and Plaza, I. (2013). Challenges, issues and trends in fall detection systems. BioMedical Engineering OnLine, 12(1):1–24. Kangas, M., Vikman, I., Nyberg, L., Korpelainen, R., Lind- blom, J., and Jämsä, T. (2012). Comparison of real-life accidental falls in older people with experimental falls in middle-aged test subjects. Gait & Posture, 35(3):500 – 505. Khan, R. A., Meyer, A., Konik, H., and Bouakaz, S. (2013). Pain detection through shape and appearance features. In Multimedia and Expo (ICME), 2013 IEEE International Conference on, pages 1–6, July. Klenk, J., Becker, C., Lieken, F., Nicolai, S., Maetzler, W., Alt, W., Zijlstra, W., Hausdorff, J., van Lummel, R., Chiari, L., and Lindemann, U. (2011). Comparison of acceleration signals of simulated and real-world back- ward falls. Medical Engineering & Physics, 33(3):368 – 373. Kujala, S. (2003). User involvement: A review of the ben- efits and challenges. Behaviour and Information Technology, 22(1):1–16. Lecouteux, B., Vacher, M., and Portet, F. (2011). Distant Speech Recognition in a Smart Home: Comparison of Several Multisource ASRs in Realistic Conditions. In In- terspeech 2011, pages 1–4, Florence, Italy. Miguel Angel Bautista, Sergio Escalera, D. S. (2015). Hupba8k: Dataset and ecoc-graph-cut based segmentation of human limbs. Neurocomputing, 150(A):173–188. Moon, T. K. (1996). The expectation-maximization algorithm. IEEE Signal Processing Magazine, 13(6):47–60. Norman, D. A. and Draper, S. W. (1986). User Centered System Design; New Perspectives on Human-Computer Interaction. L. Erlbaum Associates Inc. Pantic, M., Pentland, A., Nijholt, A., and Huang, T. (2006). Human computing and machine understanding of human behavior: survey. In ACM Int. Conf. on Multimodal In- terfaces. Pino, M., Moget, C., Benveniste, S., Picard, R., and Rigaud, A.-S. (2015). Innovative technology-based healthcare and support services for older adults: How and why in- dustrial initiatives convert to the living lab approach. In HCI International, volume Part II of LNCS 9194, pages 158–169. Springer International Publishing. Popescu, M., Li, Y., Skubic, M., and Rantz, M. (2008). An acoustic fall detector system that uses sound height information to reduce the false alarm rate. In Proc. 30th Annual Int. Conference of the IEEE-EMBS 2008, pages 4628–4631, 20–25 Aug. Povey, D., Burget, L., Agarwal, M., Akyazi, P., Kai, F., Ghoshal, A., Glembek, O., Goel, N., Karafiát, M., Rastrow, A., Rose, R. C., Schwarz, P., and Thomas, S. (2011a). The subspace gaussian mixture model—a structured model for speech recognition. Computer Speech & Language, 25(2):404 – 439. Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., and Vesely, K. (2011b). The Kaldi Speech Recognition Toolkit. In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Soci- ety, December. IEEE Catalog No.: CFP11SRW-USB. Priyank Shah, H. M. (2014). Comperehensive study and comparative analysis of different types of background subtraction algorithms. International Journal of Im- age,Graphics and Signal Processing, 6(8):47–52. Saim, R. (2014). Accidental falls amongst the elderly: Health impact and effective intervention strategies. Mas- ter’s thesis. Vacher, M., Portet, F., Fleury, A., and Noury, N. (2011). Development of Audio Sensing Technology for Ambient Assisted Living: Applications and Challenges. International journal of E-Health and medical communications, 2(1):35–54, March. Vacher, M., Lecouteux, B., Chahuara, P., Portet, F., Meil- lon, B., and Bonnefond, N. (2014). The Sweet-Home speech and multimodal corpus for home automation interaction. In The 9th edition of the Language Resources and Evaluation Conference (LREC), pages 4499–4506, Reykjavik, Iceland. Vacher, M., Aman, F., Rossato, S., and Portet, F. (2015a). Development of Automatic Speech Recognition Tech- niques for Elderly Home Support: Applications and Challenges. In J. Zou et al., editors, HCI International, volume Part II of LNCS 9194, pages 341–353, Los An- geles, CA, United States, August. Springer International Publishing Switzerland. Vacher, M., Lecouteux, B., Aman, F., Rossato, S., and Portet, F. (2015b). Recognition of Distress Calls in Distant Speech Setting: a Preliminary Experiment in a Smart Home. In 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 1–7, Dresden, Germany, September. SIG-SLPAT. Who. (2013). World population ageing: 1950-2050. Tech- nical report, Executive report, United Nations, Depart- ment of Economic and Social Affairs, Population Divi- sion. Young, V. and Mihailidis, A. (2010). An automated, speech-based emergency response system for the older adult. Gerontechnology, 9(2):261. Young, V. and Mihailidis, A. (2013). The CARES corpus: a database of older adult actor simulated emergency dia- logue for developing a personal emergency response system. I. J. Speech Technology, 16(1):55–73. Zouari, L. and Chollet, G. (2006). Efficient gaussian mixture for speech recognition. In Pattern Recognition, 2006. ICPR 2006. 18th International Conference on, volume 4, pages 294–297.]
Paragraph
   sofa: _InitialView
   begin: 32370
   end: 40634
[Aman, F., Vacher, M., Rossato, S., and Portet, F. (2013).]
Sentence
   sofa: _InitialView
   begin: 32370
   end: 32427
[ Speech Recognition of Aged Voices in the AAL Context: Detection of Distress Sentences.]
Sentence
   sofa: _InitialView
   begin: 32427
   end: 32514
[ In The 7th Int.]
Sentence
   sofa: _InitialView
   begin: 32514
   end: 32530
[ Conf.]
Sentence
   sofa: _InitialView
   begin: 32530
   end: 32536
[ on Speech Technology and Human-Computer Dialogue, SpeD 2013, pages 177–184.]
Sentence
   sofa: _InitialView
   begin: 32536
   end: 32612
[ Aman, F. (2014).]
Sentence
   sofa: _InitialView
   begin: 32612
   end: 32629
[ Reconnaissance automatique de la pa- role de personnes âgées pour les services d’assistance à domicile.]
Sentence
   sofa: _InitialView
   begin: 32629
   end: 32733
[ Ph.D. thesis, Université de Grenoble, Ecole doctorale MSTII.]
Sentence
   sofa: _InitialView
   begin: 32733
   end: 32794
[ Barnachon, M., Bouakaz, S., Boufama, B., and Guillou, E. (2012).]
Sentence
   sofa: _InitialView
   begin: 32794
   end: 32859
[ Human actions recognition from streamed mo- tion capture.]
Sentence
   sofa: _InitialView
   begin: 32859
   end: 32917
[ In Pattern Recognition (ICPR), Int.]
Sentence
   sofa: _InitialView
   begin: 32917
   end: 32953
[ Conf.]
Sentence
   sofa: _InitialView
   begin: 32953
   end: 32959
[ on, pages 3807–3810.]
Sentence
   sofa: _InitialView
   begin: 32959
   end: 32980
[ IEEE.]
Sentence
   sofa: _InitialView
   begin: 32980
   end: 32986
[ Barnachon, M., Bouakaz, S., Boufama, B., and Guillou, E. (2014).]
Sentence
   sofa: _InitialView
   begin: 32986
   end: 33051
[ Ongoing Human Action Recognition with Motion Capture.]
Sentence
   sofa: _InitialView
   begin: 33051
   end: 33105
[ Pattern Recognition, 47(1):238–247.]
Sentence
   sofa: _InitialView
   begin: 33105
   end: 33141
[ Bloch, F., Gautier, V., Noury, N., Lundy, J., Poujaud, J., Claessens, Y., and Rigaud, A.]
Sentence
   sofa: _InitialView
   begin: 33141
   end: 33230
[ (2011).]
Sentence
   sofa: _InitialView
   begin: 33230
   end: 33238
[ Evaluation under real-life conditions of a stand-alone fall detector for the elderly subjects.]
Sentence
   sofa: _InitialView
   begin: 33238
   end: 33333
[ Annals of Physical and Rehabilitation Medicine, 54:391–398.]
Sentence
   sofa: _InitialView
   begin: 33333
   end: 33393
[ Bobillier-Chaumon, M.-E., Cuvillier, B., Bouakaz, S., and Vacher, M. (2012).]
Sentence
   sofa: _InitialView
   begin: 33393
   end: 33470
[ Démarche de développement de technologies ambiantes pour le maintien à domicile des personnes dépendantes : vers une triangulation des méth- odes et des approches.]
Sentence
   sofa: _InitialView
   begin: 33470
   end: 33634
[ In Actes du 1er Congrès Eu- ropéen de Stimulation Cognitive, pages 121–122, Dijon, France.]
Sentence
   sofa: _InitialView
   begin: 33634
   end: 33725
[ Body-Bekkadja, S., Bobillier-Chaumon, M.-E., Cuvillier, B., and Cros, F. (2015).]
Sentence
   sofa: _InitialView
   begin: 33725
   end: 33806
[ Understanding the Socio- Domestic Activity: A Challenge for the Ambient Technologies Acceptance in the Case of Homecare Assis- tance.]
Sentence
   sofa: _InitialView
   begin: 33806
   end: 33940
[ In HCI International, volume Part II of LNCS 9194, pages 399–411.]
Sentence
   sofa: _InitialView
   begin: 33940
   end: 34006
[ Bouakaz, S., Vacher, M., Bobillier-Chaumon, M.-E., FrédéricAman, Bekkadja, Portet, Guillou, E., Rossato, S., Desserée, E., Traineau, P., Vimon, J.-P., and Cheva- lier, T. (2014).]
Sentence
   sofa: _InitialView
   begin: 34006
   end: 34185
[ CIRDO: Smart companion for helping elderly to live at home for longer.]
Sentence
   sofa: _InitialView
   begin: 34185
   end: 34256
[ Innovation and Re- search in BioMedical engineering, 35(2):101–108.]
Sentence
   sofa: _InitialView
   begin: 34256
   end: 34324
[ Clot, Y.]
Sentence
   sofa: _InitialView
   begin: 34324
   end: 34333
[ (1999).]
Sentence
   sofa: _InitialView
   begin: 34333
   end: 34341
[ La fonction psychologique du travail.]
Sentence
   sofa: _InitialView
   begin: 34341
   end: 34379
[ PUF, Paris, France.]
Sentence
   sofa: _InitialView
   begin: 34379
   end: 34399
[ Deeb, R., Desseree, E., and Bouakaz, S. (2012).]
Sentence
   sofa: _InitialView
   begin: 34399
   end: 34447
[ Real-time two-level foreground detection and person-silhouette extraction enhanced by body-parts tracking.]
Sentence
   sofa: _InitialView
   begin: 34447
   end: 34554
[ In Proc.]
Sentence
   sofa: _InitialView
   begin: 34554
   end: 34563
[ SPIE, volume 83010R, pages 1–8.]
Sentence
   sofa: _InitialView
   begin: 34563
   end: 34595
[ Hamill, M., Young, V., Boger, J., and Mihailidis, A.]
Sentence
   sofa: _InitialView
   begin: 34595
   end: 34648
[ (2009).]
Sentence
   sofa: _InitialView
   begin: 34648
   end: 34656
[ Development of an automated speech recognition interface for personal emergency response systems.]
Sentence
   sofa: _InitialView
   begin: 34656
   end: 34754
[ Journal of NeuroEngineering and Rehabilitation, 6(1):26.]
Sentence
   sofa: _InitialView
   begin: 34754
   end: 34811
[ He, J. and King, W. (2008).]
Sentence
   sofa: _InitialView
   begin: 34811
   end: 34839
[ The role of user participa- tion in information systems development: Implications from a meta-analysis.]
Sentence
   sofa: _InitialView
   begin: 34839
   end: 34943
[ Journal of Management Information Systems, 25(1):301–331.]
Sentence
   sofa: _InitialView
   begin: 34943
   end: 35001
[Hsieh, C., Chuang, S., Chen, S., Chen, C., and Fan, K. (2010).]
Sentence
   sofa: _InitialView
   begin: 35002
   end: 35064
[ Segmentation of human body parts using de- formable triangulation.]
Sentence
   sofa: _InitialView
   begin: 35064
   end: 35131
[ IEEE Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans, 40(3):596–610.]
Sentence
   sofa: _InitialView
   begin: 35131
   end: 35225
[ Hwang, M. and Thorn, R. (1999).]
Sentence
   sofa: _InitialView
   begin: 35225
   end: 35257
[ The effect of user en- gagement on system success: A meta-analytical integra- tion of research findings.]
Sentence
   sofa: _InitialView
   begin: 35257
   end: 35362
[ Information and Management, 35(4):229–236.]
Sentence
   sofa: _InitialView
   begin: 35362
   end: 35405
[ Igual, R., Medrano, C., and Plaza, I.]
Sentence
   sofa: _InitialView
   begin: 35405
   end: 35443
[ (2013).]
Sentence
   sofa: _InitialView
   begin: 35443
   end: 35451
[ Challenges, issues and trends in fall detection systems.]
Sentence
   sofa: _InitialView
   begin: 35451
   end: 35508
[ BioMedical Engineering OnLine, 12(1):1–24.]
Sentence
   sofa: _InitialView
   begin: 35508
   end: 35551
[ Kangas, M., Vikman, I., Nyberg, L., Korpelainen, R., Lind- blom, J., and Jämsä, T. (2012).]
Sentence
   sofa: _InitialView
   begin: 35551
   end: 35642
[ Comparison of real-life accidental falls in older people with experimental falls in middle-aged test subjects.]
Sentence
   sofa: _InitialView
   begin: 35642
   end: 35753
[ Gait & Posture, 35(3):500 – 505.]
Sentence
   sofa: _InitialView
   begin: 35753
   end: 35786
[ Khan, R. A., Meyer, A., Konik, H., and Bouakaz, S. (2013).]
Sentence
   sofa: _InitialView
   begin: 35786
   end: 35845
[ Pain detection through shape and appearance features.]
Sentence
   sofa: _InitialView
   begin: 35845
   end: 35899
[ In Multimedia and Expo (ICME), 2013 IEEE International Conference on, pages 1–6, July.]
Sentence
   sofa: _InitialView
   begin: 35899
   end: 35986
[ Klenk, J., Becker, C., Lieken, F., Nicolai, S., Maetzler, W., Alt, W., Zijlstra, W., Hausdorff, J., van Lummel, R., Chiari, L., and Lindemann, U.]
Sentence
   sofa: _InitialView
   begin: 35986
   end: 36132
[ (2011).]
Sentence
   sofa: _InitialView
   begin: 36132
   end: 36140
[ Comparison of acceleration signals of simulated and real-world back- ward falls.]
Sentence
   sofa: _InitialView
   begin: 36140
   end: 36221
[ Medical Engineering & Physics, 33(3):368 – 373.]
Sentence
   sofa: _InitialView
   begin: 36221
   end: 36269
[ Kujala, S. (2003).]
Sentence
   sofa: _InitialView
   begin: 36269
   end: 36288
[ User involvement: A review of the ben- efits and challenges.]
Sentence
   sofa: _InitialView
   begin: 36288
   end: 36349
[ Behaviour and Information Technology, 22(1):1–16.]
Sentence
   sofa: _InitialView
   begin: 36349
   end: 36399
[ Lecouteux, B., Vacher, M., and Portet, F. (2011).]
Sentence
   sofa: _InitialView
   begin: 36399
   end: 36449
[ Distant Speech Recognition in a Smart Home: Comparison of Several Multisource ASRs in Realistic Conditions.]
Sentence
   sofa: _InitialView
   begin: 36449
   end: 36557
[ In In- terspeech 2011, pages 1–4, Florence, Italy.]
Sentence
   sofa: _InitialView
   begin: 36557
   end: 36608
[ Miguel Angel Bautista, Sergio Escalera, D. S. (2015).]
Sentence
   sofa: _InitialView
   begin: 36608
   end: 36662
[ Hupba8k: Dataset and ecoc-graph-cut based segmentation of human limbs.]
Sentence
   sofa: _InitialView
   begin: 36662
   end: 36733
[ Neurocomputing, 150(A):173–188.]
Sentence
   sofa: _InitialView
   begin: 36733
   end: 36765
[ Moon, T. K. (1996).]
Sentence
   sofa: _InitialView
   begin: 36765
   end: 36785
[ The expectation-maximization algorithm.]
Sentence
   sofa: _InitialView
   begin: 36785
   end: 36825
[ IEEE Signal Processing Magazine, 13(6):47–60.]
Sentence
   sofa: _InitialView
   begin: 36825
   end: 36871
[ Norman, D. A. and Draper, S. W. (1986).]
Sentence
   sofa: _InitialView
   begin: 36871
   end: 36911
[ User Centered System Design; New Perspectives on Human-Computer Interaction.]
Sentence
   sofa: _InitialView
   begin: 36911
   end: 36988
[ L. Erlbaum Associates Inc. Pantic, M., Pentland, A., Nijholt, A., and Huang, T. (2006).]
Sentence
   sofa: _InitialView
   begin: 36988
   end: 37076
[ Human computing and machine understanding of human behavior: survey.]
Sentence
   sofa: _InitialView
   begin: 37076
   end: 37145
[ In ACM Int.]
Sentence
   sofa: _InitialView
   begin: 37145
   end: 37157
[ Conf.]
Sentence
   sofa: _InitialView
   begin: 37157
   end: 37163
[ on Multimodal In- terfaces.]
Sentence
   sofa: _InitialView
   begin: 37163
   end: 37191
[ Pino, M., Moget, C., Benveniste, S., Picard, R., and Rigaud, A.-S. (2015).]
Sentence
   sofa: _InitialView
   begin: 37191
   end: 37266
[ Innovative technology-based healthcare and support services for older adults: How and why in- dustrial initiatives convert to the living lab approach.]
Sentence
   sofa: _InitialView
   begin: 37266
   end: 37417
[ In HCI International, volume Part II of LNCS 9194, pages 158–169.]
Sentence
   sofa: _InitialView
   begin: 37417
   end: 37483
[ Springer International Publishing.]
Sentence
   sofa: _InitialView
   begin: 37483
   end: 37518
[ Popescu, M., Li, Y., Skubic, M., and Rantz, M. (2008).]
Sentence
   sofa: _InitialView
   begin: 37518
   end: 37573
[ An acoustic fall detector system that uses sound height information to reduce the false alarm rate.]
Sentence
   sofa: _InitialView
   begin: 37573
   end: 37673
[ In Proc.]
Sentence
   sofa: _InitialView
   begin: 37673
   end: 37682
[ 30th Annual Int.]
Sentence
   sofa: _InitialView
   begin: 37682
   end: 37699
[ Conference of the IEEE-EMBS 2008, pages 4628–4631, 20–25 Aug. Povey, D., Burget, L., Agarwal, M., Akyazi, P., Kai, F., Ghoshal, A., Glembek, O., Goel, N., Karafiát, M., Rastrow, A., Rose, R. C., Schwarz, P., and Thomas, S. (2011a).]
Sentence
   sofa: _InitialView
   begin: 37699
   end: 37931
[ The subspace gaussian mixture model—a structured model for speech recognition.]
Sentence
   sofa: _InitialView
   begin: 37931
   end: 38010
[ Computer Speech & Language, 25(2):404 – 439.]
Sentence
   sofa: _InitialView
   begin: 38010
   end: 38055
[ Povey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P., Qian, Y., Schwarz, P., Silovsky, J., Stemmer, G., and Vesely, K. (2011b).]
Sentence
   sofa: _InitialView
   begin: 38055
   end: 38232
[ The Kaldi Speech Recognition Toolkit.]
Sentence
   sofa: _InitialView
   begin: 38232
   end: 38270
[ In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding.]
Sentence
   sofa: _InitialView
   begin: 38270
   end: 38343
[ IEEE Signal Processing Soci- ety, December.]
Sentence
   sofa: _InitialView
   begin: 38343
   end: 38387
[ IEEE Catalog No.:]
Sentence
   sofa: _InitialView
   begin: 38387
   end: 38405
[ CFP11SRW-USB.]
Sentence
   sofa: _InitialView
   begin: 38405
   end: 38419
[ Priyank Shah, H. M. (2014).]
Sentence
   sofa: _InitialView
   begin: 38419
   end: 38447
[ Comperehensive study and comparative analysis of different types of background subtraction algorithms.]
Sentence
   sofa: _InitialView
   begin: 38447
   end: 38550
[ International Journal of Im- age,Graphics and Signal Processing, 6(8):47–52.]
Sentence
   sofa: _InitialView
   begin: 38550
   end: 38627
[ Saim, R. (2014).]
Sentence
   sofa: _InitialView
   begin: 38627
   end: 38644
[ Accidental falls amongst the elderly: Health impact and effective intervention strategies.]
Sentence
   sofa: _InitialView
   begin: 38644
   end: 38735
[ Mas- ter’s thesis.]
Sentence
   sofa: _InitialView
   begin: 38735
   end: 38754
[ Vacher, M., Portet, F., Fleury, A., and Noury, N. (2011).]
Sentence
   sofa: _InitialView
   begin: 38754
   end: 38812
[ Development of Audio Sensing Technology for Ambient Assisted Living: Applications and Challenges.]
Sentence
   sofa: _InitialView
   begin: 38812
   end: 38910
[ International journal of E-Health and medical communications, 2(1):35–54, March.]
Sentence
   sofa: _InitialView
   begin: 38910
   end: 38991
[ Vacher, M., Lecouteux, B., Chahuara, P., Portet, F., Meil- lon, B., and Bonnefond, N. (2014).]
Sentence
   sofa: _InitialView
   begin: 38991
   end: 39085
[ The Sweet-Home speech and multimodal corpus for home automation interaction.]
Sentence
   sofa: _InitialView
   begin: 39085
   end: 39162
[ In The 9th edition of the Language Resources and Evaluation Conference (LREC), pages 4499–4506, Reykjavik, Iceland.]
Sentence
   sofa: _InitialView
   begin: 39162
   end: 39278
[ Vacher, M., Aman, F., Rossato, S., and Portet, F. (2015a).]
Sentence
   sofa: _InitialView
   begin: 39278
   end: 39337
[ Development of Automatic Speech Recognition Tech- niques for Elderly Home Support: Applications and Challenges.]
Sentence
   sofa: _InitialView
   begin: 39337
   end: 39449
[ In J. Zou et al., editors, HCI International, volume Part II of LNCS 9194, pages 341–353, Los An- geles, CA, United States, August.]
Sentence
   sofa: _InitialView
   begin: 39449
   end: 39581
[ Springer International Publishing Switzerland.]
Sentence
   sofa: _InitialView
   begin: 39581
   end: 39628
[ Vacher, M., Lecouteux, B., Aman, F., Rossato, S., and Portet, F. (2015b).]
Sentence
   sofa: _InitialView
   begin: 39628
   end: 39702
[ Recognition of Distress Calls in Distant Speech Setting: a Preliminary Experiment in a Smart Home.]
Sentence
   sofa: _InitialView
   begin: 39702
   end: 39801
[ In 6th Workshop on Speech and Language Processing for Assistive Technologies, pages 1–7, Dresden, Germany, September.]
Sentence
   sofa: _InitialView
   begin: 39801
   end: 39919
[ SIG-SLPAT.]
Sentence
   sofa: _InitialView
   begin: 39919
   end: 39930
[ Who.]
Sentence
   sofa: _InitialView
   begin: 39930
   end: 39935
[ (2013).]
Sentence
   sofa: _InitialView
   begin: 39935
   end: 39943
[ World population ageing: 1950-2050.]
Sentence
   sofa: _InitialView
   begin: 39943
   end: 39979
[ Tech- nical report, Executive report, United Nations, Depart- ment of Economic and Social Affairs, Population Divi- sion.]
Sentence
   sofa: _InitialView
   begin: 39979
   end: 40101
[ Young, V. and Mihailidis, A.]
Sentence
   sofa: _InitialView
   begin: 40101
   end: 40130
[ (2010).]
Sentence
   sofa: _InitialView
   begin: 40130
   end: 40138
[ An automated, speech-based emergency response system for the older adult.]
Sentence
   sofa: _InitialView
   begin: 40138
   end: 40212
[ Gerontechnology, 9(2):261.]
Sentence
   sofa: _InitialView
   begin: 40212
   end: 40239
[ Young, V. and Mihailidis, A.]
Sentence
   sofa: _InitialView
   begin: 40239
   end: 40268
[ (2013).]
Sentence
   sofa: _InitialView
   begin: 40268
   end: 40276
[ The CARES corpus: a database of older adult actor simulated emergency dia- logue for developing a personal emergency response system.]
Sentence
   sofa: _InitialView
   begin: 40276
   end: 40410
[ I. J.]
Sentence
   sofa: _InitialView
   begin: 40410
   end: 40416
[ Speech Technology, 16(1):55–73.]
Sentence
   sofa: _InitialView
   begin: 40416
   end: 40448
[ Zouari, L. and Chollet, G. (2006).]
Sentence
   sofa: _InitialView
   begin: 40448
   end: 40483
[ Efficient gaussian mixture for speech recognition.]
Sentence
   sofa: _InitialView
   begin: 40483
   end: 40534
[ In Pattern Recognition, 2006.]
Sentence
   sofa: _InitialView
   begin: 40534
   end: 40564
[ ICPR 2006.]
Sentence
   sofa: _InitialView
   begin: 40564
   end: 40575
[ 18th International Conference on, volume 4, pages 294–297.]
Sentence
   sofa: _InitialView
   begin: 40575
   end: 40634
-------- View _InitialView end ----------------------------------

======== CAS 0 end ==================================


